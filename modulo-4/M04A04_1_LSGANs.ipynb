{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04A04_1 - LSGANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3FOk480i2d-"
      },
      "source": [
        "# Introdução\r\n",
        "\r\n",
        "Nesse notebook, vamos implementar uma função de perda proposta pela [LSGAN](https://arxiv.org/abs/1611.04076) (*Least Squares GAN*). O problema que esse trabalho ataca é aquele dos gradientes de baixa qualidade que a função de perda da Entropia Cruzada fornece em algumas condições específicas. Por isso, esse trabalho usa uma função de perda baseada no erro quadrático:\r\n",
        "\r\n",
        "![LSGAN loss](https://wiseodd.github.io/img/2017-03-02-least-squares-gan/03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0o-c0P9wcM"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ATPZX-9qXK"
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJgxyO-9z-4"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 25,      # Number of epochs.\n",
        "    'lr': 5e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-5, # L2 penalty.\n",
        "    'num_workers': 8,     # Number of workers on data loader.\n",
        "    'batch_size': 500,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'z_dim': 100,         # Dimension of z input vector.\n",
        "    'num_samples': 4,     # Number of samples to be generated in evaluation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZA6okfQ98TP"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uSTJ0ChttFD"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "opener = urllib.request.build_opener()\r\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\r\n",
        "urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQAwioI99NJ"
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvzdYk6v-gdd"
      },
      "source": [
        "# Definindo o Gerador $G$\n",
        "\n",
        "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FJQQU7-kP5"
      },
      "source": [
        "# Adversarial Generator.\n",
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=100, output_channels=1, img_size=28):\n",
        "    \n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128 * (self.img_size // 4) * (self.img_size // 4)),\n",
        "            nn.BatchNorm1d(128 * (self.img_size // 4) * (self.img_size // 4)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, self.output_channels, 4, 2, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        x = x.view(-1, 128, (self.img_size // 4), (self.img_size // 4))\n",
        "        \n",
        "        x = self.deconv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiating G.\n",
        "net_G = Generator(input_dim=args['z_dim']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsrfQ2eOsEZB"
      },
      "source": [
        "# Definindo o Discriminador $D$\n",
        "\n",
        "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pai5kiN6DW2R"
      },
      "source": [
        "# Adversarial Discriminator.\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=1, input_size=28):\n",
        "        \n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_dim,\n",
        "                      64,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64,\n",
        "                      128,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        \n",
        "        input_linear = 128 * (self.input_size // 4) * (self.input_size // 4)\n",
        "        # TO DO: Define final linear layers:\n",
        "        # 1) Linear with 128 outputs + batchnorm1d + LeakyReLu\n",
        "        # 2) Linear with 1 output (no activation)\n",
        "        self.fc = nn.Sequential(\n",
        "            # ...\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
        "        \n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiating D.\n",
        "net_D = Discriminator().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeUPraJ3EyeP"
      },
      "source": [
        "# Definindo o otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83m3L4FEzgY"
      },
      "source": [
        "# Defining optimizer for G.\n",
        "opt_G = optim.Adam(net_G.parameters(),\n",
        "                   lr=args['lr'],\n",
        "                   weight_decay=args['weight_decay'])\n",
        "\n",
        "# Defining optimizer for D.\n",
        "opt_D = optim.Adam(net_D.parameters(),\n",
        "                   lr=args['lr'],\n",
        "                   weight_decay=args['weight_decay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv0tdlhKQYOt"
      },
      "source": [
        "# Definindo um Scheduler para os Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnD3mr9Qef6"
      },
      "source": [
        "# Defining lr scheduler.\n",
        "scheduler_G = optim.lr_scheduler.StepLR(opt_G, args['epoch_num'] // 5, 0.5)\n",
        "scheduler_D = optim.lr_scheduler.StepLR(opt_D, args['epoch_num'] // 5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Cvxt95E-A0"
      },
      "source": [
        "# Definindo a loss composta\r\n",
        "\r\n",
        "Nesse ponto é onde precisamos usar uma função de perda diferente da GAN padrão. Na LSGAN, a função de perda que usamos é a MSE (*mean squared error*). Nesse caso, os valores para os labels: 0 e 1 tem uma interpretação diferente. Para otimizar o Discriminador, quando avalia as imagens reais ele deve aproximar seu escore do valor 1, enquanto que ao avaliar imagens falsas seu escore deve se aproximar de 0. Para otimizar o Gerador, temos o contrário, queremos que quando o Discriminador avalie imagens falsas, seu escore se aproxime de 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8SIpVnFCID"
      },
      "source": [
        "# TO DO: defining adversarial loss.\n",
        "criterion = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gUQrX7Fvf-"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUHQYKgFxz3"
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader,\n",
        "          net_G, net_D,\n",
        "          criterion,\n",
        "          opt_G, opt_D,\n",
        "          epoch,\n",
        "          train_loss_G, train_loss_D):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    # Predefining ones and zeros for batches.\n",
        "    y_real = torch.ones(args['batch_size'], 1).to(args['device'])\n",
        "    y_fake = torch.zeros(args['batch_size'], 1).to(args['device'])\n",
        "\n",
        "    # Setting networks for training mode.\n",
        "    net_D.train()\n",
        "    net_G.train()\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        # Obtaining images and labels for batch.\n",
        "        x, labs = batch_data\n",
        "        \n",
        "        # Creating random vector z.\n",
        "        z = torch.rand((args['batch_size'], args['z_dim']))\n",
        "        \n",
        "        # Casting to correct device (x and z).\n",
        "        x = x.to(args['device'])\n",
        "        z = z.to(args['device'])\n",
        "        \n",
        "        ###############\n",
        "        # Updating D. #\n",
        "        ###############\n",
        "        \n",
        "        # Clearing the gradients of D optimizer.\n",
        "        opt_D.zero_grad()\n",
        "\n",
        "        # Forwarding real data.\n",
        "        D_real = net_D(x) # Through D.\n",
        "        \n",
        "        # Computing loss for real data.\n",
        "        D_real_loss = criterion(D_real, y_real)\n",
        "\n",
        "        # Forwarding fake data.\n",
        "        G_out = net_G(z) # Through G.\n",
        "        D_fake = net_D( G_out.detach() ) # Through D.\n",
        "        \n",
        "        # Computing loss for fake data.\n",
        "        D_fake_loss = criterion(D_fake, y_fake)\n",
        "\n",
        "        # Computing total loss for D.\n",
        "        D_loss = D_real_loss + D_fake_loss\n",
        "        \n",
        "        # Computing backpropagation for D.\n",
        "        D_loss.backward()\n",
        "        \n",
        "        # Taking step in D optimizer.\n",
        "        opt_D.step()\n",
        "\n",
        "        ###############\n",
        "        # Updating G. #\n",
        "        ###############\n",
        "        \n",
        "        # Clearing the gradients of G optimizer.\n",
        "        opt_G.zero_grad()\n",
        "\n",
        "        # Forwarding fake data.\n",
        "        G_out = net_G(z) # Through G.\n",
        "        \n",
        "        D_fake = net_D(G_out) # Through D.\n",
        "        \n",
        "        # Computing loss for G.\n",
        "        G_loss = criterion(D_fake, y_real)\n",
        "        \n",
        "        # Computing backpropagation for G.\n",
        "        G_loss.backward()\n",
        "        \n",
        "        # Taking step in G optimizer.\n",
        "        opt_G.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss_G.append(G_loss.data.item())\n",
        "        train_loss_D.append(D_loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Printing training epoch loss.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [training time %.2f]' % (\n",
        "        epoch, (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    \n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Plotting losses.\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "        ax[0].plot(np.asarray(train_loss_G), 'r-', label='G loss')\n",
        "        ax[0].legend()\n",
        "        \n",
        "        ax[1].plot(np.asarray(train_loss_D), 'b--', label='D loss')\n",
        "        ax[1].legend()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "    return train_loss_G, train_loss_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2N1kgQNMTl"
      },
      "source": [
        "# Evaluating procedure.\n",
        "def evaluate(net_G, criterion, epoch):\n",
        "    \n",
        "    # Setting networks for training mode.\n",
        "    net_D.eval()\n",
        "    net_G.eval()\n",
        "    \n",
        "    # Creating random vector z.\n",
        "    z = torch.rand((args['num_samples'] * args['num_samples'], args['z_dim']))\n",
        "    \n",
        "    # Casting to correct device.\n",
        "    z = z.to(args['device'])\n",
        "    \n",
        "    # Generating new samples.\n",
        "    G_out = net_G(z)\n",
        "    \n",
        "    # Plotting.\n",
        "    fig, ax = plt.subplots(args['num_samples'],\n",
        "                           args['num_samples'],\n",
        "                           figsize=(8, 8))\n",
        "    \n",
        "    for i in range(args['num_samples']):\n",
        "        \n",
        "        for j in range(args['num_samples']):\n",
        "            \n",
        "            sample = G_out[j * args['num_samples'] + i]\n",
        "            \n",
        "            ax[j, i].imshow(sample.detach().cpu().numpy().squeeze(),\n",
        "                            cmap=plt.get_cmap('gray'))\n",
        "            ax[j, i].set_yticks([])\n",
        "            ax[j, i].set_xticks([])\n",
        "            \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhC6uqUpJO8Y"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbf0HVDJKTC"
      },
      "source": [
        "# Lists for losses.\n",
        "train_loss_G = []\n",
        "train_loss_D = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train_loss_G, train_loss_D = train(train_loader,\n",
        "                                       net_G, net_D,\n",
        "                                       criterion,\n",
        "                                       opt_G, opt_D,\n",
        "                                       epoch,\n",
        "                                       train_loss_G, train_loss_D)\n",
        "    \n",
        "    # Taking step on scheduler.\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Testing function for sample generation.\n",
        "        evaluate(net_G, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}