{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04A04_2 - Conditional GANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqF7LyHjnDsK"
      },
      "source": [
        "# Introdução\r\n",
        "\r\n",
        "Nesse notebook vamos implementar a GAN condicional (cGAN), que é uma modificação importante da GAN padrão que gerou várias formas diferentes delas serem usadas. Essa modificação consiste em incluir a informação do label no processo de geração. Com isso, o gerador é informado que ele deve não apenas gerar uma imaagem aleatória, mas também que essa imagem deve ser de determinada classe. Essa informação também é passada para o discriminador, e portanto ele tem o dever de verificar se aquela imagem é real sabendo que ela é de uma dada classe.\r\n",
        "\r\n",
        "Como o processo de geração fica condicionado nos labels, essa técnica ficou conhecida como GAN condicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0o-c0P9wcM"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ATPZX-9qXK"
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJgxyO-9z-4"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 25,      # Number of epochs.\n",
        "    'lr': 5e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-5, # L2 penalty.\n",
        "    'num_workers': 8,     # Number of workers on data loader.\n",
        "    'batch_size': 500,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'z_dim': 100,         # Dimension of z input vector.\n",
        "    'num_samples': 10,    # Number of samples to be generated in evaluation.\n",
        "    'num_classes': 10,    # Number of classes in dataset.\n",
        "    'img_size': 28,       # Image size (H and W).\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZA6okfQ98TP"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04rxM57vAKy0"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "opener = urllib.request.build_opener()\r\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\r\n",
        "urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQAwioI99NJ"
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHV0enX9-Ars"
      },
      "source": [
        "# Treinamento Adversarial Condicional\n",
        "\n",
        "Podemos inserir o conceito de classe na nossa GAN ao passar o rótulo *c* de cada amostra para tanto $G$ quanto $D$. Dessa forma, $G$ vai otimizar a distribuição $p(x | z, c)$ usando o rótulo para gerar amostras da classe correta.\n",
        "\n",
        "![CGANS](https://www.dropbox.com/s/gqtc5710dsrd4rh/GANs_Architecture_CGAN.png?dl=1)\n",
        "\n",
        "Fazemos isso ao adicionar one-hot encodings de $c$ para cada amostra que são concatenados ao batch na dimensão 1 (tanto em $G$ quanto em $D$). Esse tipo de rede é conhecido como uma Conditional GAN (CGAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpfnWyeBWg0e"
      },
      "source": [
        "# Atividade Prática: Transformar nossa GAN em uma CGAN\n",
        "\n",
        "1.   Implementar função *one_hot_encodings()*. Ela deve retornar 2 one-hot encodings: um para ser passado para $G$ e um para ser passado para $D$;\n",
        "2.   Reaproveitar as arquiteturas de $G$ e $D$ anteriores e adaptar a *self.fc* de $G$ para receber o one-hot encoding das classes das amostras junto de $z$, ou seja, *args\\['num_classes'\\]* dimensões a mais de input para $G$. O módulo $self.conv$ de $D$ também deve ser adaptado para receber *args\\['num_classes'\\]* canais a mais;\n",
        "3.   Complementar função *train()* para passar os encodings corretos para o *forward()* de cada rede;\n",
        "4.   Lembrar de realizar a concatenação de ($z$, *c*) no *forward()* de $G$ e ($x$, *c*) no *forward()* de $D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqOLcgqZXvTz"
      },
      "source": [
        "# Computando one-hot encodings para labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73gmwApIXv77"
      },
      "source": [
        "# TO DO: Predefining one-hot encodings for G and D inputs.\n",
        "def one_hot_encodings(labs, batch_size, num_classes, img_size):\n",
        "\n",
        "    # TO DO: create array filled zeros with dimensions (B, #Classes).\n",
        "    batch_one_hot_g = # ...\n",
        "    \n",
        "    # TO DO: create array filled zeros with dimensions (B, #Classes, 28, 28).\n",
        "    batch_one_hot_d = # ...\n",
        "    \n",
        "    # TO DO: For each label in a sample in a batch, fill the corresponding\n",
        "    #        other dimensions in batch_one_hot_g and batch_one_hot_d with 1.\n",
        "    for i in range(labs.size(0)):\n",
        "        # ...\n",
        "        \n",
        "        # ...\n",
        "    \n",
        "    return batch_one_hot_g, batch_one_hot_d\n",
        "\n",
        "# Sanity test for function one_hot_encodings().\n",
        "labs = torch.randint_like(torch.zeros((4)), high=10, dtype=torch.long).to(args['device'])\n",
        "print(labs)\n",
        "one_hot_g, one_hot_d = one_hot_encodings(labs, 4, 10, 2)\n",
        "print(one_hot_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvzdYk6v-gdd"
      },
      "source": [
        "# Definindo o Gerador $G$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FJQQU7-kP5"
      },
      "source": [
        "# Adversarial Generator.\n",
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 input_dim=100,\n",
        "                 output_channels=1,\n",
        "                 img_size=28,\n",
        "                 num_classes=10):\n",
        "    \n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define FC layers.\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim + self.num_classes, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128 * (self.img_size // 4) * (self.img_size // 4)),\n",
        "            nn.BatchNorm1d(128 * (self.img_size // 4) * (self.img_size // 4)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        # Define deconv layers.\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, self.output_channels, 4, 2, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        \n",
        "        # TO DO: Concat x and c.\n",
        "        x = # ...\n",
        "        \n",
        "        # Forward.\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        x = x.view(-1, 128, (self.img_size // 4), (self.img_size // 4))\n",
        "        \n",
        "        x = self.deconv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiating G.\n",
        "net_G = Generator(input_dim=args['z_dim']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsrfQ2eOsEZB"
      },
      "source": [
        "# Definindo o Discriminador $D$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pai5kiN6DW2R"
      },
      "source": [
        "# Adversarial Discriminator.\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 input_channels=1,\n",
        "                 output_channels=1,\n",
        "                 input_size=28,\n",
        "                 num_classes=10):\n",
        "        \n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Define conv layers.\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_channels + self.num_classes,\n",
        "                      64,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64,\n",
        "                      128,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        \n",
        "        # Define FC layers.\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, self.output_channels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "                \n",
        "    def forward(self, x, c):\n",
        "        \n",
        "        x = torch.cat([x, c], 1)\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
        "        \n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiating D.\n",
        "net_D = Discriminator().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeUPraJ3EyeP"
      },
      "source": [
        "# Definindo o otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83m3L4FEzgY"
      },
      "source": [
        "# Defining optimizer for G.\n",
        "opt_G = optim.Adam(net_G.parameters(),\n",
        "                   lr=args['lr'],\n",
        "                   weight_decay=args['weight_decay'])\n",
        "\n",
        "# Defining optimizer for D.\n",
        "opt_D = optim.Adam(net_D.parameters(),\n",
        "                   lr=args['lr'],\n",
        "                   weight_decay=args['weight_decay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv0tdlhKQYOt"
      },
      "source": [
        "# Definindo um Scheduler para os Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnD3mr9Qef6"
      },
      "source": [
        "# Defining lr scheduler.\n",
        "scheduler_G = optim.lr_scheduler.StepLR(opt_G, args['epoch_num'] // 5, 0.5)\n",
        "scheduler_D = optim.lr_scheduler.StepLR(opt_D, args['epoch_num'] // 5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Cvxt95E-A0"
      },
      "source": [
        "# Definindo a loss composta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8SIpVnFCID"
      },
      "source": [
        "# Defining adversarial loss.\n",
        "criterion = nn.BCELoss().to(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gUQrX7Fvf-"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUHQYKgFxz3"
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader,\n",
        "          net_G, net_D,\n",
        "          criterion,\n",
        "          opt_G, opt_D,\n",
        "          epoch,\n",
        "          train_loss_G, train_loss_D):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    # Predefining ones and zeros for batches.\n",
        "    y_real = torch.ones(args['batch_size'], 1).to(args['device'])\n",
        "    y_fake = torch.zeros(args['batch_size'], 1).to(args['device'])\n",
        "\n",
        "    # Setting networks for training mode.\n",
        "    net_D.train()\n",
        "    net_G.train()\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        # Obtaining images and labels for batch.\n",
        "        x, labs = batch_data\n",
        "        \n",
        "        # Creating random vector z.\n",
        "        z = torch.rand((args['batch_size'], args['z_dim']))\n",
        "        \n",
        "        # TO DO: Generating random values for batch fake_labs.\n",
        "        fake_labs = # ...\n",
        "        \n",
        "        # Casting to correct device (x and z).\n",
        "        x = x.to(args['device'])\n",
        "        z = z.to(args['device'])\n",
        "        \n",
        "        labs = labs.to(args['device'])\n",
        "        fake_labs = fake_labs.to(args['device'])\n",
        "        \n",
        "        \n",
        "        # TO DO: Computing one-hot encodings for batch labs and fake_labs.\n",
        "        one_hot_g_fake, one_hot_d_fake = # ...\n",
        "        \n",
        "        one_hot_g_real, one_hot_d_real = # ...\n",
        "        \n",
        "        ###############\n",
        "        # Updating D. #\n",
        "        ###############\n",
        "        \n",
        "        # Clearing the gradients of D optimizer.\n",
        "        opt_D.zero_grad()\n",
        "\n",
        "        # Forwarding real data.\n",
        "        D_real = net_D(x, one_hot_d_real) # Through D.\n",
        "        \n",
        "        # Computing loss for real data.\n",
        "        D_real_loss = criterion(D_real, y_real)\n",
        "\n",
        "        # Forwarding fake data.\n",
        "        G_out = net_G(z, one_hot_g_fake) # Through G.\n",
        "        D_fake = net_D(G_out.detach(), one_hot_d_fake) # Through D.\n",
        "        \n",
        "        # Computing loss for fake data.\n",
        "        D_fake_loss = criterion(D_fake, y_fake)\n",
        "\n",
        "        # Computing total loss for D.\n",
        "        D_loss = D_real_loss + D_fake_loss\n",
        "        \n",
        "        # Computing backpropagation for D.\n",
        "        D_loss.backward()\n",
        "        \n",
        "        # Taking step in D optimizer.\n",
        "        opt_D.step()\n",
        "\n",
        "        ###############\n",
        "        # Updating G. #\n",
        "        ###############\n",
        "        \n",
        "        # Clearing the gradients of G optimizer.\n",
        "        opt_G.zero_grad()\n",
        "\n",
        "        # Forwarding fake data.\n",
        "        G_out = net_G(z, one_hot_g_fake) # Through G.\n",
        "        D_fake = net_D(G_out, one_hot_d_fake) # Through D.\n",
        "        \n",
        "        # Computing loss for G.\n",
        "        G_loss = criterion(D_fake, y_real)\n",
        "        \n",
        "        # Computing backpropagation for G.\n",
        "        G_loss.backward()\n",
        "        \n",
        "        # Taking step in G optimizer.\n",
        "        opt_G.step()\n",
        "        \n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss_G.append(G_loss.data.item())\n",
        "        train_loss_D.append(D_loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Printing training epoch loss.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [training time %.2f]' % (\n",
        "        epoch, (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    \n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Plotting losses.\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "        ax[0].plot(np.asarray(train_loss_G), 'r-', label='G loss')\n",
        "        ax[0].legend()\n",
        "        \n",
        "        ax[1].plot(np.asarray(train_loss_D), 'b--', label='D loss')\n",
        "        ax[1].legend()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "    return train_loss_G, train_loss_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2N1kgQNMTl"
      },
      "source": [
        "# Evaluating procedure.\n",
        "def evaluate(net_G, criterion, epoch):\n",
        "    \n",
        "    # Setting networks for training mode.\n",
        "    net_D.eval()\n",
        "    net_G.eval()\n",
        "    \n",
        "    fake_labs = torch.randint_like(torch.zeros((args['num_samples'] * args['num_samples'])),\n",
        "                                   high=10,\n",
        "                                   dtype=torch.long).to(args['device'])\n",
        "    \n",
        "    one_hot_g, one_hot_d = one_hot_encodings(fake_labs,\n",
        "                                             args['num_samples'] * args['num_samples'],\n",
        "                                             args['num_classes'],\n",
        "                                             args['img_size'])\n",
        "    \n",
        "    # Creating random vector z.\n",
        "    z = torch.rand((args['num_samples'] * args['num_samples'], args['z_dim']))\n",
        "    \n",
        "    # Casting to correct device.\n",
        "    z = z.to(args['device'])\n",
        "    \n",
        "    # Generating new samples.\n",
        "    G_out = net_G(z, one_hot_g)\n",
        "    \n",
        "    # Plotting.\n",
        "    fig, ax = plt.subplots(args['num_samples'],\n",
        "                           args['num_samples'],\n",
        "                           figsize=(20, 20))\n",
        "    \n",
        "    for i in range(args['num_samples']):\n",
        "        \n",
        "        for j in range(args['num_samples']):\n",
        "            \n",
        "            sample = G_out[j * args['num_samples'] + i]\n",
        "            \n",
        "            ax[j, i].imshow(sample.detach().cpu().numpy().squeeze(),\n",
        "                            cmap=plt.get_cmap('gray'))\n",
        "            ax[j, i].set_yticks([])\n",
        "            ax[j, i].set_xticks([])\n",
        "            ax[j, i].set_title('Label: ' + str(fake_labs[j * args['num_samples'] + i].item()))\n",
        "            \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhC6uqUpJO8Y"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbf0HVDJKTC"
      },
      "source": [
        "# Lists for losses.\n",
        "train_loss_G = []\n",
        "train_loss_D = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train_loss_G, train_loss_D = train(train_loader,\n",
        "                                       net_G, net_D,\n",
        "                                       criterion,\n",
        "                                       opt_G, opt_D,\n",
        "                                       epoch,\n",
        "                                       train_loss_G, train_loss_D)\n",
        "    \n",
        "    # Taking step on scheduler.\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Testing function for sample generation.\n",
        "        evaluate(net_G, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}