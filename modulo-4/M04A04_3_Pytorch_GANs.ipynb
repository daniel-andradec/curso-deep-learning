{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04A04_3 - Pytorch GANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgmPsjyNc0Oh"
      },
      "source": [
        "# Introdução\r\n",
        "\r\n",
        "Nesse notebook, vamos explorar algumas modificações de GANs que melhoram a GAN original em alguns aspectos. Em duas dessas abordagens (DCGAN e WGAN-GP), o objetivo era melhorar a estabilidade do treinamento, e em uma delas (Pix2Pix) o objetivo era extender as GANs para uma tarefa ligeiramente diferente, de \"tradução\" de imagem para imagem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXs-3iyk2e3l"
      },
      "source": [
        "!rm -r PyTorch-GAN\n",
        "!git clone https://github.com/eriklindernoren/PyTorch-GAN\n",
        "%cd /content/PyTorch-GAN/\n",
        "!sudo pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZaC-TNwWPB_"
      },
      "source": [
        "# DCGANs\n",
        "\n",
        "A primeira arquitetura que vamos ver é a [DCGAN](https://arxiv.org/abs/1511.06434), que foi um trabalho seminal de 2016 que trouxe alguns padrões de arquitetura e de treinamento para as GANs. Antes desse trabalho, a geração de imagens de resolução média ainda era um desafio e com essa abordagem se atraiu ainda mais interesse de pesquisadores para as GANs. A arquitetura do Gerador é a seguinte:\n",
        "\n",
        "![DCGAN Architecture](https://miro.medium.com/max/700/1*KvMnRfb76DponICrHIbSdg.png)\n",
        "\n",
        "\n",
        "\n",
        "A cartilha para treinamento das GANs proposta pelo artigo é a seguinte:\n",
        "\n",
        "![DCGAN guideline](https://i.imgur.com/08EVNUb.png)\n",
        "\n",
        "\n",
        "\n",
        "Imagens geradas ao longo do treinamneto das DCGANs:\n",
        "\n",
        "![DCGANs](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/dcgan.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4KNnCTweVjU"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhbTmvfONfK"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "opener = urllib.request.build_opener()\r\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\r\n",
        "urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOKdjbZSls2s"
      },
      "source": [
        "Primeiramente vamos acessar o diretório de implementação da DCGAN, do repositório que estamos usando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k2I21GllUD3"
      },
      "source": [
        "%cd /content/PyTorch-GAN/implementations/dcgan/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APMVFDTolw6X"
      },
      "source": [
        "Declarando hiperparâmetros para usar no treinamento desse modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XfRCIYlpk0"
      },
      "source": [
        "class Opt:\r\n",
        "    epoch = 1\r\n",
        "    n_epochs = 10\r\n",
        "    batch_size = 128\r\n",
        "    lr = 0.0002\r\n",
        "    b1 = 0.5\r\n",
        "    b2 = 0.999\r\n",
        "    n_cpu = 4\r\n",
        "    latent_dim = 100\r\n",
        "    img_size = 64\r\n",
        "    channels = 1\r\n",
        "    sample_interval = 200\r\n",
        "\r\n",
        "opt = Opt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmuEX1R3l6yj"
      },
      "source": [
        "Agora vamos declarar a arquitetura do Gerador e do Discriminador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsc-XVE5nYJa"
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False\r\n",
        "\r\n",
        "def weights_init_normal(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find(\"Conv\") != -1:\r\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\r\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\r\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\r\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "\r\n",
        "        self.init_size = opt.img_size // 4\r\n",
        "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\r\n",
        "\r\n",
        "        self.conv_blocks = nn.Sequential(\r\n",
        "            nn.BatchNorm2d(128),\r\n",
        "            nn.Upsample(scale_factor=2),\r\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(128, 0.8),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            nn.Upsample(scale_factor=2),\r\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(64, 0.8),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\r\n",
        "            nn.Tanh(),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, z):\r\n",
        "        out = self.l1(z)\r\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\r\n",
        "        img = self.conv_blocks(out)\r\n",
        "        return img\r\n",
        "\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "\r\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\r\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\r\n",
        "            if bn:\r\n",
        "                block.append(nn.BatchNorm2d(out_filters, 0.8))\r\n",
        "            return block\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            *discriminator_block(opt.channels, 16, bn=False),\r\n",
        "            *discriminator_block(16, 32),\r\n",
        "            *discriminator_block(32, 64),\r\n",
        "            *discriminator_block(64, 128),\r\n",
        "        )\r\n",
        "\r\n",
        "        # The height and width of downsampled image\r\n",
        "        ds_size = opt.img_size // 2 ** 4\r\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\r\n",
        "\r\n",
        "    def forward(self, img):\r\n",
        "        out = self.model(img)\r\n",
        "        out = out.view(out.shape[0], -1)\r\n",
        "        validity = self.adv_layer(out)\r\n",
        "\r\n",
        "        return validity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW4MX7vfnlgv"
      },
      "source": [
        "# Initialize generator and discriminator\r\n",
        "generator = Generator()\r\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uf9PG4ynZat"
      },
      "source": [
        "Função de perda:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I57VEqQrncaT"
      },
      "source": [
        "# Loss function\r\n",
        "adversarial_loss = torch.nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaMJ8ExInpzh"
      },
      "source": [
        "if cuda:\r\n",
        "    generator.cuda()\r\n",
        "    discriminator.cuda()\r\n",
        "    adversarial_loss.cuda()\r\n",
        "\r\n",
        "# Initialize weights\r\n",
        "generator.apply(weights_init_normal)\r\n",
        "discriminator.apply(weights_init_normal)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O54I2PACnqOJ"
      },
      "source": [
        "Carregando dataloader para o MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwqAXLHLnvJO"
      },
      "source": [
        "# Configure data loader\r\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\r\n",
        "dataloader = torch.utils.data.DataLoader(\r\n",
        "    datasets.MNIST(\r\n",
        "        \"../../data/mnist\",\r\n",
        "        train=True,\r\n",
        "        download=True,\r\n",
        "        transform=transforms.Compose(\r\n",
        "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\r\n",
        "        ),\r\n",
        "    ),\r\n",
        "    batch_size=opt.batch_size,\r\n",
        "    shuffle=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFKi2FxUnvtA"
      },
      "source": [
        "Definindo otimizadores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwlRVusnzqO"
      },
      "source": [
        "# Optimizers\r\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\r\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMZDriNJn201"
      },
      "source": [
        "Definindo o processo de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUNSFaxcWRl9"
      },
      "source": [
        "## definindo uma função para declaração de tensores, dependendo se estamos na GPU ou não\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            \n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "            )\n",
        "            \n",
        "            fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
        "            \n",
        "            for b in range(10):\n",
        "                \n",
        "                ax[b].imshow(gen_imgs.data[b].detach().cpu().numpy().squeeze())\n",
        "                ax[b].set_yticks([])\n",
        "                ax[b].set_xticks([])\n",
        "            \n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6O97M_F4xcl"
      },
      "source": [
        "# Pix2Pix\n",
        "\n",
        "O [Pix2Pix](https://phillipi.github.io/pix2pix/) trouxe para as GANs uma tarefa bastante diferente da GAN tradicional. A partir da GAN condicional, os autores desse trabalho observaram que podemos condicionar o Gerador usando (praticamente) qualquer informação que pode ser fornecida como entrada de uma rede neural. No Pix2Pix, então foi proposta a idéia de condicionar a geração em uma imagem, com o objetivo de gerar uma outra imagem. Por isso, esse processo ficou chamado de tradução imagem-para-imagem (*image-to-image translation*). No artigo original, eles exemplificam vários domínios onde essa arquitetura pode ser usada, e criaram uma ferramenta para que a comunidade em geral pudesse usar a arquitetura de várias formas diferentes. O treinamento dessa arquitetura é feita da seguinte forma:\n",
        "\n",
        "![Pix2Pix D and G](https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67)\n",
        "\n",
        "Exemplos de imagens geradas por essa arquitetura (no artigo original, resultados melhores são reportados):\n",
        "\n",
        "![Pix2Pix Examples](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/pix2pix.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-SPxPVr_pL6"
      },
      "source": [
        "Fazendo alguns procedimentos necessários:\r\n",
        "\r\n",
        "- Download dos dados usados para esse experimento (Facades Dataset)\r\n",
        "- Migrando para o diretório (importado do github) que contém as implementações das arquiteturas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zcu4AbF-Oyi"
      },
      "source": [
        "## download dos dados Facades\r\n",
        "%cd /content/PyTorch-GAN/\r\n",
        "%cd data/\r\n",
        "!bash download_pix2pix_dataset.sh facades > out.log\r\n",
        "\r\n",
        "## migrando para diretório com implementações dos modelos\r\n",
        "%cd /content/PyTorch-GAN/implementations/pix2pix/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgOzUw9L-RUT"
      },
      "source": [
        "import argparse\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import itertools\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "import sys\r\n",
        "\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "## importando modelos e classes necessárias para carregar os dados (classe\r\n",
        "## personalizada para ler do disco o dataset Facades)\r\n",
        "from models import *\r\n",
        "from datasets import *\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_mi4NzmrgEO"
      },
      "source": [
        "Hiperparâmetros para treinamento desse modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf5RZ50H-TcD"
      },
      "source": [
        "class Opt:\r\n",
        "    epoch = 1\r\n",
        "    n_epochs = 20\r\n",
        "    dataset_name = 'facades'\r\n",
        "    batch_size = 1\r\n",
        "    lr = 0.0002\r\n",
        "    b1 = 0.5\r\n",
        "    b2 = 0.999\r\n",
        "    decay_epoch = 100\r\n",
        "    n_cpu = 4\r\n",
        "    img_height = 256\r\n",
        "    img_width = 256\r\n",
        "    channels = 3\r\n",
        "    sample_interval = 200\r\n",
        "    checkpoint_interval = -1\r\n",
        "\r\n",
        "opt = Opt()\r\n",
        "print(opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBV_OlTi-WJV"
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pJSU0wbr1K6"
      },
      "source": [
        "Funções de perda. No caso dessa tarefa, queremos que o Gerador seja capaz de criar imagens tanto que pareçam reais, quanto que seguem a estrutura global da imagem real \"ground truth\". Portanto, temos duas funções de perda:\r\n",
        "\r\n",
        "- Uma loss padrão das GANs de separação entre imagens reais e falsas `MSELoss`.\r\n",
        "- Outra que penaliza o quanto a imagem gerada por G se difere da imagem real esperada `L1Loss`. Essa loss é dita \"pixelwise\" porque ela penaliza a diferença de valores de cada pixel da imagem gerada com a imagem original.\r\n",
        "\r\n",
        "A segunda loss é necessária porque, se ela não existisse, o Gerador não seria penalizado por gerar uma imagem que parece real mas que não segue a estrutura da imagem na qual estamos condicionando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dphQh-Q-X3D"
      },
      "source": [
        "# Loss functions\r\n",
        "criterion_GAN = torch.nn.MSELoss()\r\n",
        "criterion_pixelwise = torch.nn.L1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSMQVZGbApbl"
      },
      "source": [
        "# Loss weight of L1 pixel-wise loss between translated image and real image\r\n",
        "lambda_pixel = 100\r\n",
        "\r\n",
        "# Calculate output of image discriminator (PatchGAN)\r\n",
        "patch = (1, opt.img_height // 2 ** 4, opt.img_width // 2 ** 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To7LtS8MBBnc"
      },
      "source": [
        "Instanciando o Gerador e Discriminador. No caso desse modelo, estamos usando a implementação definida no arquivo `models.py` presente no github que importamos. Ao importar esse módulo acima temos acesso à classe que instancia o objeto. A arqutietura do Gerador é uma UNet, já que essa tarefa tem um objetivo parecido com o de segmentação semântica. A arquitetura do Discriminador também é semelhante ao Discriminador padrão, com exceção de que ele da como saída um \"mapa de escores\", ao invés de um escore único. Essa técnica ficou conhecida como PatchGAN, porque em cada localidade desse mapa de escores, o Discriminador está avaliando um patch (corte) da imagem de entrada localmente, ao invés da imagem inteira."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv9anGQW-j88"
      },
      "source": [
        "# Initialize generator and discriminator\r\n",
        "generator = GeneratorUNet()\r\n",
        "discriminator = Discriminator()\r\n",
        "\r\n",
        "if cuda:\r\n",
        "    generator = generator.cuda()\r\n",
        "    discriminator = discriminator.cuda()\r\n",
        "    criterion_GAN.cuda()\r\n",
        "    criterion_pixelwise.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTVU3qj1-la_"
      },
      "source": [
        "# Initialize weights\r\n",
        "generator.apply(weights_init_normal)\r\n",
        "discriminator.apply(weights_init_normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRmoKQ-whSY"
      },
      "source": [
        "Definindo otimizadores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZPRFBPD-nuZ"
      },
      "source": [
        "# Optimizers\r\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\r\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS5PbZvuAiy6"
      },
      "source": [
        "Definindo os objetos para carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwI43HLb-qyl"
      },
      "source": [
        "# Configure dataloaders\r\n",
        "transforms_ = [\r\n",
        "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "]\r\n",
        "\r\n",
        "dataloader = DataLoader(\r\n",
        "    ImageDataset(\"../../data/%s\" % opt.dataset_name, transforms_=transforms_),\r\n",
        "    batch_size=opt.batch_size,\r\n",
        "    shuffle=True,\r\n",
        "    num_workers=opt.n_cpu,\r\n",
        ")\r\n",
        "\r\n",
        "val_dataloader = DataLoader(\r\n",
        "    ImageDataset(\"../../data/%s\" % opt.dataset_name, transforms_=transforms_, mode=\"val\"),\r\n",
        "    batch_size=10,\r\n",
        "    shuffle=True,\r\n",
        "    num_workers=1,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHLyv_ZK-sFI"
      },
      "source": [
        "# Tensor type\r\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1ORKgZBIYA"
      },
      "source": [
        "Função para mostrar imagens geradas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvU7p5M8-1gM"
      },
      "source": [
        "def sample_images(batches_done):\r\n",
        "\r\n",
        "    \"\"\"Saves a generated sample from the validation set\"\"\"\r\n",
        "    imgs = next(iter(val_dataloader))\r\n",
        "    real_A = Variable(imgs[\"B\"].type(Tensor))\r\n",
        "    real_B = Variable(imgs[\"A\"].type(Tensor))\r\n",
        "    fake_B = generator(real_A)\r\n",
        "    # img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\r\n",
        "    # save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\r\n",
        "    \r\n",
        "    fig, ax = plt.subplots(min(real_A.size(0), 2), 3, figsize=(12, 8))\r\n",
        "    \r\n",
        "    for i in range(min(real_A.size(0), 2)):\r\n",
        "    \r\n",
        "        ax[i, 0].imshow(real_A.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\r\n",
        "        ax[i, 0].set_yticks([])\r\n",
        "        ax[i, 0].set_xticks([])\r\n",
        "        ax[i, 0].set_title('Real [A]')\r\n",
        "        \r\n",
        "        ax[i, 1].imshow(fake_B.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\r\n",
        "        ax[i, 1].set_yticks([])\r\n",
        "        ax[i, 1].set_xticks([])\r\n",
        "        ax[i, 1].set_title('Fake [B]')\r\n",
        "        \r\n",
        "        ax[i, 2].imshow(real_B.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\r\n",
        "        ax[i, 2].set_yticks([])\r\n",
        "        ax[i, 2].set_xticks([])\r\n",
        "        ax[i, 2].set_title('Real [B]')\r\n",
        "    \r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1orSkADcBNv1"
      },
      "source": [
        "Definindo processo de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zdJwcDf8nMJ"
      },
      "source": [
        "################################################################################\r\n",
        "#  Training ####################################################################\r\n",
        "################################################################################\r\n",
        "\r\n",
        "prev_time = time.time()\r\n",
        "\r\n",
        "for epoch in range(opt.epoch, opt.n_epochs + 1):\r\n",
        "    \r\n",
        "    for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "        # Model inputs\r\n",
        "        real_A = Variable(batch[\"B\"].type(Tensor))\r\n",
        "        real_B = Variable(batch[\"A\"].type(Tensor))\r\n",
        "\r\n",
        "        # Adversarial ground truths\r\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\r\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\r\n",
        "\r\n",
        "        # ------------------\r\n",
        "        #  Train Generators\r\n",
        "        # ------------------\r\n",
        "\r\n",
        "        # Clearing gradients for G optimizer.\r\n",
        "        optimizer_G.zero_grad()\r\n",
        "\r\n",
        "        # GAN loss.\r\n",
        "        fake_B = generator(real_A)\r\n",
        "        pred_fake = discriminator(fake_B, real_A)\r\n",
        "        loss_GAN = criterion_GAN(pred_fake, valid)\r\n",
        "        \r\n",
        "        # Pixel-wise loss\r\n",
        "        loss_pixel = criterion_pixelwise(fake_B, real_B)\r\n",
        "\r\n",
        "        # Total loss\r\n",
        "        loss_G = loss_GAN + lambda_pixel * loss_pixel\r\n",
        "\r\n",
        "        # G backward and optimizer step.\r\n",
        "        loss_G.backward()\r\n",
        "        optimizer_G.step()\r\n",
        "\r\n",
        "        # ---------------------\r\n",
        "        #  Train Discriminator\r\n",
        "        # ---------------------\r\n",
        "\r\n",
        "        # Clearing gradients for D optimizer.\r\n",
        "        optimizer_D.zero_grad()\r\n",
        "\r\n",
        "        # Real loss\r\n",
        "        pred_real = discriminator(real_B, real_A)\r\n",
        "        loss_real = criterion_GAN(pred_real, valid)\r\n",
        "\r\n",
        "        # Fake loss\r\n",
        "        pred_fake = discriminator(fake_B.detach(), real_A)\r\n",
        "        loss_fake = criterion_GAN(pred_fake, fake)\r\n",
        "\r\n",
        "        # Total loss\r\n",
        "        loss_D = 0.5 * (loss_real + loss_fake)\r\n",
        "\r\n",
        "        # D backward and optimizer step.\r\n",
        "        loss_D.backward()\r\n",
        "        optimizer_D.step()\r\n",
        "\r\n",
        "        # --------------\r\n",
        "        #  Log Progress\r\n",
        "        # --------------\r\n",
        "\r\n",
        "        # Determine approximate time left\r\n",
        "        batches_done = epoch * len(dataloader) + i\r\n",
        "        batches_left = opt.n_epochs * len(dataloader) - batches_done\r\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\r\n",
        "        prev_time = time.time()\r\n",
        "\r\n",
        "        # If at sample interval save image\r\n",
        "        if batches_done % opt.sample_interval == 0:\r\n",
        "\r\n",
        "            # Print log\r\n",
        "            sys.stdout.write(\r\n",
        "                '[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s'\r\n",
        "                % (\r\n",
        "                    epoch,\r\n",
        "                    opt.n_epochs,\r\n",
        "                    i,\r\n",
        "                    len(dataloader),\r\n",
        "                    loss_D.item(),\r\n",
        "                    loss_G.item(),\r\n",
        "                    loss_pixel.item(),\r\n",
        "                    loss_GAN.item(),\r\n",
        "                    time_left,\r\n",
        "                )\r\n",
        "            )\r\n",
        "            \r\n",
        "            sample_images(batches_done)\r\n",
        "\r\n",
        "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\r\n",
        "        \r\n",
        "        # Save model checkpoints\r\n",
        "        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\r\n",
        "        torch.save(discriminator.state_dict(), \"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zcsh_5ZYbAA"
      },
      "source": [
        "# WGAN-GP\r\n",
        "\r\n",
        "Uma outra modificação na GAN padrão que surgiu com o objetivo de melhorar a estabilidade do treinamento foi a [Wasserstein-GAN](https://arxiv.org/abs/1701.07875). Essa modificação consiste em alterar a função de perda da GAN. No caso da WGAN, a função de perda mede o quanto o discriminador consegue separar dados reais e falsos sem considerar um \"limite\" de separação. Enquanto o discriminador puder aumentar mais o \"gap\" entre o score para dados reais e falsos ele vai aumentar. A vantagem disso é que a sua derivada tem valores significativos mesmo quando o discriminador é muito bom em separar os dados. Portanto o gradiente para o Gerador sempre é informativo. \r\n",
        "\r\n",
        "Uma restrição descrita no artigo original da WGAN era que para que as vantagens teóricas que a função de perda de Wasserstein se concretizassem, o discriminador precisava ser uma função *1-Lipchitz contínua*. Isso significa que o gradiente do discriminador com relação à imagem de entrada deve ter norma menor que 1. Para assegurar essa restrição, o artigo original propunha *clipar* os pesos do discriminador em valores em um intervalo pequeno (como $[-0.01, 0.01]$).\r\n",
        "\r\n",
        "Outro trabalho que extendeu o trabalho da WGAN propõe uma outra forma de assegurar essa restrição que funciona melhor: ter na função de perda um termo que penaliza a norma do gradiente de D ser diferente de 1:\r\n",
        "\r\n",
        "![WGAN-GP loss](https://i.imgur.com/fjyTgFi.png)\r\n",
        "\r\n",
        "Como essa função de perda proposta tem uma penalização para a norma do gradiente, esse trabalho ficou conhecido como [WGAN-GP](https://arxiv.org/abs/1704.00028) (*Wasserstein GAN Gradient Penalty*).\r\n",
        "\r\n",
        "![WGAN-GP](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/wgan_gp.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ygu7mHlChTQ"
      },
      "source": [
        "Importando bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gE_1DabCd_q"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import sys\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.autograd as autograd\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWwYIReY8DWi"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "opener = urllib.request.build_opener()\r\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\r\n",
        "urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBaM_VWtRtyb"
      },
      "source": [
        "Declarando hiperparâmetros usados para o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcdZLWTqEKA7"
      },
      "source": [
        "class Opt:\r\n",
        "    n_epochs = 25 \t\t\t## number of epochs of training\r\n",
        "    batch_size = 64 \t\t## size of the batches\r\n",
        "    lr = 0.0002 \t\t\t## adam: learning rate\r\n",
        "    b1 = 0.5 \t\t\t\t## adam: decay of first order momentum of gradient\r\n",
        "    b2 = 0.999\t\t\t\t## adam: decay of first order momentum of gradient\r\n",
        "    n_cpu = 8\t\t\t\t## number of cpu threads to use during batch generation\r\n",
        "    latent_dim = 100\t\t## dimensionality of the latent space\r\n",
        "    img_size = 28\t\t\t## size of each image dimension\r\n",
        "    channels = 1\t\t\t## number of image channels\r\n",
        "    n_critic = 5\t\t\t## number of training steps for discriminator per iter\r\n",
        "    clip_value = 0.01\t\t## lower and upper clip value for disc. weights\r\n",
        "    sample_interval = 400\t## interval betwen image samples\r\n",
        "\r\n",
        "opt = Opt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MaDalYR97E"
      },
      "source": [
        "Definindo a arquitetura do Gerador e do Discriminador. A princípio, a arquitetura dos modelos pode ser constituída de camadas convolucionais (ou convolucional transposta) como definimos nos modelos anteriores. Porém, nesse experimento vamos usar camadas *fully connected* tanto para o gerador quanto para o discriminador. Mesmo assim, como a função de perda da WGAN-GP é mais poderosa, o gerador é capaz de aprender a criar imagens que se parecem com os dados originais do MNIST em poucas épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L45_TITPEQQ0"
      },
      "source": [
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\r\n",
        "\r\n",
        "cuda = True if torch.cuda.is_available() else False\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "\r\n",
        "        def block(in_feat, out_feat, normalize=True):\r\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\r\n",
        "            if normalize:\r\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\r\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\r\n",
        "            return layers\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            *block(opt.latent_dim, 128, normalize=False),\r\n",
        "            *block(128, 256),\r\n",
        "            *block(256, 512),\r\n",
        "            *block(512, 1024),\r\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, z):\r\n",
        "        img = self.model(z)\r\n",
        "        img = img.view(img.shape[0], *img_shape)\r\n",
        "        return img\r\n",
        "\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            nn.Linear(512, 256),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            nn.Linear(256, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, img):\r\n",
        "        img_flat = img.view(img.shape[0], -1)\r\n",
        "        validity = self.model(img_flat)\r\n",
        "        return validity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch1v-sl9ESTZ"
      },
      "source": [
        "# Loss weight for gradient penalty\r\n",
        "lambda_gp = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXJQxqvcTGlG"
      },
      "source": [
        "Instanciando o gerador e o discriminador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YozvxLZ4EVKd"
      },
      "source": [
        "# Initialize generator and discriminator\r\n",
        "generator = Generator()\r\n",
        "discriminator = Discriminator()\r\n",
        "\r\n",
        "if cuda:\r\n",
        "    generator.cuda()\r\n",
        "    discriminator.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7pdZxBuTXer"
      },
      "source": [
        "Criando os DataLoaders necessários para carregar os dados do MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d75bGXxkEXb_"
      },
      "source": [
        "# Configure data loader\r\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\r\n",
        "dataloader = torch.utils.data.DataLoader(\r\n",
        "    datasets.MNIST(\r\n",
        "        \"../../data/mnist\",\r\n",
        "        train=True,\r\n",
        "        download=True,\r\n",
        "        transform=transforms.Compose(\r\n",
        "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\r\n",
        "        ),\r\n",
        "    ),\r\n",
        "    batch_size=opt.batch_size,\r\n",
        "    shuffle=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qEMPHk9TfDb"
      },
      "source": [
        "Definindo os otimizadores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvlMUKO7EYeS"
      },
      "source": [
        "# Optimizers\r\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\r\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hRrlPVTEaFH"
      },
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBMlsRGPThrj"
      },
      "source": [
        "Função para calcular o *gradient penalty*. Como o PyTorch já possui um módulo responsável pelos processamentos necessários para a diferenciação automática, podemos calcular essa penalização (e a sua derivada) através de funções do próprio [`autograd`](https://pytorch.org/docs/stable/autograd.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6LZHsCZEcxh"
      },
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\r\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\r\n",
        "    \r\n",
        "    # Random weight term for interpolation between real and fake samples\r\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\r\n",
        "    \r\n",
        "    # Get random interpolation between real and fake samples\r\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\r\n",
        "    d_interpolates = D(interpolates)\r\n",
        "    fake = Tensor(real_samples.shape[0], 1).fill_(1.0)\r\n",
        "    fake.requires_grad_(False)\r\n",
        "    \r\n",
        "    # Get gradient w.r.t. interpolates\r\n",
        "    gradients = autograd.grad(\r\n",
        "        outputs=d_interpolates,\r\n",
        "        inputs=interpolates,\r\n",
        "        grad_outputs=fake,\r\n",
        "        create_graph=True,\r\n",
        "        retain_graph=True,\r\n",
        "        only_inputs=True,\r\n",
        "    )[0]\r\n",
        "    gradients = gradients.view(gradients.size(0), -1)\r\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\r\n",
        "    return gradient_penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8k50hYmbDI-"
      },
      "source": [
        "Procedimentos de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGzsIDp2YlaQ"
      },
      "source": [
        "# ----------\r\n",
        "#  Training\r\n",
        "# ----------\r\n",
        "\r\n",
        "batches_done = 0\r\n",
        "for epoch in range(opt.n_epochs):\r\n",
        "    for i, (imgs, _) in enumerate(dataloader):\r\n",
        "\r\n",
        "        # Configure input\r\n",
        "        real_imgs = Variable(imgs.type(Tensor))\r\n",
        "\r\n",
        "        # ---------------------\r\n",
        "        #  Train Discriminator\r\n",
        "        # ---------------------\r\n",
        "\r\n",
        "        optimizer_D.zero_grad()\r\n",
        "\r\n",
        "        # Sample noise as generator input\r\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\r\n",
        "\r\n",
        "        # Generate a batch of images\r\n",
        "        fake_imgs = generator(z)\r\n",
        "\r\n",
        "        # Real images\r\n",
        "        real_validity = discriminator(real_imgs)\r\n",
        "        # Fake images\r\n",
        "        fake_validity = discriminator(fake_imgs)\r\n",
        "        # Gradient penalty\r\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\r\n",
        "        # Adversarial loss\r\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\r\n",
        "\r\n",
        "        d_loss.backward()\r\n",
        "        optimizer_D.step()\r\n",
        "\r\n",
        "        optimizer_G.zero_grad()\r\n",
        "\r\n",
        "        # Train the generator every n_critic steps\r\n",
        "        if i % opt.n_critic == 0:\r\n",
        "\r\n",
        "            # -----------------\r\n",
        "            #  Train Generator\r\n",
        "            # -----------------\r\n",
        "\r\n",
        "            # Generate a batch of images\r\n",
        "            fake_imgs = generator(z)\r\n",
        "            # Loss measures generator's ability to fool the discriminator\r\n",
        "            # Train on fake images\r\n",
        "            fake_validity = discriminator(fake_imgs)\r\n",
        "            g_loss = -torch.mean(fake_validity)\r\n",
        "\r\n",
        "            g_loss.backward()\r\n",
        "            optimizer_G.step()\r\n",
        "\r\n",
        "        if batches_done % opt.sample_interval == 0:\r\n",
        "            # save_image(fake_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\r\n",
        "            \r\n",
        "            print(\r\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\r\n",
        "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\r\n",
        "            )\r\n",
        "\r\n",
        "            nrow_, ncol_ = 2, 3\r\n",
        "            c = 0\r\n",
        "            fig, ax = plt.subplots(nrows=nrow_, ncols=ncol_, figsize=(8, 6))\r\n",
        "            for i in range(nrow_):\r\n",
        "                for j in range(ncol_):                    \r\n",
        "                    ax[i, j].imshow(fake_imgs.data[c].detach().cpu().numpy().squeeze() * 0.5 + 0.5)\r\n",
        "                    ax[i, j].set_yticks([])\r\n",
        "                    ax[i, j].set_xticks([])\r\n",
        "                    c += 1\r\n",
        "            \r\n",
        "            plt.show()\r\n",
        "\r\n",
        "        batches_done += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}