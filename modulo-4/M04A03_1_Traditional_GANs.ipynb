{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04A03_1 - Traditional GANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0o-c0P9wcM"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ATPZX-9qXK"
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJgxyO-9z-4"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 50,      # Number of epochs.\n",
        "    'lr': 5e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-5, # L2 penalty.\n",
        "    'num_workers': 8,     # Number of workers on data loader.\n",
        "    'batch_size': 500,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'z_dim': 100,         # Dimension of z input vector.\n",
        "    'num_samples': 4,     # Number of samples to be generated in evaluation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZA6okfQ98TP"
      },
      "source": [
        "# Carregando o MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDuTqmwgBffF"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "opener = urllib.request.build_opener()\r\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\r\n",
        "urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQAwioI99NJ"
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvzdYk6v-gdd"
      },
      "source": [
        "# Definindo as arquiteturas de $D$ e $G$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AExpivZsGO4"
      },
      "source": [
        "Nesse primeiro notebook, vamos ver como um gerador bastante básico desempenha a função de gerar uma imagem. Nesse caso nosso gerador terá apenas camadas densas (*fully-connected*), recebendo como entrada um vetor de ruído gaussiano $z$ e dando como saída uma imagem de dimensões $(28 \\times 28)$. Nesse caso a arquitetura já estará implementada e é constituída de 3 camadas, em que:\r\n",
        "\r\n",
        "1. A primeira camada recebe como entrada o ruído $z$ de tamanho $100$ e retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\r\n",
        "1. A segunda camada recebe como entrada a saída da camanda anterior e também retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\r\n",
        "1. A terceira camada recebe como entrada a saída da camanda anterior e retorna uma saída de tamanho $784 = 1 \\times 28 \\times 28$. Depois disso temos uma ativação sigmóide.\r\n",
        "\r\n",
        "A sigmóide é usada porque nossas imagens retornadas do conjunto de dados estão normalizadas para o intervalo $[0, 1]$, através do `transforms.ToTensor()`. Portanto, nossas imagens geradas também precisam estar nessa mesma faixa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FJQQU7-kP5"
      },
      "source": [
        "# Adversarial Generator.\n",
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=100, output_channels=1, img_size=28):\n",
        "    \n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.img_size = img_size\n",
        "        img_size_flat = self.img_size * self.img_size * self.output_channels\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, img_size_flat),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "                \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.output_channels, self.img_size, self.img_size)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiating G.\n",
        "net_G = Generator(input_dim=args['z_dim']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsrfQ2eOsEZB"
      },
      "source": [
        "A arquitetura do discriminador será uma CNN padrão. Nesta GAN básica, essa arquitetura não tem nada de diferente das outras redes convolucionais que vimos, até porque o trabalho dela ainda é receber uma imagem como entrada e dar um *score* único na saída. A única particularidade é que nesse caso ao invés desse *score* ser a probabilidade da imagem ser de uma dada classe, é a probabilidade da imagem ser real. Portanto, podemos pensar que é como um problema de classificação binária, em que as classes das imagens são `real` e `falsa`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pai5kiN6DW2R"
      },
      "source": [
        "# Adversarial Discriminator.\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=1, output_channels=1, input_size=28):\n",
        "        \n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_dim,\n",
        "                      64,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64,\n",
        "                      128,\n",
        "                      kernel_size=(4, 4),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, self.output_channels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
        "        \n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiating D.\n",
        "net_D = Discriminator().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzlZ8vsUlycb"
      },
      "source": [
        "# Atividade Prática: Implementando uma GAN\r\n",
        "\r\n",
        "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\r\n",
        "\r\n",
        "1.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\r\n",
        "1.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\r\n",
        "1.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\r\n",
        "1. Complemente a função *train()*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeUPraJ3EyeP"
      },
      "source": [
        "# Definindo o otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83m3L4FEzgY"
      },
      "source": [
        "# TO DO: defining optimizer for G.\n",
        "opt_G = # ...\n",
        "\n",
        "# TO DO: defining optimizer for D.\n",
        "opt_D = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv0tdlhKQYOt"
      },
      "source": [
        "# Definindo um Scheduler para os Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnD3mr9Qef6"
      },
      "source": [
        "# TO DO: defining lr scheduler.\n",
        "scheduler_G = # ...\n",
        "scheduler_D = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Cvxt95E-A0"
      },
      "source": [
        "# Definindo a loss composta\n",
        "\n",
        "Como foi dito na parte da definição do discriminador, a tarefa do discriminador nas GANs pode ser vista como uma tarefa de classificação binária. Portanto, na definição dessa função de perda, podemos usar a mesma loss de entropia cruzada binária que usamos nesses problemas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8SIpVnFCID"
      },
      "source": [
        "# TO DO: defining adversarial loss.\n",
        "criterion = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gUQrX7Fvf-"
      },
      "source": [
        "# Criando funções para Treino e Teste\r\n",
        "\r\n",
        "O processo de treinamento das GANs é bastante diferente do treinamento de uma rede padrão para classificação. Porém, podemos \"separar\" o treinamento das GANs em duas partes, o que deixa o entendimento e a implementação mais fácil. Na primeira parte, consideramos que o Gerador é fixo, e atualizamos apenas o Discriminador; na segunda parte fazemos um processo análogo: consideramos que o Discriminador é fixo e atualizamos apenas o Gerador. Essas duas etapas podem ser detalhadas da seguinte forma:\r\n",
        "\r\n",
        "## Atualizando o discriminador\r\n",
        "\r\n",
        "Nessa etapa, consideramos que o Gerador é fixo, e portanto atualizamos apenas o Discriminador. Assim, essa etapa se torna ainda mais parecida de um treinamento de uma rede para classificação binária. Podemos pensar que as imagens que o Discriminador deve separar vêm de duas \"fontes de dados\" diferentes. As imagens reais vêm do conjunto de dados, e as imagens falsas vêm do Gerador. Portanto, precisamos treinar o Discriminador a classificar de qual dessas duas fontes de dados cada imagem vêm. Mais especificamente, nessa etapa seguimos os seguintes passos:\r\n",
        "\r\n",
        "1. Como as iamgens reais já são dadas pelo dataset, podemos primeiramente obter o *score* do Discriminador para essas imagens. Para isso, precisamos apenas passar as imagens reais pelo Discriminador.\r\n",
        "1. Obtemos a perda para as imagens reais. Queremos que o Discriminador classifique os dados reais como a classe `real`, que é representada pelos labels `y = 1`. Portanto, essa etapa consiste em calcular o erro de entropia cruzada entre os *scores* obtidos para as imagens reais e um tensor de 1's (que representa os labels das imagens reais). Essa perda calculada será uma das duas losses que usaremos para atualizar o Discriminador.\r\n",
        "1. Obtemos as imagens falsas: precisamos obter imagens falsas através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\r\n",
        "1. Obtemos os *scores* do Discriminador para as imagens falsas. Essa etapa parece com a etapa 1, com a diferença que são imagens falsas.\r\n",
        "1. Obtemos a perda para as imagens falsas. Como queremos que o Discriminador classifique essas imagens como `falso`, então calculamos a perda considerando que os labels dessas imagens são todos 0's. Essa etapa parece com a etapa 2, com a exceção que os labels serão todos zeros nesse caso. Essa loss será a segunda usada para compor a perda final do Discriminador.\r\n",
        "1. Por fim, agora que já temos a loss para imagens reais e para as falsas, podemos obter a loss final simplesmente somando as duas. Assim, podemos calcular o backpropagation e a usar o otimizador do Discriminador para atualizar os pesos.\r\n",
        "\r\n",
        "## Atualizando o gerador\r\n",
        "\r\n",
        "Nessa etapa consideramos que o Discriminador é fixo, e portanto atualizamos apenas o Gerador. De um certo ponto de vista, podemos pensar que o Gerador é uma rede que faz o trabalho de geração de dados, e o Discriminador é uma função matemática $f(x)$ fixa, completamente diferenciável, que funciona como uma função de perda para essa nossa tarefa de \"geração de dados\". O que queremos para atualizar o Gerador é: dado as imagens criadas pelo Gerador, queremos que a saída dada pelo Discriminador nessas imagens seja o mais próximo da classe `real` possível, porque isso significa que o Gerador consegue enganar o Discriminador. Portanto, usamos a loss de entropia cruzada, mas usamos como labels das imagens **falsas** o valor `y = 1`, porque nessa etapa o Gerador que estamos atualizando.\r\n",
        "\r\n",
        "Seguimos os seguintes passos:\r\n",
        "\r\n",
        "1. Obtemos as imagens falsas, através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\r\n",
        "1. Obtemos os *scores* do Discriminador para as imagens falsas.\r\n",
        "1. Obtemos a perda para as imagens falsas. Nesse caso, queremos que o Discriminador classifique essas imagens como `real` porque estamos atualizando **o Gerador**, então calculamos a perda considerando que os labels dessas imagens são todos 1's.\r\n",
        "1. Por fim, agora que já temos a loss para o Gerador (no caso do Gerador é penas uma), podemos calcular o backpropagation e a usar o otimizador do Gerador para atualizar os pesos.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUHQYKgFxz3"
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader,\n",
        "          net_G, net_D,\n",
        "          criterion,\n",
        "          opt_G, opt_D,\n",
        "          epoch,\n",
        "          train_loss_G, train_loss_D):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    # Predefining ones and zeros for batches.\n",
        "    y_real = torch.ones(args['batch_size'], 1).to(args['device'])\n",
        "    y_fake = torch.zeros(args['batch_size'], 1).to(args['device'])\n",
        "\n",
        "    # Setting networks for training mode.\n",
        "    net_D.train()\n",
        "    net_G.train()\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        # Obtaining images and labels for batch.\n",
        "        x, labs = batch_data\n",
        "        \n",
        "        # TO DO: Creating random vector z.\n",
        "        z = # ...\n",
        "        \n",
        "        # TO DO: Casting to correct device.\n",
        "        x = # ...\n",
        "        z = # ...\n",
        "        \n",
        "        ###############\n",
        "        # Updating D. #\n",
        "        ###############\n",
        "        \n",
        "        # TO DO: Clearing the gradients of D optimizer.\n",
        "\n",
        "        # TO DO: Forwarding real data.\n",
        "        D_real = # ...\n",
        "        \n",
        "        # TO DO: Computing loss for real data.\n",
        "        D_real_loss = # ...\n",
        "\n",
        "        # TO DO: Forwarding fake data.\n",
        "        G_out = # ...\n",
        "        D_fake = # ...\n",
        "        \n",
        "        # TO DO: Computing loss for fake data.\n",
        "        D_fake_loss = # ...\n",
        "\n",
        "        # TO DO: Computing total loss for D.\n",
        "        D_loss = # ...\n",
        "        \n",
        "        # TO DO: Computing backpropagation for D.\n",
        "        \n",
        "        # TO DO: Taking step in D optimizer.\n",
        "\n",
        "        ###############\n",
        "        # Updating G. #\n",
        "        ###############\n",
        "        \n",
        "        # TO DO: Clearing the gradients of G optimizer.\n",
        "\n",
        "        # TO DO: Forwarding fake data.\n",
        "        G_out = # ...\n",
        "        D_fake = # ...\n",
        "        \n",
        "        # TO DO: Computing loss for G.\n",
        "        G_loss = # ...\n",
        "        \n",
        "        # TO DO: Computing backpropagation for G.\n",
        "        \n",
        "        # TO DO: Taking step in G optimizer.\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss_G.append(G_loss.data.item())\n",
        "        train_loss_D.append(D_loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Printing training epoch loss.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [training time %.2f]' % (\n",
        "        epoch, (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    \n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Plotting losses.\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "        ax[0].plot(np.asarray(train_loss_G), 'r-', label='G loss')\n",
        "        ax[0].legend()\n",
        "        \n",
        "        ax[1].plot(np.asarray(train_loss_D), 'b--', label='D loss')\n",
        "        ax[1].legend()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "    return train_loss_G, train_loss_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2N1kgQNMTl"
      },
      "source": [
        "# Test procedure.\n",
        "def test(net_G, criterion, epoch):\n",
        "    \n",
        "    # Setting networks for training mode.\n",
        "    net_D.eval()\n",
        "    net_G.eval()\n",
        "    \n",
        "    # Creating random vector z.\n",
        "    z = torch.rand((args['num_samples'] * args['num_samples'], args['z_dim']))\n",
        "    \n",
        "    # Casting to correct device.\n",
        "    z = z.to(args['device'])\n",
        "    \n",
        "    # Generating new samples.\n",
        "    G_out = net_G(z)\n",
        "    \n",
        "    # Plotting.\n",
        "    fig, ax = plt.subplots(args['num_samples'], args['num_samples'], figsize=(16, 16))\n",
        "    \n",
        "    for i in range(args['num_samples']):\n",
        "        \n",
        "        for j in range(args['num_samples']):\n",
        "            \n",
        "            sample = G_out[j * args['num_samples'] + i]\n",
        "            \n",
        "            ax[j, i].imshow(sample.detach().cpu().numpy().squeeze(), cmap=plt.get_cmap('gray'))\n",
        "            ax[j, i].set_yticks([])\n",
        "            ax[j, i].set_xticks([])\n",
        "            \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhC6uqUpJO8Y"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbf0HVDJKTC"
      },
      "source": [
        "# Lists for losses.\n",
        "train_loss_G = []\n",
        "train_loss_D = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train_loss_G, train_loss_D = train(train_loader,\n",
        "                                       net_G, net_D,\n",
        "                                       criterion,\n",
        "                                       opt_G, opt_D,\n",
        "                                       epoch,\n",
        "                                       train_loss_G, train_loss_D)\n",
        "    \n",
        "    # Taking step on scheduler.\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    # Testing function for sample generation.\n",
        "    test(net_G, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}