{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"S05A01_Distributed_Representation(Gabarito).ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"YV37HIeFpMG2"},"source":["# Preâmbulo"]},{"cell_type":"code","metadata":{"id":"MhRxuCa9pMHH"},"source":["!wget https://www.dropbox.com/s/f8k3xoywff0h3br/questions-words.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXwJx2tzpMHK"},"source":["from sklearn.manifold import TSNE\n","import pandas as pd\n","import numpy as np\n","import string\n","\n","import torch\n","from torch import nn\n","### LIB DE TEXTO DO PYTORCH\n","import torchtext\n","from torchtext import data\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style('darkgrid')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cKDHOxVApMHL"},"source":["## Analogias\n","\n","Uma forma intrínseca de avaliar a qualidade de um modelo de linguagem é realizar as chamadas analogias a partir das **representações distribuídas** gerada pelo modelo.\n","\n","Analogias são associações de mesma natureza entre palavras (como flexões de gênero ou número). A geometria dessas associações pode ser visualizada no espaço vetorial onde as palavras são projetadas e, em modelos bem treinados, deve ser possível encontrar semelhanças entre associações de mesma natureza.\n","<img width=600 src=\"https://vecto.space/assets/img/queen.png\">\n","\n","Vamos trabalhar com um popular conjunto de validação através do método de analogias. Ele consiste em pares de associações, onde o primeiro par deve ser usado como referência para completar o segundo par. No exemplo a seguir, Brasil é a palavra que deve ser inferida a partir das três palavras marcadas em negrito. \n","> **Buenos Aires** está para **Argentina** assim como **Brasília** está para <span style=\"color:red\"><u>**Brasil**<u></span>\n","    \n","Na prática, essa é a composição do conjunto `questions-words` para validação de modelos de linguagem."]},{"cell_type":"code","metadata":{"id":"zVP4ftpWpMHN","outputId":"a5bfe684-719e-468c-a58c-cc59af4550b3"},"source":["df = pd.read_csv('questions-words.csv')\n","display(df.head(10))\n","display(df.tail(10))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>analogy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Athens Greece Baghdad</td>\n","      <td>Iraq</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Athens Greece Bangkok</td>\n","      <td>Thailand</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Athens Greece Beijing</td>\n","      <td>China</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Athens Greece Berlin</td>\n","      <td>Germany</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Athens Greece Bern</td>\n","      <td>Switzerland</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Athens Greece Cairo</td>\n","      <td>Egypt</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Athens Greece Canberra</td>\n","      <td>Australia</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Athens Greece Hanoi</td>\n","      <td>Vietnam</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Athens Greece Havana</td>\n","      <td>Cuba</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Athens Greece Helsinki</td>\n","      <td>Finland</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    input      analogy\n","0   Athens Greece Baghdad         Iraq\n","1   Athens Greece Bangkok     Thailand\n","2   Athens Greece Beijing        China\n","3    Athens Greece Berlin      Germany\n","4      Athens Greece Bern  Switzerland\n","5     Athens Greece Cairo        Egypt\n","6  Athens Greece Canberra    Australia\n","7     Athens Greece Hanoi      Vietnam\n","8    Athens Greece Havana         Cuba\n","9  Athens Greece Helsinki      Finland"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>analogy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>19534</th>\n","      <td>write writes sing</td>\n","      <td>sings</td>\n","    </tr>\n","    <tr>\n","      <th>19535</th>\n","      <td>write writes sit</td>\n","      <td>sits</td>\n","    </tr>\n","    <tr>\n","      <th>19536</th>\n","      <td>write writes slow</td>\n","      <td>slows</td>\n","    </tr>\n","    <tr>\n","      <th>19537</th>\n","      <td>write writes speak</td>\n","      <td>speaks</td>\n","    </tr>\n","    <tr>\n","      <th>19538</th>\n","      <td>write writes swim</td>\n","      <td>swims</td>\n","    </tr>\n","    <tr>\n","      <th>19539</th>\n","      <td>write writes talk</td>\n","      <td>talks</td>\n","    </tr>\n","    <tr>\n","      <th>19540</th>\n","      <td>write writes think</td>\n","      <td>thinks</td>\n","    </tr>\n","    <tr>\n","      <th>19541</th>\n","      <td>write writes vanish</td>\n","      <td>vanishes</td>\n","    </tr>\n","    <tr>\n","      <th>19542</th>\n","      <td>write writes walk</td>\n","      <td>walks</td>\n","    </tr>\n","    <tr>\n","      <th>19543</th>\n","      <td>write writes work</td>\n","      <td>works</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     input   analogy\n","19534    write writes sing     sings\n","19535     write writes sit      sits\n","19536    write writes slow     slows\n","19537   write writes speak    speaks\n","19538    write writes swim     swims\n","19539    write writes talk     talks\n","19540   write writes think    thinks\n","19541  write writes vanish  vanishes\n","19542    write writes walk     walks\n","19543    write writes work     works"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"IYImMUqapMHP"},"source":["## Torchtext\n","Documentação: https://pytorch.org/text/stable/index.html\n","\n","Similar ao `torchvision` para imagens, o pacote torchtext facilita o trabalho com dados textuais. Em sua documentação é possível explorar toda a sua gama de possibilidades, entre modelos pré-treinados, métricas, ferramentas, datasets, etc.\n","\n","Aqui vamos conhecer dois elementos importantes para o carregamento de dados.\n","\n","\n","### Field\n","\n","Objeto que carrega informações de como os dados devem ser processados. A seguir temos a assinatura da sua classe com alguns exemplos de parâmetros que podemos controlar. \n","\n","```python\n","torchtext.data.Field(dtype=torch.int64, preprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, stop_words=None, is_target=False)\n","```\n","\n","No nosso caso, ambos entrada e saída são sequências de caracteres que passarão pelo mesmo pré-processamento:\n","* `tokenize`: Separação em **tokens**. Por padrão o Field realiza a tokenização `string.split`\n","    * Ex: \"Bom dia Brasil!\" $\\rightarrow$ `[\"Bom\", \"dia\", \"Brasil\", \"!\"]` <br><br>\n","    \n","* `lower`: Conversão para letras minúsculas, assim evitamos duplicidade de palavras (Atenas $\\neq$ atenas).\n","\n","\n","### TabularDataset\n","\n","É simples carregar dados tabulares utilizando a classe `TabularDataset`. Basta informar:\n","* `path`: O caminho do sistema onde o arquivo se encontra <br><br>\n","* `format`: A formatação do arquivo (csv, tsv, json) <br><br>\n","* `fields`: Lista de tuplas `(nome, Field)` representando respectivamente o nome associado a cada coluna da sua tabela e o pré-processamento que os dados devem receber. <br><br>\n","* `skip_header`: Se o seu arquivo possui uma linha de cabeçalho, você pode removê-la definindo esse parâmetro como `True`.\n","\n","```python\n","torchtext.data.TabularDataset(path, format, fields, skip_header=False)\n","```"]},{"cell_type":"code","metadata":{"id":"pYuogpWhpMHR","outputId":"6924dd77-8cd7-47f3-e36f-855d22df09d0"},"source":["INPUT  = data.Field(lower = True)\n","TARGET = data.Field(lower = True)\n","\n","dataset = data.TabularDataset('questions-words.csv', format='csv',\n","                                           fields=[('input', INPUT), ('target',TARGET)],\n","                                           skip_header=True\n","                                        )\n","\n","for k, sample in enumerate(dataset):\n","    if k % 1000 == 0:\n","        print(k, vars(sample))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 {'input': ['athens', 'greece', 'baghdad'], 'target': ['iraq']}\n","1000 {'input': ['baku', 'azerbaijan', 'dushanbe'], 'target': ['tajikistan']}\n","2000 {'input': ['dublin', 'ireland', 'kathmandu'], 'target': ['nepal']}\n","3000 {'input': ['lusaka', 'zambia', 'tehran'], 'target': ['iran']}\n","4000 {'input': ['rome', 'italy', 'windhoek'], 'target': ['namibia']}\n","5000 {'input': ['zagreb', 'croatia', 'astana'], 'target': ['kazakhstan']}\n","6000 {'input': ['philadelphia', 'pennsylvania', 'tampa'], 'target': ['florida']}\n","7000 {'input': ['wichita', 'kansas', 'shreveport'], 'target': ['louisiana']}\n","8000 {'input': ['shreveport', 'louisiana', 'oxnard'], 'target': ['california']}\n","9000 {'input': ['complete', 'completely', 'lucky'], 'target': ['luckily']}\n","10000 {'input': ['comfortable', 'uncomfortable', 'clear'], 'target': ['unclear']}\n","11000 {'input': ['good', 'better', 'high'], 'target': ['higher']}\n","12000 {'input': ['young', 'younger', 'tight'], 'target': ['tighter']}\n","13000 {'input': ['weak', 'weakest', 'bright'], 'target': ['brightest']}\n","14000 {'input': ['slow', 'slowing', 'describe'], 'target': ['describing']}\n","15000 {'input': ['ireland', 'irish', 'greece'], 'target': ['greek']}\n","16000 {'input': ['feeding', 'fed', 'sitting'], 'target': ['sat']}\n","17000 {'input': ['slowing', 'slowed', 'decreasing'], 'target': ['decreased']}\n","18000 {'input': ['finger', 'fingers', 'onion'], 'target': ['onions']}\n","19000 {'input': ['play', 'plays', 'sing'], 'target': ['sings']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dGwBs08spMHS"},"source":["## Representando os dados como Tensores\n","\n","Para transformar palavras em dados numéricos, uma solução muito utilizada é mapeá-las em um dicionário contendo o vocabulário completo do conjunto. \n","\n","<img src=\"https://static.packt-cdn.com/products/9781786465825/graphics/B05525_03_01.jpg\" width=\"500\">\n","\n","A depender da quantidade de palavras em seu vocabulário, uma representação *One-Hot* pode ser computacionalmente inviável. Por isso utilizamos as **representações distribuídas**, associando vetores densos a cada palavra de nosso dicionário de modo que esse espaço vetorial aproxime palavras que costumam aparecer no mesmo contexto.\n","\n","Vamos explorar algumas maneiras de transformar palavras em representações distribuídas. A principal é a partir de **modelos de linguagem pré-treinados**. Através do pacote `torchtext` é possível consultar todos os modelos ali disponíveis.\n","\n","Algumas nomenclaturas comuns são:\n","\n","* charngram.**100d**: Indica que a representação desse modelo possui 100 dimensões. <br>\n","* glove.**6B**.300d: Indica que o modelo foi treinado com 6 Bilhões de tokens."]},{"cell_type":"code","metadata":{"id":"c7WQDq-WpMHT","outputId":"b94329af-e57b-4dd2-8482-7afcb523c6b3"},"source":["torchtext.vocab.pretrained_aliases.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d'])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jZ8J8KmQpMHU"},"source":["Vamos explorar por exemplo o modelo `glove.6B.100d`"]},{"cell_type":"code","metadata":{"id":"Hml-ePoipMHV","outputId":"117e8c12-74c2-4761-b89f-60992dd7496d"},"source":["glove = torchtext.vocab.GloVe(name='6B', dim=100)\n","print(\"Um total de %d tokens são mapeados por esse modelo.\"% len(glove.stoi))\n","print(\"Os 10 primeiros tokens são\", glove.itos[:10])\n","print(\"A dimensionalidade da matriz de representação é:\", glove.vectors.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Um total de 400000 tokens são mapeados por esse modelo.\n","Os 10 primeiros tokens são ['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]\n","A dimensionalidade da matriz de representação é: torch.Size([400000, 100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DaAbkR8rpMHW"},"source":["Usando o objeto do tipo `Field` podemos **construir um vocabulário** contendo somente as palavras (e vetores) relevantes para o nosso problema."]},{"cell_type":"code","metadata":{"id":"ZJ2lDVLrpMHX","outputId":"981a9d91-bae8-46c2-a5d3-92e6325ce45e"},"source":["MAX_VOCAB_SIZE=1000\n","INPUT.build_vocab(dataset, \n","                  max_size=MAX_VOCAB_SIZE,\n","                  vectors='glove.6B.100d')\n","\n","print(\"Um total de %d tokens são mapeados por esse vocabulário.\"% len(INPUT.vocab.stoi))\n","print(\"Os 10 primeiros tokens são\", INPUT.vocab.itos[:10])\n","\n","print('\\nÍndice da palavra \"fast\" no dicionário:', INPUT.vocab.stoi['fast'])\n","print('Palavra do índice 100 do dicionário:', INPUT.vocab.itos[100])\n","\n","print('\\nDimensionalidade da representação distribuída:', INPUT.vocab.vectors.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Um total de 907 tokens são mapeados por esse vocabulário.\n","Os 10 primeiros tokens são ['<unk>', '<pad>', 'california', 'texas', 'slow', 'florida', 'arizona', 'quick', 'japan', 'russia']\n","\n","Índice da palavra \"fast\" no dicionário: 36\n","Palavra do índice 100 do dicionário: sing\n","\n","Dimensionalidade da representação distribuída: torch.Size([907, 100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qwa3xwzcpMHY"},"source":["## Visualizando o espaço vetorial\n","\n","Como seria impraticável visualizar um espaço vetorial de centenas de dimensões, um artifício muito utilizado é a abordagem de redução de dimensionalidade intitulada **t-distributed Stochastic Neighbor Embedding (tSNE)**.\n","\n","O pacote Scikit-Learn nos traz essa funcionalidade de forma simplificada. Caso queira entender melhor o funcionamento desse método, recomendo a leitura [da documentação](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)."]},{"cell_type":"code","metadata":{"id":"pX25RC1jpMHZ"},"source":["vectors2d = TSNE(n_components=2).fit_transform(INPUT.vocab.vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_Pb-DStpMHZ","outputId":"40b65cb4-807f-49d3-b3aa-edf6f2ac2dc0"},"source":["fig, ax = plt.subplots(figsize=(7, 7))\n","\n","examples = [11000,11080, 11102]\n","\n","for example in examples:\n","    print(vars(dataset[example]))\n","    sample = dataset[example]\n","    entrada = sample.input\n","    analogia = sample.target[0]\n","\n","    in_vec  = np.asarray([vectors2d[INPUT.vocab.stoi[e]] for e in entrada])\n","    out_vec = vectors2d[INPUT.vocab.stoi[analogia]]\n","\n","\n","    ax.scatter(in_vec[[0,2], 0], in_vec[[0,2], 1], s=30, color='dodgerblue')\n","    ax.scatter(in_vec[1, 0], in_vec[1, 1], s=30, color='r')\n","    ax.scatter(out_vec[0], out_vec[1], s=30, color='r')\n","\n","    for i, word in enumerate(entrada):\n","        ax.text(in_vec[i,0]+0.2, in_vec[i,1], word, fontsize=14 )\n","    ax.text(out_vec[0]+0.2, out_vec[1], analogia, fontsize=14 )\n","        \n","        \n","plt.show()        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input': ['good', 'better', 'high'], 'target': ['higher']}\n","{'input': ['hard', 'harder', 'sharp'], 'target': ['sharper']}\n","{'input': ['hard', 'harder', 'fast'], 'target': ['faster']}\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 504x504 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"NfgkGOtJpMHb"},"source":["## Criando analogias\n","\n","Como dissemos, em um modelo pré-treinado, a geometria do espaço vetorial possui similaridades entre associações de mesma natureza. Podemos explorar essa característica para validar a qualidade de um modelo.\n","\n","Dado um conjunto de 3 palavras da nossa entrada, exemplo: <br>\n","`palavras = ['man', 'king', 'woman'`]\n","\n","Podemos predizer a associaçao entre o primeiro par de palavras: <br>\n","`associacao = palavra[1] - palavra[0]`\n","\n","e projetar essa associação na terceira palavra: <br>\n","`projecao = palavra[2] + associacao`\n","\n","Essa projeção é um vetor no espaço da representação distribuída que deve estar na vizinhança da analogia que buscamos, nesse caso a palavra `queen`.\n","\n","<img width=400 src=\"https://pbs.twimg.com/media/DKWbi9nXoAAd_un.jpg\">"]},{"cell_type":"code","metadata":{"id":"3ZsmhX4rpMHd"},"source":["def get_analogy(token_a, token_b, token_c, embed):\n","    \n","    vecs = [embed.vectors[embed.stoi[t]] \n","                for t in [token_a, token_b, token_c]]\n","    \n","    analogy = vecs[1] - vecs[0] + vecs[2]\n","    \n","    distances = np.dot(embed.vectors, analogy) / np.linalg.norm(embed.vectors)\n","    best = np.argsort(distances)\n","    best = [embed.itos[best[k]] for k in range(-1, -4, -1) if embed.itos[best[k]] not in [token_a, token_b, token_c]]\n","\n","    return best[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjLPEUe9pMHd","outputId":"1d24fa0f-d80a-4e69-f95a-272198833387"},"source":["idx = 100\n","print(vars(dataset[idx]))\n","words, analogy = dataset[idx].input, dataset[idx].target\n","\n","prediction = get_analogy(words[0], words[1], words[2], INPUT.vocab)\n","print(f'\\n{words[0].capitalize()} is to {words[1].capitalize()} as {words[2].capitalize()} is to {prediction.capitalize()}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input': ['berlin', 'germany', 'ottawa'], 'target': ['canada']}\n","\n","Berlin is to Germany as Ottawa is to Canada\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ahzg8jBnpMHe"},"source":["## Como aprendemos esses vetores?\n","\n","Acabamos de ver a representação de palaras como ínidices de um vocabulário fixo. Apesar do índice informar a qual palavra estamos nos referindo, ele não incorpora nenhuma informação semântica sobre a palavra, como por exemplo o contexto no qual ela costuma aparecer. Essa representação semântica pode ser aprendida através de uma **camada de Embedding**.  \n","\n","![](https://drive.google.com/uc?export=view&id=1pliMSOcjjOZAiR26ycowSeUJsj5cy9W_)\n","\n","Pense na camada de Embedding como uma tabela $V \\times D$, onde $V$ é o número de palavras do seu vocabulário e $D$ é o número de dimensões do espaço vetorial onde você deseja projetar. Colocando a camada de Embedding no início de sua rede neural, o treinamento vai otimizar os parâmetros da sua tabela encontrando o espaço vetorial que mapeia as relações intrínsecas entre as palavras dentro do contexto da otimização. Internamente, essa tabela nada mais é do que uma matriz de pesos a ser otimizada.\n","\n","\n","No Pytorch, a instância dessa classe recebe como parâmetro ```(vocab_size, embedding_size, padding_idx)```\n","* ```vocab_size```: Tamanho do vocabulário. Note que **não** se trata da dimensionalidade da entrada.\n","* ```embedding_size```: Dimensionalidade do espaço latente. Caso haja o aproveitamento de embeddings pré treinadas deve-se definir a dimensionalidade da camada em função dos pesos que serão importados (ex: glove.6b.100d, ```embedding_size=100```)."]},{"cell_type":"code","metadata":{"id":"OFiTmkY5pMHg","outputId":"6afee095-d873-44cf-dc87-a9bac59a70c4"},"source":["class Embed(nn.Module):\n","  \n","  def __init__(self,vocab_size, embedding_size, embedding_weights=None):\n","    super(Embed, self).__init__()\n","    \n","    self.embed = nn.Embedding(vocab_size, embedding_size)\n","    \n","    if embedding_weights is not None:\n","        self.embed.weight.data.copy_(embedding_weights)\n","    \n","  def forward(self, X):\n","    return self.embed(X)\n","    \n","\n","embedding_size = INPUT.vocab.vectors.shape[1]\n","vocab_size     = len(INPUT.vocab)\n","\n","pretrained_embeddings = INPUT.vocab.vectors\n","\n","net = Embed(vocab_size, embedding_size,\n","           pretrained_embeddings)\n","\n","print(net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Embed(\n","  (embed): Embedding(907, 100)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2e3t6Sc-pMHh"},"source":["A seguir vamos refazer todos os passos do carregamento de dados para agregá-los em um só lugar. A única novidade aqui é o uso do `Iterator`, equivalente ao `DataLoader` que já conhecemos, mas com alguns facilitadores para trabalhar com dados textuais.\n","\n","```python\n","torchtext.data.Iterator(dataset, batch_size, sort_key=None, device=None, shuffle=None, \n","                        sort=None, sort_within_batch=None)\n","```"]},{"cell_type":"code","metadata":{"id":"khtXRiKypMHi","outputId":"e8f3df13-c15d-475e-b706-9c20f46bef75"},"source":["#### Passo 1: Defina os fields e o carregamento do dataset\n","\n","INPUT  = data.Field(lower = True)\n","TARGET = data.Field(lower = True)\n","\n","dataset = data.TabularDataset('questions-words.csv', format='csv',\n","                                           fields=[('input', INPUT), ('target',TARGET)],\n","                                           skip_header=True\n","                                        )\n","\n","#### Passo 2: Defina o vocabulário **para todos os fields**\n","\n","MAX_VOCAB_SIZE=1000\n","INPUT.build_vocab(dataset, \n","                  max_size=MAX_VOCAB_SIZE,\n","                  vectors='glove.6B.100d')\n","\n","TARGET.build_vocab(dataset, \n","                  max_size=MAX_VOCAB_SIZE,\n","                  vectors='glove.6B.100d')\n","\n","#### Passo 3: Defina o Iterator (nosso loader de batches)\n","loader = data.Iterator(dataset, batch_size=10)\n","\n","for batch in loader:\n","    \n","    print(f'Input: {batch.input}\\nshape: {batch.input.shape}')\n","    print(f'\\nTarget: {batch.target}\\nshape: {batch.target.shape}')\n","    \n","    inp = batch.input\n","    lab = batch.target\n","    \n","    embed = net(inp)\n","    print(f'\\nEmbed shape:{embed.shape}')\n","    \n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: tensor([[140,  54, 161, 103,   4, 220, 253, 205, 374, 215],\n","        [591, 727, 823, 151, 150, 558, 594, 120,   3, 548],\n","        [215, 368,  16, 395, 401,  15, 372, 206, 219, 133]])\n","shape: torch.Size([3, 10])\n","\n","Target: tensor([[103, 294, 455, 340, 350, 138,   3, 207, 111,  94]])\n","shape: torch.Size([1, 10])\n","\n","Embed shape:torch.Size([3, 10, 100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8TtLlDUmpMHi"},"source":["### Um pequeno exercício\n","\n","Refaça as analogias (funções copiadas abaixo) adaptando o código para usar a camada de embedding que definimos acima para adquirir as representações distribuídas de cada palavra. "]},{"cell_type":"code","metadata":{"id":"dGOI4lPapfvP"},"source":["def get_analogy(token_a, token_b, token_c, embed):\r\n","\r\n","    vecs = [net([embed.stoi[t]])\r\n","                for t in [token_a, token_b, token_c]]\r\n","    \r\n","    analogy = vecs[1] - vecs[0] + vecs[2]\r\n","\r\n","    distances = np.dot(embed, analogy) / np.linalg.norm(embed)\r\n","    best = np.argsort(distances)\r\n","    best = [embed.itos[best[k]] for k in range(-1, -4, -1) if embed.itos[best[k]] not in [token_a, token_b, token_c]]\r\n","\r\n","    return best[0]\r\n","\r\n","idx = 100\r\n","print(vars(dataset[idx]))\r\n","words, analogy = dataset[idx].input, dataset[idx].target\r\n","\r\n","prediction = get_analogy(words[0], words[1], words[2], net)\r\n","print(f'\\n{words[0].capitalize()} is to {words[1].capitalize()} as {words[2].capitalize()} is to {prediction.capitalize()}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NRMb9MapMHj"},"source":["def get_analogy(token_a, token_b, token_c, embed):\n","\n","    vecs = [embed[INPUT.vocab.stoi[t]]\n","                for t in [token_a, token_b, token_c]]\n","    \n","    analogy = vecs[1] - vecs[0] + vecs[2]\n","\n","    distances = np.dot(embed, analogy) / np.linalg.norm(embed)\n","    best = np.argsort(distances)\n","    best = [INPUT.vocab.itos[best[k]] for k in range(-1, -4, -1) if INPUT.vocab.itos[best[k]] not in [token_a, token_b, token_c]]\n","\n","    return best[0]\n","\n","idx = 100\n","print(vars(dataset[idx]))\n","words, analogy = dataset[idx].input, dataset[idx].target\n","\n","prediction = get_analogy(words[0], words[1], words[2], net.embed.weight.detach().numpy())\n","print(f'\\n{words[0].capitalize()} is to {words[1].capitalize()} as {words[2].capitalize()} is to {prediction.capitalize()}')"],"execution_count":null,"outputs":[]}]}