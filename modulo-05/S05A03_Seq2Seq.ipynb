{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A03_Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCy34_fj_E8"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports básicos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeSdMJtajy_z"
      },
      "source": [
        "! pip install torchtext==0.8.1\n",
        "! pip install unidecode\n",
        "\n",
        "# Basic imports.\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "! python -m spacy download en\n",
        "! python -m spacy download fr\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWXOc1GkDTQ"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 100,       # Number of epochs.\n",
        "    'lr': 1e-3,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 6,     # Number of workers on data loader.\n",
        "    'batch_size': 10,     # Mini-batch size.\n",
        "    'max_length': 50,    # Maximun length of predicted sentence\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsYj_w9H6TL"
      },
      "source": [
        "# Generating Sequences\n",
        "\n",
        "\n",
        "Dentre os tipos de problemas solucionáveis com modelos recorrentes, dois deles são baseados em geração de sequências: Os problemas One-to-Many,  e os Many-to-Many não sincronizados. <br>\n",
        "\n",
        "Tipicamente os modelos de geração de sequências são baseados em arquiteturas **Encoder-Decoder**, onde a entrada é codificada para uma forma fixa, e então decodificada passo a passo em uma sequência.\n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0K-eTs1VAso"
      },
      "source": [
        "## One-to-Many: Image Captioning\n",
        "\n",
        "O problema de legendar (ou descrever) imagens, se encaixa na categoria One-to-Many, pois cada imagem é considerada uma unidade atômica, ou seja, não é modelada como uma sequência. Já a saída é uma sequência de caracteres semanticamente relacionados com a imagem. Na figura a seguir vemos uma representação superficial do problema.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10YhOB7pvnhXUhqu08JNJ-YcgoyPp8c3K\" height=\"350\">\n",
        "\n",
        "Em termos de modelagem de solução, temos que a imagem deve ser mapeada para um espaço latente que provê um vetor de características de contexto (**context feature**).  Essa etapa consiste na codificação da sua entrada (**encoder**), destacando características semanticamente relevantes para a etapa de decodificação (**decoder**).\n",
        "\n",
        "Sendo a saída uma sequência de palavras, é comum que o decoder seja composto por camadas recorrentes, que na primeira iteração recebem como entrada:\n",
        "*  Hidden state inicial ($h_0$): context feature, ou seja, a saída produzida pelo encoder\n",
        "*  Input inicial ($x_0$): Token especial de início de sequência (**```<sos>```** - start of sequence)\n",
        "\n",
        "As iterações seguintes recebem como entrada os resultados produzidos na iteração anterior, ou seja:\n",
        "*  Hidden state inicial ($h_t$): $h_{t-1}$\n",
        "*  Input inicial ($x_t$): $y_{t-1}$\n",
        "\n",
        "O fim das iterações é determinado pela geração do token **```<eos>```** indicando o término da sequência (end of sequence). A figura a seguir apresenta uma representação visual desse pipeline. Em azul é apresentada a entrada única do decoder (```<sos>```) e em vermelho as saídas do modelo.<br><br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j7SUTfGIHi7XIPv8YRKF1XLTTmJfvQrd\" width=\"750\"><br><br>\n",
        "\n",
        "\n",
        "O código a seguir apresenta uma ilustração de pequeno porte de um modelo de Image Captioning, sem adição de transformações ou modelos de atenção. Para problemas do mundo real, aplicar o conceito de Atenção melhora significativemente a qualidade de modelos, mas a princípio vamos concentrar na arquitetura recorrente Encoder-Decoder. <br>\n",
        "Para quem quiser saber mais, recomendo esse tutorial de Image Captioning: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
        "\n",
        "Perceba no código a seguir a implementação de dois tipos de ```forward()```:\n",
        "* ```forward_inference()```: A forma mais direta de decodificar a entrada em uma sequência de tokens, usando a saída do timestep anterior como entrada do timestep seguinte. Requer a implementação de um **loop explícito**, diferente do forward encapsulado que vínhamos utilizando.\n",
        "* ```forward()```: Em tempo de treino, é possível realizar o forward encapsulado, alimentando a sequência target como entrada do modelo recorrente. Nesse caso o comprimento das sequências é conhecido, e o loop encapsulado pode ser interrompido ao final da sequência target. **Na prática recomenda-se fazer parte do treinamento com os targets e outra parte sem**. A figura a seguir apresenta a diferença sutil que permite o encapsulamento do forward de batches. Novamente em azul são apresentadas as entradas fornecidas ao modelo, e em vermelho as saídas. <br><br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XBnnnbAWx2y63JiZ9G8zBtxxll0YpV_F\" width=\"750\"> <br><br>\n",
        "\n",
        "Vale explicitar que **cada iteração no decoder é composta pelas camadas de embedding, RNN e Linear**. Em problemas que precisam fazer classifcação em múltiplas iterações, usa-se de um artifício para permitir o forward encapsulado. Dada uma saída recorrente com shape ```(seq_len, batch_size, hidden_size)```, o forward na camada linear é realizado redimensionando a saída para a forma  ```(seq_len * batch_size, hidden_size)```, de modo que a camada linear interprete como um grande batch de características. Como a dependência temporal é modelada internamente pela GRU, as amostras podem ser alimentadas individualmente para a camada Linear.\n",
        "\n",
        "```python\n",
        "outputs_rnn, hidden = self.gru(inputs, hidden)\n",
        "\n",
        "input_linear = output_rnn.view(output_rnn.size(0)*output_rnn.size(1), output_rnn.size(2))\n",
        "output = self.softmax(self.out(input_linear))\n",
        "\n",
        "```\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eFDgd6gn_l-lcKcQIuQA7ooI_ZkvWYrp\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14vqCWN4SQ70"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, output_size, max_length, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embed_size  = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Word embedding\n",
        "        self.embedding = nn.Embedding(self.output_size, self.embed_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        # Recurrent feature\n",
        "        self.gru = nn.GRU(self.embed_size, self.hidden_size)\n",
        "        \n",
        "        # Classify next word\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, hidden, target=None, lengths=None):\n",
        "\n",
        "        if target is None:\n",
        "          return self.forward_inference(hidden)\n",
        "      \n",
        "        embedded = self.dropout(self.embedding(target))\n",
        "\n",
        "        packed_inputs = nn.utils.rnn.pack_padded_sequence(embedded, lengths)        \n",
        "        packed_outputs, hidden = self.gru(packed_inputs, hidden)\n",
        "        output_rnn, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        \n",
        "        output = output_rnn.view(output_rnn.size(0)*output_rnn.size(1), output_rnn.size(2))\n",
        "        output = self.softmax(self.out(output))\n",
        "        output = output.view(output_rnn.size(0), output_rnn.size(1), -1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "      \n",
        "    def forward_inference(self, hidden):\n",
        "      \n",
        "        # Start inference with <sos> token\n",
        "        input = torch.tensor(TEXT.vocab.stoi[\"<sos>\"]).to(args['device'])\n",
        "        \n",
        "        outputs = []\n",
        "        # Iterate to a maximum length\n",
        "        for i in range(self.max_length):\n",
        "          \n",
        "          # Forward single sample\n",
        "          embedded = self.embedding(input).view(1,1,-1)\n",
        "          output_rnn, hidden = self.gru(embedded, hidden)\n",
        "          output = self.softmax(self.out(output_rnn[0]))\n",
        "          \n",
        "          outputs.append(output)\n",
        "          \n",
        "          # Current output feeds future input\n",
        "          topv, topi = output.topk(1)\n",
        "          input = topi.squeeze().detach()  \n",
        "\n",
        "          # Finish inference when <eos> generated\n",
        "          if input == TEXT.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "            \n",
        "        # Return sequence of tokens produced \n",
        "        # Either interrupted by producing <eos>\n",
        "        # Or interrupted by max_length\n",
        "        return torch.stack(outputs)\n",
        "          \n",
        "        \n",
        "      \n",
        "\n",
        "####### Build encoder ###########\n",
        "# encoder = models.resnet18(pretrained=True)\n",
        "encoder = models.resnet18()\n",
        "num_features = encoder.fc.in_features\n",
        "\n",
        "# Remove linear layers\n",
        "modules = list(encoder.children())[:-2]\n",
        "encoder = nn.Sequential(*modules)\n",
        "#################################\n",
        "\n",
        "\n",
        "####### Build decoder ###########\n",
        "vocab_size   = len(TEXT.vocab)\n",
        "embed_size   = 100\n",
        "hidden_size  = num_features\n",
        "max_length   = args['max_length'] \n",
        "\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_length).to(args['device'])\n",
        "#################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMQRCmguT4A"
      },
      "source": [
        "## Sequence-to-sequence models (Seq2Seq)\n",
        "\n",
        "Modelos Sequence-to-Sequence (Seq2Seq) partem do mesmo princípio do Image Captioning, porém a entrada também é sequencial, de modo que a codificação também é realizada por um modelo recorrente. \n",
        "\n",
        "A atividade de hoje é no contexto de Neural Machine Translation (NMT), cujo pipeline é representado de forma simplificada a seguir. Note que o idioma source (francês) necessita apenas do token de finalização de sentença, enquanto o idioma target precisa de ambos os inicializadores e os finalizadores (```<sos>```, ```<eos>```), visto que a entrada da rede precisa do token de inicialização, mas a saída, através da qual será calculada a loss, é produzida apenas com a finalização.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Imagem retirada do tutorial de NMT do Pytorch: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "Encontre modelos de linguagem pré-treinados e arquiteturas implementadas em: http://opennmt.net/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIb6Cw3kEMr"
      },
      "source": [
        "# Baixando Dataset\n",
        "!wget https://www.dropbox.com/s/gq36ksk347d36ln/translation_data.zip\n",
        "!unzip translation_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ7knpeZs2fv"
      },
      "source": [
        "# Criando CSV de treino e teste para carregar com o TabularDataset\n",
        "\n",
        "translation_path = 'data/eng-fra.txt'\n",
        "\n",
        "samples = open(translation_path).read().split('\\n')\n",
        "  \n",
        "# Write txt to csv\n",
        "lines = (line.split(\"\\t\") for line in samples)\n",
        "with open('translation_data.csv', 'w') as out_file:\n",
        "    writer = csv.writer(out_file)\n",
        "    writer.writerow(('English', 'French'))\n",
        "    writer.writerows(lines)\n",
        "    \n",
        "df = pd.read_csv('translation_data.csv')\n",
        "\n",
        "# Reducing data (throwing out samples)\n",
        "train, _ = train_test_split(df, test_size=0.85)\n",
        "\n",
        "# Split train and test set \n",
        "train, test = train_test_split(train, test_size=0.02)\n",
        "\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n",
        "\n",
        "\n",
        "df = pd.read_csv('test.csv')\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnMn_u9wNc5-"
      },
      "source": [
        "# Preparação do dataset:\n",
        "# Tokenização e inclusão dos tokens especiais (<eos>, <sos>, <pad>, <unk>)\n",
        "TEXT_FR = data.Field(tokenize = 'spacy', \n",
        "                     tokenizer_language='fr',\n",
        "                     lower=True, \n",
        "                     include_lengths=True, \n",
        "                     eos_token = \"<eos>\")\n",
        "\n",
        "TEXT_EN = data.Field(tokenize = 'spacy', \n",
        "                     tokenizer_language='en', \n",
        "                     lower=True,\n",
        "                     include_lengths=True, \n",
        "                     init_token = \"<sos>\", \n",
        "                     eos_token = \"<eos>\")\n",
        "\n",
        "fields = [('text_en', TEXT_EN), ('text_fr', TEXT_FR)]\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                  path = '.',\n",
        "                                  train = 'train.csv',\n",
        "                                  test = 'test.csv',\n",
        "                                  format = 'csv',\n",
        "                                  fields = fields,\n",
        "                                  skip_header = True)\n",
        "\n",
        "for sample in train_data:\n",
        "  print(sample.text_fr)\n",
        "  print(sample.text_en)\n",
        "  break\n",
        "  \n",
        "print(len(train_data), len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzHUADPwPJLz"
      },
      "source": [
        "# Criando vocabulário\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT_EN.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "TEXT_FR.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "# Instanciando bucket iterator\n",
        "# Note que a ordenação é definida pelo \n",
        "# comprimento do par ** (en, fr) **\n",
        "# Precisaremos empacotar ambas as sequências \n",
        "# para realizar o forward encapsulado.\n",
        "train_iterator = data.BucketIterator(\n",
        "    train_data, \n",
        "    batch_size = args['batch_size'],\n",
        "    sort_key = lambda x:(len(x.text_fr), len(x.text_en)),\n",
        "    sort_within_batch = True,\n",
        "    device = args['device'])\n",
        "\n",
        "\n",
        "test_iterator = data.BucketIterator(\n",
        "    test_data, \n",
        "    batch_size = 1,\n",
        "    sort_within_batch = False,\n",
        "    device = args['device'])\n",
        "\n",
        "for k, batch in enumerate(train_iterator):\n",
        "  text_fr, lengths_fr = batch.text_fr\n",
        "  text_en, lengths_en = batch.text_en\n",
        "  \n",
        "  print(text_fr)\n",
        "  print(text_en)\n",
        "  print(lengths_fr)\n",
        "  print(lengths_en)\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSkInJB2lAs4"
      },
      "source": [
        "# Atividade Prática\n",
        "\n",
        "Implemente a arquitetura apresentada na figura abaixo através da realização dos seguintes passos:\n",
        "\n",
        "### EncoderRNN\n",
        "\n",
        "Implemente a classe **EncoderRNN** composta de um passo de representação de palavras e um passo de caracterização de sequência, ou seja, implemente as seguintes camadas:\n",
        "*  Embedding: como não usaremos vetores pré-treinados, a dimensão de saída dessa camada é um hiperparâmetro livre. Sugestão de tamanho: ```100```. Sua entrada é definida pelo tamanho do dicionário do idioma source (nesse caso o francês).\n",
        "*  Dropout: ```0.1``` <br><br>\n",
        "*  GRU: Defina ```hidden_size = 128``` para o encoder\n",
        "\n",
        "### DecoderRNN\n",
        "\n",
        "Implemente a classe **DecoderRNN**. Novamente é necessário uma camada de representação de palavras, seguida de uma camada de caracterização de sequências. Além disso, o decoder também deve possuir uma camada Linear de classificação, que transformará a representação de cada timestep (saída da RNN) em uma predição da próxima palavra.\n",
        "\n",
        "* Embedding: A entrada definida pelo vocabulário do idioma target (inglês), saída é um hiperparâmetro livre (sugestão: ```100```).\n",
        "* Dropout: ```0.1``` <br><br>\n",
        "* GRU: Seus hiperparâmetros são inferíveis a partir das outras informações. **Lembre-se que a inicialização do hidden state é dada pelo último hidden state do encoder** (veja na função train). <br><br>\n",
        "* Linear: Parâmetros inferíveis pelas outras informações. Quantas classes tem a predição de palavras em inglês?\n",
        "* LogSoftmax: ativação da classificação.\n",
        "\n",
        "No decoder, **implemente ambos os forward** para treinamento (encapsulado em batches) e para inferência (loop explícito sem targets).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j8aLVymyvhGtM0lpfON0aDyPvUf4700U\" width=\"850\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2CyngmcRWP"
      },
      "source": [
        "class EncoderRNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(EncoderRNN, self).__init__()\r\n",
        "        \r\n",
        "        ### TODO\r\n",
        "\r\n",
        "    def forward(self, inputs, lengths):\r\n",
        "        \r\n",
        "        ### TODO\r\n",
        "\r\n",
        "        # Por favor, mantenha o retorno dessa forma\r\n",
        "        return outputs, hidden[-1:]\r\n",
        "\r\n",
        "    \r\n",
        "      \r\n",
        "      \r\n",
        "class DecoderRNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DecoderRNN, self).__init__()\r\n",
        "    \r\n",
        "        ### TODO arquitetura do decoder\r\n",
        "\r\n",
        "        self.max_length = args['max_length'] \r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, hidden, target=None, lengths=None):\r\n",
        "\r\n",
        "        if target is None:\r\n",
        "          return self.forward_inference(hidden)\r\n",
        "      \r\n",
        "        ### TODO forward em batches (target como entrada)\r\n",
        "        \r\n",
        "        return output\r\n",
        "\r\n",
        "      \r\n",
        "    def forward_inference(self, hidden):\r\n",
        "        \r\n",
        "        # Definição do input inicial <sos>\r\n",
        "        input = torch.tensor(TEXT_EN.vocab.stoi[\"<sos>\"]).to(args['device'])\r\n",
        "        \r\n",
        "        # Loop explícito para alimentar cada saída como próxima entrada\r\n",
        "        outputs = []\r\n",
        "        for i in range(self.max_length):\r\n",
        "          \r\n",
        "          ### TODO forward no decoder\r\n",
        "          \r\n",
        "          topv, topi = output.topk(1)\r\n",
        "          input = topi.squeeze().detach()\r\n",
        "          # qual a importância do .detach() acima?  \r\n",
        "\r\n",
        "          outputs.append(output)\r\n",
        "          if input == TEXT_EN.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "            \r\n",
        "        return torch.stack(outputs)\r\n",
        "          \r\n",
        "\r\n",
        "encoder = #...\r\n",
        "decoder = #...\r\n",
        "\r\n",
        "print(encoder)\r\n",
        "print(decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbtjvr-PM9GM"
      },
      "source": [
        "# Setting optimizer.\n",
        "encoder_optimizer = #...\n",
        "\n",
        "decoder_optimizer = #...\n",
        "\n",
        "# Setting loss.\n",
        "criterion = #...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC7WBLNIUwFj"
      },
      "source": [
        "def train(train_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "\n",
        "        text_lengths = text_lengths.cpu()\n",
        "        labs_lengths = labs_lengths.cpu()\n",
        "        \n",
        "        # Ignorando batches não ordenados para acelerar o treinamento\n",
        "        if sorted(labs_lengths, reverse=True) != list(labs_lengths.data):\n",
        "          continue\n",
        "\n",
        "        # Clears the gradients of optimizer.\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        enc, enc_hidden = encoder(text, text_lengths)\n",
        "        outs = decoder(enc_hidden, labs[:-1], labs_lengths-1)\n",
        "        \n",
        "        # Computing loss.\n",
        "        loss = 0.\n",
        "        for k, out in enumerate(outs):\n",
        "          loss += criterion(out, labs[k+1])\n",
        "        loss = loss.mean()\n",
        "        \n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Weight update\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "    print('--------------------------------------------------------------------')\n",
        "\n",
        "def test(test_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode (not computing gradients).\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    print('********************************************************************')\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "        \n",
        "        text_lengths = text_lengths.cpu()\n",
        "        labs_lengths = labs_lengths.cpu()\n",
        "        \n",
        "        # Forwarding.\n",
        "        enc, enc_hidden  = encoder(text, text_lengths)\n",
        "        outs = decoder(enc_hidden)\n",
        "\n",
        "        if i < 2:\n",
        "          print('Input:',  [TEXT_FR.vocab.itos[t] for t in text])\n",
        "          print('Label:',  [TEXT_EN.vocab.itos[t] for t in labs[1:]])\n",
        "          print('Output:', [TEXT_EN.vocab.itos[np.argmax(t.cpu().data)] for t in outs], '\\n')\n",
        "        \n",
        "        \n",
        "        # Computing approximate loss \n",
        "        labs = labs[1:]\n",
        "        minlen = min(len(labs), len(outs))\n",
        "        \n",
        "        loss = 0.\n",
        "        for k in range(minlen):\n",
        "          loss += criterion(outs[k], labs[k])\n",
        "        loss = loss.mean()\n",
        "                \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "\n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "   \n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), (toc - tic)))\n",
        "    print('********************************************************************')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7scMkTCQhADF"
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_iterator, criterion, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_iterator, criterion, epoch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}