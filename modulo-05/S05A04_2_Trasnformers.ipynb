{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A04_2_Trasnformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QVpXv6Aisra"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M9W--OUwv4G"
      },
      "source": [
        "# !pip install torchtext==0.8.1\r\n",
        "\r\n",
        "import math, time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchtext import data, datasets\r\n",
        "\r\n",
        "args = {\r\n",
        "    'batch_size': 32,\r\n",
        "    'bptt_len': 35,\r\n",
        "    'epoch_num': 100,\r\n",
        "    'lr': 5.0,\r\n",
        "    'step_size': 1.0,\r\n",
        "    'gamma': 0.95 \r\n",
        "}\r\n",
        "\r\n",
        "args['device'] = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HNII9FzRz_8"
      },
      "source": [
        "## Modelo de Linguagem com WikiText2\r\n",
        "\r\n",
        "Vamos preparar um conjunto de treinamento de modelo de linguagem, cujo objetivo é sempre prever a palavra imediatamente seguinte. Para tal, dado uma sequência `frase` de `n` tokens, vamos estruturar os dados da seguinte forma:\r\n",
        "\r\n",
        "* `entrada = frase[:-1]`\r\n",
        "* `saida = frase[1:]`\r\n",
        "\r\n",
        "Em outras palavras, cada par ($e_i$, $s_i$) de entrada e saída são os tokens consecutivos  $k$ e $k+1$ da nossa `frase` original. Por exemplo:\r\n",
        "\r\n",
        "* `frase = ['qual', 'e', 'a', 'musica', '?']`\r\n",
        "* `entrada = ['qual', 'e', 'a',      'musica']`\r\n",
        "* `saida   = ['e',    'a', 'musica', '?']`\r\n",
        "\r\n",
        "No pacote `torchtext` existe um iterator específico para esse tipo de carregamento, o [BPTTIterator](https://pytorch.org/text/0.8.1/data.html#bpttiterator). Dada uma entrada única contendo todo o texto de seus dados, basta informar ao iterator o `bptt_len` (aqui representando o tamanho da sequência desejada) e ele dividirá o seu texto em partes iguais de tamanho `bptt_len`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qqcz0j3xkQk",
        "outputId": "a8a40f65-d358-4dc0-8bf3-236736da8b2d"
      },
      "source": [
        "TEXT = data.Field(tokenize='basic_english')\r\n",
        "train, val, test = datasets.WikiText2.splits(text_field=TEXT)\r\n",
        "TEXT.build_vocab( train, max_size=25_000 )\r\n",
        "\r\n",
        "train_loader = data.BPTTIterator(train, args['batch_size'], args['bptt_len'])\r\n",
        "val_loader   = data.BPTTIterator(val,   args['batch_size'], args['bptt_len'])\r\n",
        "test_loader  = data.BPTTIterator(test,  args['batch_size'], args['bptt_len']) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 76.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n",
            "extracting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BPTTIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4GZtysvz5zH",
        "outputId": "47c807e0-77a7-49e7-da9f-1b024f521894"
      },
      "source": [
        "print('Validation Data [0:100]:', val.examples[0].text[0:100])\r\n",
        "\r\n",
        "for sample in test_loader:\r\n",
        "  print('\\nBatch text shape:', sample.text.shape)\r\n",
        "  print('Batch target shape:', sample.target.shape)\r\n",
        "  \r\n",
        "  print('\\n', sample.text)\r\n",
        "  print('\\n', sample.target)\r\n",
        "\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data [0:100]: ['<eos>', '=', 'homarus', 'gammarus', '=', '<eos>', '<eos>', 'homarus', 'gammarus', ',', 'known', 'as', 'the', 'european', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'atlantic', 'ocean', ',', 'mediterranean', 'sea', 'and', 'parts', 'of', 'the', 'black', 'sea', '.', 'it', 'is', 'closely', 'related', 'to', 'the', 'american', 'lobster', ',', 'h', '.', 'americanus', '.', 'it', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'in', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', 'lobster', 'red', 'on', 'cooking', '.']\n",
            "\n",
            "Batch text shape: torch.Size([35, 32])\n",
            "Batch target shape: torch.Size([35, 32])\n",
            "\n",
            " tensor([[    9,     4,   782,  ...,   606,   431,    24],\n",
            "        [   11,    24,   730,  ...,     8,    65,     3],\n",
            "        [  634,   153,  7557,  ...,  2331,    24,    24],\n",
            "        ...,\n",
            "        [    7,  2650,   345,  ...,     5,    22,    41],\n",
            "        [  915,    58,    31,  ..., 21413,  4736,  2980],\n",
            "        [    4,     2,   304,  ...,     6,  2231,     2]])\n",
            "\n",
            " tensor([[   11,    24,   730,  ...,     8,    65,     3],\n",
            "        [  634,   153,  7557,  ...,  2331,    24,    24],\n",
            "        [    0,   166,  1416,  ...,    49,     4,   102],\n",
            "        ...,\n",
            "        [  915,    58,    31,  ..., 21413,  4736,  2980],\n",
            "        [    4,     2,   304,  ...,     6,  2231,     2],\n",
            "        [   37,   446,    62,  ...,  7543,  5099,   979]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vud3Akv8T-0f"
      },
      "source": [
        "## Transformer Model\r\n",
        "\r\n",
        "Proposto no paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf), o trabalho foi uma sacada incrível de explorar uma arquitetura totalmente composta de blocos de **Self Attention** para realizar tarefas que até então exigiam uma arquitetura recorrente, de forma muito melhor paralelizável. \r\n",
        "\r\n",
        "<img width=800 src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\">\r\n",
        "\r\n",
        "O PyTorch já implementa tanto o encoder quando o decoder da arquitetura do transformer, nos permitindo implementar facilmente essas camadas. Veja a documentação: https://pytorch.org/docs/master/nn.html#transformer-layers\r\n",
        "\r\n",
        "\r\n",
        "Ambos o encoder e o decoder exigem um par de definições de camada. Precisamos definir a característica de uma única camada através do objeto ```nn.TransformerEncoderLayer```, para então replicá-los com o objeto ```nn.TransformerEncoder```, como apresentado a seguir. O mesmo vale para o decoder, com os objetos correspondentes ```nn.TransformerDecoderLayer``` e ```nn.TransformerDecoder```.\r\n",
        "\r\n",
        "```python\r\n",
        "encoder_layers = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout)\r\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\r\n",
        "```\r\n",
        "\r\n",
        "Adicionalmente, precisamos implementar outros dois recursos da arquitetura:\r\n",
        "\r\n",
        "* `PositionalEncoding`: aqui definido como um `nn.Module`, é a codificação que marca a posição de cada elemento na sequência. Por se tratar de uma arquitetura feed-forward, os transformers não implementam temporalidade nativamente. Os positional encodings vão injetar essa informação na rede.   \r\n",
        "* `generate_square_subsequent_mask`: Essa máscara permite que o módulo de self attention relacione cada elemento da sequência apenas com si mesmo e seus antecessores (veja daqui a duas células ;) \r\n",
        "\r\n",
        "A implementação a seguir foi adaptada deste tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYbg4kIm13id"
      },
      "source": [
        "class TransformerModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout=0.5):\r\n",
        "        super(TransformerModel, self).__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, dropout)\r\n",
        "        \r\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout)\r\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\r\n",
        "      \r\n",
        "        self.embed_size = embed_size\r\n",
        "        self.decoder = nn.Linear(embed_size, vocab_size)\r\n",
        "\r\n",
        "        self.init_weights()\r\n",
        "\r\n",
        "    def generate_square_subsequent_mask(self, sz):\r\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n",
        "        return mask\r\n",
        "\r\n",
        "    def init_weights(self):\r\n",
        "        initrange = 0.1\r\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n",
        "        self.decoder.bias.data.zero_()\r\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\r\n",
        "\r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        src = self.embedding(src) * math.sqrt(self.embed_size)\r\n",
        "        src = self.pos_encoder(src)\r\n",
        "        \r\n",
        "        output = self.transformer_encoder(src, src_mask)\r\n",
        "        output = self.decoder(output)\r\n",
        "        return output\r\n",
        "\r\n",
        "\r\n",
        "class PositionalEncoding(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embed_size, dropout=0.1, max_len=5000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "        pe = torch.zeros(max_len, embed_size)\r\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\r\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\r\n",
        "        self.register_buffer('pe', pe)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x + self.pe[:x.size(0), :]\r\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "famGTvHC1-fb",
        "outputId": "18df60bd-5559-4f12-c1f1-0fb3f8a03fdc"
      },
      "source": [
        "vocab_size = len(TEXT.vocab) # the size of vocabulary\r\n",
        "embed_size = 200 # embedding dimension\r\n",
        "hidden_size = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\r\n",
        "num_layers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\r\n",
        "num_heads = 2 # the number of heads in the multiheadattention models\r\n",
        "dropout = 0.2 # the dropout value\r\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout).to(args['device'])\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel(\n",
            "  (embedding): Embedding(25002, 200)\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
            "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
            "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=200, out_features=25002, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1OKcYXQZnRt"
      },
      "source": [
        "#### Positional Encodings\r\n",
        "\r\n",
        "* Codificação única que marque a posição de cada palavra. \r\n",
        "* Deve ter a mesma dimensionalidade de `embed_size` para poder ser somada às palavras.\r\n",
        "\r\n",
        "No paper original do transformer foi definida como:\r\n",
        "\r\n",
        "$PE_{(pos,2i)} = sin(pos/10000^{2i/embed\\_size})$<br>\r\n",
        "$PE_{(pos,2i+1)} = cos(pos/10000^{2i/embed\\_size})$\r\n",
        "\r\n",
        "Explicação bacana: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "mbTdIg88XtQE",
        "outputId": "1f6076e3-db54-4b2d-e224-9e79f88001d4"
      },
      "source": [
        "plt.figure(figsize=(14, 14))\r\n",
        "print(model.pos_encoder.pe.squeeze(1).shape)\r\n",
        "plt.imshow(model.pos_encoder.pe.squeeze(1).cpu()[:args['bptt_len']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 200])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd51f36d610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x1008 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p_gCEUZZp6G"
      },
      "source": [
        "#### Attention Mask\r\n",
        "\r\n",
        "Na self attention, ambas a query e as chaves são providas pela mesma saída, já que busca-se encontrar pesos de importância para \"si mesmo\".\r\n",
        "\r\n",
        "A máscara aqui é definida como uma matriz quadrada $n \\times n$, onde $n$ é a quantidade de elementos da sequência (o nosso `seq_len`). Atribuindo uma máscara que forma um triângulo inferior permite-se que cada elemento $e_i$ do timestep $i$ \"dê atenção\" somente a si mesmo e seus antecessores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "PsiwSYZHAPNU",
        "outputId": "5aefb687-04ac-4650-ce87-ee93a3cca006"
      },
      "source": [
        "src_mask = model.generate_square_subsequent_mask(args['bptt_len'])\r\n",
        "\r\n",
        "print(src_mask)\r\n",
        "plt.imshow(src_mask)\r\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f4a72e22b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwuFZlqph5cd"
      },
      "source": [
        "## Treinando nosso Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yRZpvyt2u1g"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'])\r\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args['step_size'], args['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UagOtFGB208U"
      },
      "source": [
        "def train():\r\n",
        "    \r\n",
        "    model.train() # Turn on the train mode\r\n",
        "    total_loss = 0.\r\n",
        "    log_interval = len(train_loader)\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    src_mask = model.generate_square_subsequent_mask(args['bptt_len']).to(args['device'])\r\n",
        "    \r\n",
        "    for i, batch in enumerate(train_loader):\r\n",
        "\r\n",
        "        data = batch.text.to(args['device'])\r\n",
        "        targets = batch.target.to(args['device'])\r\n",
        "\r\n",
        "        # last batch\r\n",
        "        if data.size(0) != args['bptt_len']:\r\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(args['device'])\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        output = model(data, src_mask)\r\n",
        "\r\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        if (i+1) % log_interval == 0:\r\n",
        "          \r\n",
        "            cur_loss = total_loss / log_interval\r\n",
        "            elapsed = time.time() - start_time\r\n",
        "            print('-'*50)\r\n",
        "            print(f'epoch {epoch} | {i+1}/{len(train_loader)} batches | '\r\n",
        "                  f'lr {scheduler.get_lr()[0]:02.2f} | ms/batch {(elapsed * 1000 / log_interval):5.2f} | '\r\n",
        "                  f'loss {cur_loss:5.2f}')\r\n",
        "            print('-'*50)\r\n",
        "            total_loss = 0\r\n",
        "            start_time = time.time()\r\n",
        "\r\n",
        "def evaluate(eval_model, data_source):\r\n",
        "    eval_model.eval() # Turn on the evaluation mode\r\n",
        "    total_loss = 0.\r\n",
        "    src_mask = model.generate_square_subsequent_mask(args['bptt_len']).to(args['device'])\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        \r\n",
        "        for i, batch in enumerate(train_loader):\r\n",
        "\r\n",
        "            data = batch.text.to(args['device'])\r\n",
        "            targets = batch.target.to(args['device'])\r\n",
        "\r\n",
        "            # last batch\r\n",
        "            if data.size(0) != args['bptt_len']:\r\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(args['device'])\r\n",
        "\r\n",
        "            output = eval_model(data, src_mask)\r\n",
        "            output_flat = output.view(-1, vocab_size)\r\n",
        "            total_loss += criterion(output_flat, targets.view(-1)).item()\r\n",
        "\r\n",
        "    return total_loss / len(train_loader) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMRrpQY137j3",
        "outputId": "416deb02-0c56-40c1-c63b-c16e4ac93d5b"
      },
      "source": [
        "for epoch in range(args['epoch_num']):\r\n",
        "    epoch_start_time = time.time()\r\n",
        "    \r\n",
        "    train()\r\n",
        "    val_loss = evaluate(model, val_loader)\r\n",
        "    \r\n",
        "    print('*' * 50)\r\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '.format(epoch, \r\n",
        "                                                                                (time.time() - epoch_start_time),\r\n",
        "                                                                                 val_loss))\r\n",
        "    print('*' * 50)\r\n",
        "\r\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "epoch 0 | 1864/1864 batches | lr 5.00 | ms/batch 21.54 | loss  5.24\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "| end of epoch   0 | time: 54.31s | valid loss  5.16 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 1 | 1864/1864 batches | lr 4.51 | ms/batch 21.43 | loss  5.12\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   1 | time: 53.95s | valid loss  5.04 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 2 | 1864/1864 batches | lr 4.29 | ms/batch 21.55 | loss  5.02\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   2 | time: 54.26s | valid loss  4.98 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 3 | 1864/1864 batches | lr 4.07 | ms/batch 21.54 | loss  4.93\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   3 | time: 54.19s | valid loss  4.90 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 4 | 1864/1864 batches | lr 3.87 | ms/batch 21.56 | loss  4.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   4 | time: 54.32s | valid loss  4.82 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 5 | 1864/1864 batches | lr 3.68 | ms/batch 21.55 | loss  4.78\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   5 | time: 54.26s | valid loss  4.77 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 6 | 1864/1864 batches | lr 3.49 | ms/batch 21.55 | loss  4.71\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   6 | time: 54.28s | valid loss  4.73 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 7 | 1864/1864 batches | lr 3.32 | ms/batch 21.59 | loss  4.66\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   7 | time: 54.34s | valid loss  4.68 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 8 | 1864/1864 batches | lr 3.15 | ms/batch 21.52 | loss  4.60\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   8 | time: 54.12s | valid loss  4.63 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 9 | 1864/1864 batches | lr 2.99 | ms/batch 21.51 | loss  4.55\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch   9 | time: 54.12s | valid loss  4.59 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 10 | 1864/1864 batches | lr 2.84 | ms/batch 21.44 | loss  4.50\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  10 | time: 54.01s | valid loss  4.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 11 | 1864/1864 batches | lr 2.70 | ms/batch 21.43 | loss  4.46\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  11 | time: 53.97s | valid loss  4.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 12 | 1864/1864 batches | lr 2.57 | ms/batch 21.41 | loss  4.42\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  12 | time: 53.89s | valid loss  4.46 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 13 | 1864/1864 batches | lr 2.44 | ms/batch 21.42 | loss  4.38\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  13 | time: 53.97s | valid loss  4.43 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 14 | 1864/1864 batches | lr 2.32 | ms/batch 21.41 | loss  4.35\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  14 | time: 53.95s | valid loss  4.40 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 15 | 1864/1864 batches | lr 2.20 | ms/batch 21.48 | loss  4.31\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  15 | time: 54.11s | valid loss  4.36 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 16 | 1864/1864 batches | lr 2.09 | ms/batch 21.51 | loss  4.28\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  16 | time: 54.19s | valid loss  4.30 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 17 | 1864/1864 batches | lr 1.99 | ms/batch 21.63 | loss  4.25\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  17 | time: 54.39s | valid loss  4.28 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 18 | 1864/1864 batches | lr 1.89 | ms/batch 21.53 | loss  4.23\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  18 | time: 54.18s | valid loss  4.26 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 19 | 1864/1864 batches | lr 1.79 | ms/batch 21.47 | loss  4.20\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  19 | time: 54.06s | valid loss  4.24 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 20 | 1864/1864 batches | lr 1.70 | ms/batch 21.48 | loss  4.18\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  20 | time: 54.07s | valid loss  4.23 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 21 | 1864/1864 batches | lr 1.62 | ms/batch 21.48 | loss  4.15\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  21 | time: 54.04s | valid loss  4.18 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 22 | 1864/1864 batches | lr 1.54 | ms/batch 21.47 | loss  4.13\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  22 | time: 54.01s | valid loss  4.16 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 23 | 1864/1864 batches | lr 1.46 | ms/batch 21.45 | loss  4.11\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  23 | time: 54.02s | valid loss  4.13 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 24 | 1864/1864 batches | lr 1.39 | ms/batch 21.48 | loss  4.09\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  24 | time: 54.03s | valid loss  4.11 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 25 | 1864/1864 batches | lr 1.32 | ms/batch 21.41 | loss  4.08\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  25 | time: 53.90s | valid loss  4.09 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 26 | 1864/1864 batches | lr 1.25 | ms/batch 21.45 | loss  4.06\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  26 | time: 54.02s | valid loss  4.07 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 27 | 1864/1864 batches | lr 1.19 | ms/batch 21.45 | loss  4.05\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  27 | time: 54.00s | valid loss  4.05 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 28 | 1864/1864 batches | lr 1.13 | ms/batch 21.42 | loss  4.03\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  28 | time: 53.93s | valid loss  4.03 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 29 | 1864/1864 batches | lr 1.07 | ms/batch 21.38 | loss  4.02\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  29 | time: 53.89s | valid loss  4.01 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 30 | 1864/1864 batches | lr 1.02 | ms/batch 21.43 | loss  4.00\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  30 | time: 53.96s | valid loss  3.97 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 31 | 1864/1864 batches | lr 0.97 | ms/batch 21.43 | loss  3.99\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  31 | time: 53.91s | valid loss  3.95 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 32 | 1864/1864 batches | lr 0.92 | ms/batch 21.43 | loss  3.98\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  32 | time: 53.99s | valid loss  3.95 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 33 | 1864/1864 batches | lr 0.87 | ms/batch 21.42 | loss  3.97\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  33 | time: 53.93s | valid loss  3.92 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 34 | 1864/1864 batches | lr 0.83 | ms/batch 21.42 | loss  3.96\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  34 | time: 53.92s | valid loss  3.91 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 35 | 1864/1864 batches | lr 0.79 | ms/batch 21.41 | loss  3.95\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  35 | time: 53.93s | valid loss  3.89 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 36 | 1864/1864 batches | lr 0.75 | ms/batch 21.44 | loss  3.94\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  36 | time: 53.97s | valid loss  3.88 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 37 | 1864/1864 batches | lr 0.71 | ms/batch 21.42 | loss  3.94\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  37 | time: 53.95s | valid loss  3.85 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 38 | 1864/1864 batches | lr 0.68 | ms/batch 21.43 | loss  3.93\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  38 | time: 53.96s | valid loss  3.85 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 39 | 1864/1864 batches | lr 0.64 | ms/batch 21.47 | loss  3.92\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  39 | time: 54.02s | valid loss  3.83 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 40 | 1864/1864 batches | lr 0.61 | ms/batch 21.50 | loss  3.91\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  40 | time: 54.10s | valid loss  3.82 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 41 | 1864/1864 batches | lr 0.58 | ms/batch 21.45 | loss  3.91\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  41 | time: 54.00s | valid loss  3.79 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 42 | 1864/1864 batches | lr 0.55 | ms/batch 21.44 | loss  3.90\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  42 | time: 53.96s | valid loss  3.78 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 43 | 1864/1864 batches | lr 0.52 | ms/batch 21.51 | loss  3.90\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  43 | time: 54.17s | valid loss  3.77 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 44 | 1864/1864 batches | lr 0.50 | ms/batch 21.45 | loss  3.89\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  44 | time: 54.04s | valid loss  3.76 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 45 | 1864/1864 batches | lr 0.47 | ms/batch 21.49 | loss  3.89\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  45 | time: 54.11s | valid loss  3.76 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 46 | 1864/1864 batches | lr 0.45 | ms/batch 21.48 | loss  3.88\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  46 | time: 54.06s | valid loss  3.75 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 47 | 1864/1864 batches | lr 0.43 | ms/batch 21.44 | loss  3.88\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  47 | time: 53.99s | valid loss  3.73 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 48 | 1864/1864 batches | lr 0.40 | ms/batch 21.49 | loss  3.87\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  48 | time: 54.07s | valid loss  3.72 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 49 | 1864/1864 batches | lr 0.38 | ms/batch 21.44 | loss  3.87\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  49 | time: 54.03s | valid loss  3.71 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 50 | 1864/1864 batches | lr 0.37 | ms/batch 21.51 | loss  3.87\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  50 | time: 54.15s | valid loss  3.70 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 51 | 1864/1864 batches | lr 0.35 | ms/batch 21.47 | loss  3.86\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  51 | time: 54.05s | valid loss  3.69 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 52 | 1864/1864 batches | lr 0.33 | ms/batch 21.49 | loss  3.86\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  52 | time: 54.07s | valid loss  3.69 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 53 | 1864/1864 batches | lr 0.31 | ms/batch 21.49 | loss  3.86\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  53 | time: 54.09s | valid loss  3.68 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 54 | 1864/1864 batches | lr 0.30 | ms/batch 21.49 | loss  3.86\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  54 | time: 54.13s | valid loss  3.67 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 55 | 1864/1864 batches | lr 0.28 | ms/batch 21.52 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  55 | time: 54.18s | valid loss  3.66 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 56 | 1864/1864 batches | lr 0.27 | ms/batch 21.49 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  56 | time: 54.10s | valid loss  3.65 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 57 | 1864/1864 batches | lr 0.26 | ms/batch 21.46 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  57 | time: 54.01s | valid loss  3.65 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 58 | 1864/1864 batches | lr 0.24 | ms/batch 21.45 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  58 | time: 53.97s | valid loss  3.64 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 59 | 1864/1864 batches | lr 0.23 | ms/batch 21.42 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  59 | time: 53.96s | valid loss  3.63 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 60 | 1864/1864 batches | lr 0.22 | ms/batch 21.44 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  60 | time: 53.98s | valid loss  3.62 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 61 | 1864/1864 batches | lr 0.21 | ms/batch 21.47 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  61 | time: 54.03s | valid loss  3.61 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 62 | 1864/1864 batches | lr 0.20 | ms/batch 21.45 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  62 | time: 54.02s | valid loss  3.61 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 63 | 1864/1864 batches | lr 0.19 | ms/batch 21.44 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  63 | time: 54.03s | valid loss  3.61 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 64 | 1864/1864 batches | lr 0.18 | ms/batch 21.59 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  64 | time: 54.36s | valid loss  3.60 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 65 | 1864/1864 batches | lr 0.17 | ms/batch 21.52 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  65 | time: 54.19s | valid loss  3.61 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 66 | 1864/1864 batches | lr 0.16 | ms/batch 21.62 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  66 | time: 54.44s | valid loss  3.60 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 67 | 1864/1864 batches | lr 0.15 | ms/batch 21.55 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  67 | time: 54.23s | valid loss  3.59 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 68 | 1864/1864 batches | lr 0.15 | ms/batch 21.47 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  68 | time: 54.07s | valid loss  3.59 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 69 | 1864/1864 batches | lr 0.14 | ms/batch 21.52 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  69 | time: 54.13s | valid loss  3.58 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 70 | 1864/1864 batches | lr 0.13 | ms/batch 21.54 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  70 | time: 54.19s | valid loss  3.57 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 71 | 1864/1864 batches | lr 0.12 | ms/batch 21.55 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  71 | time: 54.25s | valid loss  3.57 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 72 | 1864/1864 batches | lr 0.12 | ms/batch 21.62 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  72 | time: 54.33s | valid loss  3.57 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 73 | 1864/1864 batches | lr 0.11 | ms/batch 21.55 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  73 | time: 54.25s | valid loss  3.56 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 74 | 1864/1864 batches | lr 0.11 | ms/batch 21.53 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  74 | time: 54.16s | valid loss  3.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 75 | 1864/1864 batches | lr 0.10 | ms/batch 21.45 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  75 | time: 54.02s | valid loss  3.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 76 | 1864/1864 batches | lr 0.10 | ms/batch 21.45 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  76 | time: 54.00s | valid loss  3.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 77 | 1864/1864 batches | lr 0.09 | ms/batch 21.45 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  77 | time: 54.06s | valid loss  3.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 78 | 1864/1864 batches | lr 0.09 | ms/batch 21.53 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  78 | time: 54.17s | valid loss  3.55 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 79 | 1864/1864 batches | lr 0.08 | ms/batch 21.45 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  79 | time: 54.00s | valid loss  3.54 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 80 | 1864/1864 batches | lr 0.08 | ms/batch 21.49 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  80 | time: 54.09s | valid loss  3.54 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 81 | 1864/1864 batches | lr 0.07 | ms/batch 21.43 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  81 | time: 53.97s | valid loss  3.54 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 82 | 1864/1864 batches | lr 0.07 | ms/batch 21.51 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  82 | time: 54.20s | valid loss  3.53 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 83 | 1864/1864 batches | lr 0.07 | ms/batch 21.63 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  83 | time: 54.37s | valid loss  3.53 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 84 | 1864/1864 batches | lr 0.06 | ms/batch 21.48 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  84 | time: 54.09s | valid loss  3.53 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 85 | 1864/1864 batches | lr 0.06 | ms/batch 21.48 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  85 | time: 54.05s | valid loss  3.53 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 86 | 1864/1864 batches | lr 0.06 | ms/batch 21.48 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  86 | time: 54.12s | valid loss  3.52 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 87 | 1864/1864 batches | lr 0.05 | ms/batch 21.51 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  87 | time: 54.11s | valid loss  3.52 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 88 | 1864/1864 batches | lr 0.05 | ms/batch 21.47 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  88 | time: 54.01s | valid loss  3.51 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 89 | 1864/1864 batches | lr 0.05 | ms/batch 21.51 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  89 | time: 54.13s | valid loss  3.51 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 90 | 1864/1864 batches | lr 0.05 | ms/batch 21.43 | loss  3.84\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  90 | time: 53.97s | valid loss  3.51 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 91 | 1864/1864 batches | lr 0.04 | ms/batch 21.48 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  91 | time: 54.04s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 92 | 1864/1864 batches | lr 0.04 | ms/batch 21.49 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  92 | time: 54.07s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 93 | 1864/1864 batches | lr 0.04 | ms/batch 21.49 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  93 | time: 54.07s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 94 | 1864/1864 batches | lr 0.04 | ms/batch 21.49 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  94 | time: 54.07s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 95 | 1864/1864 batches | lr 0.04 | ms/batch 21.47 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  95 | time: 54.05s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 96 | 1864/1864 batches | lr 0.03 | ms/batch 21.46 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  96 | time: 54.02s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 97 | 1864/1864 batches | lr 0.03 | ms/batch 21.50 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  97 | time: 54.10s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 98 | 1864/1864 batches | lr 0.03 | ms/batch 21.45 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  98 | time: 54.04s | valid loss  3.50 | \n",
            "**************************************************\n",
            "--------------------------------------------------\n",
            "epoch 99 | 1864/1864 batches | lr 0.03 | ms/batch 21.47 | loss  3.85\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "| end of epoch  99 | time: 54.04s | valid loss  3.50 | \n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}