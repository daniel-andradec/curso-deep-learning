{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "S03A03_1 - Regional CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RoAEkUgn4uEq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "# Object Detection e Instance Segmentation em Pytorch\n",
        "\n",
        "O principal dataset de Instance Segmentation atual (o MS-COCO) possui a sua própria API com funcionalidades que ajudam no load dos dados e na avaliação de resultados no dataset. Essa API é pensada para ser integrada com o Pytorch e o torchvision, portanto vamos baixá-la para que possamos utilizar algumas funcionalidades no dataset que utilizaremos nesse notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIoe_tHTQgV"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install pycocotools\n",
        "git clone https://github.com/cocodataset/cocoapi.git\n",
        "cd cocoapi/PythonAPI\n",
        "python setup.py build_ext install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0rqK-A3Nbl"
      },
      "source": [
        "# Baixando e carregando o dataset Penn-Fudan\n",
        "\n",
        "O dataset [*Penn-Fudan*](https://www.cis.upenn.edu/~jshi/ped_html/) para detecção e segmentação de pedestres contém apenas 170 imagens com 345 instâncias de pedestres. Como não há uma classe *Dataset* padrão para esse dataset no Pytorch, precisaremos implementar o load dos dados manualmente com um *Dataset* personalizado.\n",
        "\n",
        "Primeiramente precisamos baixar o dataset que pode ser encontrado em formato zip: [PennFudanPed.zip](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4TBwhHTdkd"
      },
      "source": [
        "!rm -r ./*\n",
        "\n",
        "# Download the Penn-Fudan dataset.\n",
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -O PennFudanPed.zip\n",
        "\n",
        "# Extract it in the current folder.\n",
        "!unzip -q PennFudanPed.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfwuU-jI3j93"
      },
      "source": [
        "A estrutura de pastas do dataset após extração do zip é organizada como mostrado abaixo:\n",
        "\n",
        "```\n",
        "PennFudanPed/\n",
        "  PedMasks/\n",
        "    FudanPed00001_mask.png\n",
        "    FudanPed00002_mask.png\n",
        "    FudanPed00003_mask.png\n",
        "    FudanPed00004_mask.png\n",
        "    ...\n",
        "  PNGImages/\n",
        "    FudanPed00001.png\n",
        "    FudanPed00002.png\n",
        "    FudanPed00003.png\n",
        "    FudanPed00004.png\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-HPDSRWBNUA"
      },
      "source": [
        "Podemos carregar e plotar rapidamente uma imagem do dataset usando a biblioteca [PIL](https://pillow.readthedocs.io/en/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjuVFgexFfh"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFHKCvCTxiff"
      },
      "source": [
        "mask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')\n",
        "\n",
        "# Each mask instance has a different color, from zero to N, where N is the\n",
        "# number of instances. In order to make visualization easier, let's add a color\n",
        "# palette to the mask.\n",
        "mask.putpalette([\n",
        "    0, 0, 0, # black background\n",
        "    255, 0, 0, # index 1 is red\n",
        "    255, 255, 0, # index 2 is yellow\n",
        "    255, 153, 0, # index 3 is orange\n",
        "])\n",
        "\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "Cada imagem tem uma máscara de segmentação com uma cor correspondendod a cada instância diferente de um pedestre. Precisamos, portanto, construir uma subclasse da classe *Dataset* do Pytorch que implemente a leitura desse tipo específico de dado.\n",
        "\n",
        "O torchvision contém [scripts de referência](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) que podem ser usados para se adaptar código existente para uma nova tarefa de Detecção de Objetos, Detecção de Keypoints ou Instance Segmentation. Para integração com esse código, o *Dataset* customizado para o novo conjunto de dados deve retornar no método *\\_\\_getitem\\_\\_()* uma sequência específica de dados:\n",
        "\n",
        "* image: uma imagem no formato da biblioteca PIL com dimensões (H, W)\n",
        "* target: um dicionário contendo os seguintes campos:\n",
        "    * `'boxes'` (`FloatTensor[N, 4]`): coordenadas dos `N` bounding boxes no formato `[x0, y0, x1, y1]`, com valores no eixo $x$ no intervalo [$0$, $W$] e no eixo $y$ no intervalo [$0$, $H$];\n",
        "    * `'labels'` (`Int64Tensor[N]`): o rótulo de classe de cada bounding box;\n",
        "    * `'image_id'` (`Int64Tensor[1]`): um identificador para a imagem. Esse identificador deve ser único entre todas as amostras do dataset e é usado pela função de avaliação;\n",
        "    * `'area'` (`Tensor[N]`): a área do bounding box, ou seja, (`x1`-`x0`) $\\times$ (`y1`-`y0`). Essa informação é utilizada no método de avaliação que estamos reaproveitando da biblioteca do dataset COCO;\n",
        "    * `'iscrowd'` (`UInt8Tensor[N]`): instâncias com o atributo `iscrowd=True` serão ignoradas durante a avaliação da performance da rede. No nosso caso, assumiremos sempre que `iscrowd=False` devido às características do dataset Penn-Fudan;\n",
        "    * (opcional) `'masks'` (`UInt8Tensor[N, H, W]`): as máscaras de segmentação para cada objeto na cena;\n",
        "    * (opcional) `'keypoints'` (`FloatTensor[N, K, 3]`): os `K` keypoints presentes em cada objeto no formato `[x, y, visibility]` (na última dimensão do tensor).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, root, transforms=None):\n",
        "        \n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6f3ZOTJ4Km9"
      },
      "source": [
        "Imprimindo uma amostra para demonstrar como está estruturado cada sample do dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEARO4B_ye0s"
      },
      "source": [
        "dataset = PennFudanDataset('PennFudanPed/')\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "Pode-se ver que o dataset retorna uma imagem no formato da biblioteca [PIL](https://pillow.readthedocs.io/en/stable/) e um dicionário contendo várias informações dos bounding boxes na imagem e segmentações desses bounding boxes. É notável que a estrutura desses dados é consideravelmente mais complexa do que os datasets que foram usados/implementados nas aulas anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoAEkUgn4uEq"
      },
      "source": [
        "# Detecção em Imagens\n",
        "\n",
        "Os principais modelos de detecção são os da família das Regional CNNs (R-CNNs). Entre 2014 e 2016 foram propostos três artigo seminais usando como base R-CNNs:\n",
        "\n",
        "1.   A [R-CNN clássica](https://arxiv.org/pdf/1311.2524.pdf);\n",
        "2.   A [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf);\n",
        "3.   E a [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf).\n",
        "\n",
        "Como os nomes indicam, as arquiteturas de detecção foram sendo otimizadas para serem rodadas em tempo real, de modo que pudessem ser usadas em aplicações como carros autônomos. Toda a família das R-CNNs parte de um princípio consideravelmente simples: alimentar subregiões de imagens para uma CNN pré-treinada em um dataset grande e obter uma predição para qual tipo de objeto está naquele patch da imagem. R-CNNs clássicas usavam a CNN apenas como extratora de features, contando com uma seleção de patches feita pelo algoritmo [Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/) e uma classificação usando um [SVM](https://link.springer.com/article/10.1007%2FBF00994018) para realizar a inferência da classe do patch. O esquema de uma R-CNN clássica pode ser visto nas imagens abaixo.\n",
        "\n",
        "![R-CNN Clássica 1](https://miro.medium.com/max/700/1*REPHY47zAyzgbNKC6zlvBQ.png)\n",
        "\n",
        "![R-CNN Clássica 2](https://miro.medium.com/max/556/1*NX5yYTi-eQjP0pMWs3UbUg.png)\n",
        "\n",
        "R-CNNs clássicas são altamente limitadas em qualidade de detecção e eficiência por uma série de fatores, incluindo a quantidade enorme de regiões propostas pelo Selective Search (~2000 regiões por imagem, que tinham que ser passadas pela CNN) e a impossiblidade de backpropagar erro de acordo com as predições de classe do SVM que fazia a inferência. Visando mitigar parte desses problemas, a primeira arquitetura que usava a própria CNN para realizar as inferências das classes dos patches foi a Fast R-CNN. A Fast R-CNN permitia uma integração do pipeline de inferência com a extração de features, de forma que a extração poderia ser tunada para as especificações do dataset, ao invés de usar uma CNN pré-treinada sem fine-tuning. \n",
        "\n",
        "![Fast R-CNN](https://miro.medium.com/max/700/1*0pMP3aY8blSpva5tvWbnKA.png)\n",
        "\n",
        "Além de ganhos expressivos em qualidade de detecção as Fast R-CNNs faziam o forward da imagem apenas uma vez pela CNN e selecionavam os patches candidatos usando os feature maps retornados pela CNN, ao invés de passar os ~2000 patches separadamente pela rede como fazia a R-CNN. Isso trouxe ganhos de desempenho de treinamento de aproximadamente uma ordem de magnitude e ganhos no tempo de teste entre uma e duas ordens de magnitude, como pode ser visto na figura abaixo.\n",
        "\n",
        "![Time Comparison 1](https://miro.medium.com/max/700/1*m2QO_wbUPA05mY2q4v7mjg.png)\n",
        "\n",
        "Fast R-CNNs ainda usavam um algoritmo de seleção de regiões não integrado à arquitetura, o que impossibilitava o uso em tempo real dessa arquitetura, já que uma imagem levava aproximadamente 2 segundos para ser processada. O gargalo de eficiência da arquitetura passou, então, a ser o algoritmo de proposição de regiões e não mais os forwards dos patches pela CNN. Pouco tempo depois, as Faster R-CNNs foram propostas por parte do mesmo time de pesquisadores que propôs a R-CNN clássica e a Fast R-CNN, finalmente integrando todo o pipeline de detecção. Além da completa e total falta de criatividade dos pesquisadores para nomear arquiteturas, é notável que as Faster R-CNNs permitem um treinamento end-to-end da arquitetura toda ao incluir uma Region Proposal Network (RPN) para substituir o Selective Search, que era o gargalo da Fast R-CNN. Os gradientes da RPN propagavam para a CNN extratora de features, ajudando no treinamento, bem como se aproveitavam dos gradientes da classificação dos patches. Dessa forma, além do considerável ganho de eficiência, todas as subtarefas do treinamento podiam se ajudar mutuamente.\n",
        "\n",
        "![Faster R-CNN](https://miro.medium.com/max/700/1*pSnVmJCyQIRKHDPt3cfnXA.png)\n",
        "\n",
        "Em 2016, pela primeira vez havia uma arquitetura end-to-end para detecção de objetos que podia ser executada numa escala próxima de em tempo real, já que, mais uma vez, o ganho de eficiência em relação às Fast R-CNNs foi por volta de uma ordem de magnitude. Implementações eficientes de Faster R-CNNs conseguem processar até 5 frames/segundo.\n",
        "\n",
        "![Time Comparison 2](https://miro.medium.com/max/700/1*4gGddZpKeNIPBoVxYECd5w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6OfiQkUrfnr"
      },
      "source": [
        "# Aproveitando as arquiteturas do torchvision\n",
        "\n",
        "Há duas formas principais para se reaproveitar as arquiteturas de detecção pré-treinadas do torchvision:\n",
        "\n",
        "1.   Começar com o modelo pré-treinado e apenas fazer o fine-tuning da última camada;\n",
        "2.   Modificar o modelo para adicionar um backbone diferente do padrão (ResNet-50), a qual pode ser demasiadamente grande para algumas aplicações.\n",
        "\n",
        "No nosso exemplo, como a ResNet-50/Faster R-CNN já haviam sido treinadas com um dataset consideravelmente parecido com o Penn-Fudan (o [MS-COCO](http://cocodataset.org)), queremos aproveitar toda a estrutura e modificar apenas o módulo do preditor de bounding boxes para o novo dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "# The Penn-Fudan dataset has only two classes: background and person.\n",
        "num_classes = 2\n",
        "\n",
        "# Load an instance segmentation model pre-trained on COCO.\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# Getting the number of input features for the classifier.\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained Box Prediction Head with a new one.\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "print(model.roi_heads.box_predictor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhxfki7kiU1p"
      },
      "source": [
        "# Instance Segmentation\n",
        "\n",
        "Dentre as tarefas clássicas da área de Visão Computacional, Instance Segmentation é considerada uma das mais difíceis e não havia forma eficiente de realizá-la usando Deep Learning até 2017. Essa tarefa requer que o algoritmo tenha discernimento entre diferentes objetos da mesma classe numa mesma imagem, sabendo discerní-los. A diferença entre Instance Segmentation e Semantic Segmentation é ilustrada nas imagens abaixo.\n",
        "\n",
        "![Semantic Segmentation](https://i.loli.net/2017/09/12/59b6d1993b0fe.png)\n",
        "\n",
        "![Instance Segmentation](https://i.loli.net/2017/09/12/59b6f7187a586.png)\n",
        "\n",
        "Em 2017 foi proposta a [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf), que é considerada até hoje o estado-da-arte para Instance Segmentation. A Mask R-CNN (novamente proposta pelo mesmo time da R-CNN clássica, Fast R-CNN e Faster R-CNN) conta com uma Faster R-CNN para realizar a detecção dos objetos, passando a saída de cada patch detectado por uma rede de segmentação (i.e. FCN, U-Net, SegNet...), como pode ser visto nas imagens abaixo.\n",
        "\n",
        "![Mask R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image04.png)\n",
        "\n",
        "![Mask R-CNN Details](https://www.dropbox.com/s/b1hnjon5lxp2qps/Mask_RCNN_Scheme.png?dl=1)\n",
        "\n",
        "Como vamos aproveitar a rede pré-treinada do torchvision, também precisamos modificar o módulo de segmentação da Mask R-CNN para o novo dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3-8QV8eiHhq"
      },
      "source": [
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "# Getting the number of input features for the mask classifier.\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "\n",
        "# Replacing the mask predictor with a new one.\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                   hidden_layer,\n",
        "                                                   num_classes)\n",
        "\n",
        "print(model.roi_heads.mask_predictor)\n",
        "\n",
        "# Casting model to device.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WXLwePV5ieP"
      },
      "source": [
        "# Funções de treinamento e avaliação do desempenho da rede\n",
        "\n",
        "O torchvision possui em um de seus branches funções de avaliação para o dataset COCO que podem ser reaproveitadas para a nossa tarefa. Precisamos, portanto, copiar o repositório abaixo para usá-las."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDb7PBw55b-"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9e_pdv54nG"
      },
      "source": [
        "Como os métodos tradicionais do módulo [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) do torchvision foram pensados para tratar apenas de tarefas de rotulação esparsa (i.e. Classificação), eles não estão preparados para realizar mudanças nos rótulos de tarefas como segmentação ou detecção. Portanto, o arquivo transforms.py (impresso abaixo) sobrescreve algumas dessas transformações do torchvision para lidar com a tarefa específica em mãos.\n",
        "\n",
        "PS.: não se faz necessário realizar nenhum tipo de normalização nos dados, já que isso já é feito internamente pela Mask R-CNN pré-treinada do torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omGp9iqYs8Km"
      },
      "source": [
        "!cat transforms.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    \n",
        "    transforms = []\n",
        "    \n",
        "    # Converts the image, a PIL image, into a PyTorch Tensor.\n",
        "    transforms.append(T.ToTensor())\n",
        "    \n",
        "    if train:\n",
        "        \n",
        "        # During training, randomly flip the training images and ground truths\n",
        "        # for data augmentation.\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    \n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "# Juntando todos os passos\n",
        "\n",
        "Como não há diferença padrão entre treino e teste no dataset Penn-Fudan, podemos usar a função *torch.utils.data.Subset()* para realizar a divisão aleatória desses dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "# Instantiating and dividing train and test sets.\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# Instantiating Dataloaders for the Datasets.\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "# Setando o otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoenkCj18C4h"
      },
      "source": [
        "# Construindo um otimizador.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params,\n",
        "                            lr=0.005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLD92rk3mEhs"
      },
      "source": [
        "# Scheduler para o Learning Rate\n",
        "\n",
        "Algoritmos menos adaptáveis, como o SGD tradicional sem regularizações adicionais (como o caso do Adagrad, RMSProp ou Adam), se beneficiam consideravelmente de uma diminuição do Learning Rate ao longo das epochs. O Pytorch possui várias opções no pacote [*optim*](https://pytorch.org/docs/stable/optim.html) para diferentes [agendadores de diminuição do Learning Rate](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) ao longo do treinamento da rede. A classe [*StepLR*](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR), por exemplo, divide o Learning Rate do algoritmo de otimização a cada $N$ epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5nxkbS1mC5z"
      },
      "source": [
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-O_DfbxvCxL"
      },
      "source": [
        "# Funções de treino e teste\n",
        "\n",
        "Abaixo são implementadas as funções de treino e teste para a nossa tarefa de Instance Segmentation. É notável a complexidade adicional dessas funções comparadas com as funções de treino e teste que temos visto. Isso se dá primariamente devido ao número variável de bounding boxes e máscaras de segmentação que podem ter em cada imagem, já que o número de pessoas nas imagens não é fixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRwU04n_P0Eh"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "\n",
        "def train(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    \n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        image = list(img.to(device) for img in image)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(image)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s57gwcD-Qg4n"
      },
      "source": [
        "# Intersection over Union computation for both segmentation and bound boxes.\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = ['bbox']\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append('segm')\n",
        "    return iou_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "# Finalmente treinando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-h4OWK0aoc"
      },
      "source": [
        "# Training for 10 epochs.\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # Training function.\n",
        "    train(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    \n",
        "    # Update the learning rate via scheduler.\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # Evaluating on the test dataset.\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mYGFLxkO8F"
      },
      "source": [
        "# Visualizando uma predição do modelo\n",
        "\n",
        "Abaixo selecionamos uma amostra do subset de teste e realizamos a predição da Mask R-CNN nessa amostra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[0]\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmN602iKsuey"
      },
      "source": [
        "Novamente, ao contrário das arquiteturas anteriores, temos uma saída de tamanho variável para as regressões da nossa Mask R-CNN devido ao número variável de pedestres nas imagens. Nessa imagem duas pessoas foram identificadas pela Mask R-CNN, portanto, todos os elementos do dicionário terão dois subconjuntos de saídas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkmb3qUu6zw3"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT21rzotFbH"
      },
      "source": [
        "Imprimindo a imagem e as predições da Mask R-CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpqN9t1u7B2J"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "rect0 = patches.Rectangle((prediction[0]['boxes'][0, 0], prediction[0]['boxes'][0, 1]),\n",
        "                          prediction[0]['boxes'][0, 2] - prediction[0]['boxes'][0, 0],\n",
        "                          prediction[0]['boxes'][0, 3] - prediction[0]['boxes'][0, 1],\n",
        "                          linewidth=2, edgecolor='red', facecolor='none')\n",
        "rect1 = patches.Rectangle((prediction[0]['boxes'][1, 0], prediction[0]['boxes'][1, 1]),\n",
        "                          prediction[0]['boxes'][1, 2] - prediction[0]['boxes'][1, 0],\n",
        "                          prediction[0]['boxes'][1, 3] - prediction[0]['boxes'][1, 1],\n",
        "                          linewidth=2, edgecolor='green', facecolor='none')\n",
        "\n",
        "# Original image.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "\n",
        "ax.imshow(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "ax.set_yticks([])\n",
        "ax.set_xticks([])\n",
        "ax.set_title('Image')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# First detection/segmentation.\n",
        "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "ax[0].imshow(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "ax[0].add_patch(rect0)\n",
        "ax[0].set_yticks([])\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_title('First Detection')\n",
        "\n",
        "ax[1].imshow(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy(), 'jet')\n",
        "ax[1].set_yticks([])\n",
        "ax[1].set_xticks([])\n",
        "ax[1].set_title('First Segmentation')\n",
        "\n",
        "ax[2].imshow(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "ax[2].imshow(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy(), 'jet', alpha=0.3)\n",
        "ax[2].set_yticks([])\n",
        "ax[2].set_xticks([])\n",
        "ax[2].set_title('First Segmentation Overlay')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Second detection/segmentation.\n",
        "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "ax[0].imshow(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "ax[0].add_patch(rect1)\n",
        "ax[0].set_yticks([])\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_title('Second Detection')\n",
        "\n",
        "ax[1].imshow(prediction[0]['masks'][1, 0].mul(255).byte().cpu().numpy(), 'jet')\n",
        "ax[1].set_yticks([])\n",
        "ax[1].set_xticks([])\n",
        "ax[1].set_title('Second Segmentation')\n",
        "\n",
        "ax[2].imshow(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "ax[2].imshow(prediction[0]['masks'][1, 0].mul(255).byte().cpu().numpy(), 'jet', alpha=0.3)\n",
        "ax[2].set_yticks([])\n",
        "ax[2].set_xticks([])\n",
        "ax[2].set_title('Second Segmentation Overlay')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "\n",
        "\n",
        "<!-- And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1. -->"
      ]
    }
  ]
}