{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S03A02_1-Semantic_Segmentation_Architectures.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQi1bQq1MTNh"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY5VBxjD4gkJ"
      },
      "source": [
        "# Downloading the JSRT dataset.\n",
        "!rm -r ./*\n",
        "!mkdir jsrt\n",
        "!wget https://www.dropbox.com/sh/grjpx4oeacg32c0/AAAPpBgABESAWz-se4eCP9-Ya -O jsrt.zip\n",
        "!unzip -q jsrt.zip -d jsrt\n",
        "!ls jsrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCr1F07lJ3lN"
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from skimage import io\n",
        "from skimage import transform\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZHHFWXeif2A"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 50,      # Number of epochs.\n",
        "    'dataset': 'jsrt',    # Dataset.\n",
        "    'task': 'lungs',      # Task.\n",
        "#     'task': 'lungs',      # Task.\n",
        "#     'task': 'heart',      # Task.\n",
        "    'n_classes': 2,       # Number of classes in segmentation task.\n",
        "    'network': 'unet',    # Network architecture.\n",
        "    'lr': 1e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 8,      # Mini-batch size.\n",
        "    'w_size': 256,        # Width size for image resizing.\n",
        "    'h_size': 256,        # Height size for image resizing.\n",
        "    'show_freq': 2,       # Frequency for showing predictions.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EemBZ7ISIYP"
      },
      "source": [
        "# Random initialization for weights and biases.\n",
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwMOIO2uleJ"
      },
      "source": [
        "def evaluate(prds, labs, num_classes):\n",
        "    \n",
        "    iou_list = []\n",
        "    f1_list = []\n",
        "    \n",
        "    for prd, lab in zip(prds, labs):\n",
        "        \n",
        "        iou = metrics.jaccard_score(lab.ravel(), prd.ravel())\n",
        "        f1 = metrics.f1_score(lab.ravel(), prd.ravel())\n",
        "        \n",
        "        iou_list.append(iou)\n",
        "        f1_list.append(f1)\n",
        "        \n",
        "    iou_list = np.asarray(iou_list)\n",
        "    f1_list = np.asarray(f1_list)\n",
        "    \n",
        "    return iou_list, f1_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqNqprCdmdQ4"
      },
      "source": [
        "# Dataloaders Customizados\n",
        "\n",
        "Os frameworks de Deep Learning modernos (i.e. MXNet e Pytorch) permitem a criação de dataloaders customizados ao se sobrescreverem classes desses frameworks. Esse tipo de dataloader é especialmente útil no caso de tarefas diferentes das de classificação que temos visto até agora (i.e. segmentação e detecção de imagens, processamento de áudio, processamento de linguagem, natural, etc), nas quais os labels podem ser mais esparsos ou densos que rótulos de classificação.\n",
        "\n",
        "Usando como base as classes [*Dataloader*](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) e [*Dataset*](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) do subpacote [*data*](https://pytorch.org/docs/stable/data.html) do Pytorch, podemos customizar a leitura dos dados ao mesmo tempo em que paralelizamos a leitura das amostras dos nossos batches. A paralelização da leitura de amostras em várias [threads](https://www.tutorialspoint.com/python/python_multithreading) torna o uso da GPU o mais eficiente possível, já que não é necessário deixar a GPU esperando pelo carregamento de novas amostras para compor um batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJImDO6sMTVX"
      },
      "source": [
        "# Class that reads a sequence of image paths from a text file and creates a data.Dataset with them.\n",
        "class ListDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, mode, dataset, task, resize_to=(256, 256), in_channels=1, num_classes=2):\n",
        "\n",
        "        # Initializing variables.\n",
        "        self.root = root\n",
        "        self.mode = mode\n",
        "        self.dataset = dataset\n",
        "        self.task = task\n",
        "        self.resize_to = resize_to\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Creating list of image files.\n",
        "        self.imgs = self.make_dataset()\n",
        "        \n",
        "        # Check for consistency in list.\n",
        "        if len(self.imgs) == 0:\n",
        "\n",
        "            raise (RuntimeError('Found 0 images, please check the data set'))\n",
        "\n",
        "    def make_dataset(self):\n",
        "\n",
        "        # Making sure the mode is correct.\n",
        "        assert self.mode in ['train', 'test']\n",
        "\n",
        "        # Joining input paths.\n",
        "        self.img_path = os.path.join(self.root,\n",
        "                                     self.dataset,\n",
        "                                     'images')\n",
        "        self.msk_path = os.path.join(self.root,\n",
        "                                     self.dataset,\n",
        "                                     'ground_truths',\n",
        "                                     self.task)\n",
        "\n",
        "        # Reading paths from file.\n",
        "        items = [l.strip('\\n') for l in open(os.path.join(self.root,\n",
        "                                                          self.dataset,\n",
        "                                                          self.task + '_' + self.mode + '.txt')).readlines()]\n",
        "\n",
        "        # Returning list.\n",
        "        return items\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Reading items from list and mounting .\n",
        "        f = self.imgs[index]\n",
        "        \n",
        "        img_path = os.path.join(self.img_path, f)\n",
        "        msk_path = os.path.join(self.msk_path, f)\n",
        "        \n",
        "        # Reading images.\n",
        "        img = io.imread(img_path)\n",
        "        msk = io.imread(msk_path)\n",
        "\n",
        "        # Resising images.\n",
        "        img = transform.resize(img, self.resize_to,\n",
        "                               preserve_range=True, order=1)\n",
        "        msk = transform.resize(msk, self.resize_to,\n",
        "                               preserve_range=True, order=0)\n",
        "\n",
        "        # Casting images to the appropriate dtypes.\n",
        "        img = img.astype(np.float32)\n",
        "        msk = msk.astype(np.int64)\n",
        "        \n",
        "        if self.num_classes == 2:\n",
        "            msk[msk > 0] = 1\n",
        "\n",
        "        # Normalization (z-score).\n",
        "        if self.in_channels == 1:\n",
        "            img = (img - img.mean()) / img.std()\n",
        "        else:\n",
        "            for i in range(img.shape[2]):\n",
        "                img[:,:,i] = (img[:,:,i] - img[:,:,i].mean()) / img[:,:,i].std()\n",
        "\n",
        "        # Fixing channel dimension.\n",
        "        if self.in_channels == 1:\n",
        "\n",
        "            # If number of input channels is 1, simply add a new dimension.\n",
        "            img = np.expand_dims(img, axis=0) # From (H, W) to (C=1, H, W).\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            # Else, transpose array.\n",
        "            img = np.transpose(img, (2, 0, 1)) # From (H, W, C) to (C, H, W).\n",
        "\n",
        "        # Turning numpy arrays to tensors.\n",
        "        img = torch.from_numpy(img)\n",
        "        msk = torch.from_numpy(msk)\n",
        "\n",
        "        # Returning to iterator.\n",
        "        return img, msk\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.imgs)\n",
        "\n",
        "# Instantiating dataloaders.\n",
        "root = './'\n",
        "\n",
        "# Setting datasets.\n",
        "train_set = ListDataset(root,\n",
        "                        'train',\n",
        "                        args['dataset'],\n",
        "                        args['task'],\n",
        "                        (args['h_size'], args['w_size']))\n",
        "test_set = ListDataset(root,\n",
        "                       'test',\n",
        "                       args['dataset'],\n",
        "                       args['task'],\n",
        "                       (args['h_size'], args['w_size']))\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batch_size=1,\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR8JAGoDjA4p"
      },
      "source": [
        "for i, batch_data in enumerate(train_loader):\n",
        "    \n",
        "    img, lab = batch_data\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    ax[0].imshow(img[0, 0].numpy(), cmap=plt.get_cmap('gray'))\n",
        "    ax[0].set_yticks([])\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_title('Image')\n",
        "    \n",
        "    ax[1].imshow(lab[0].numpy(), cmap=plt.get_cmap('gray'))\n",
        "    ax[1].set_yticks([])\n",
        "    ax[1].set_xticks([])\n",
        "    ax[1].set_title('Mask')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMH6KkQO3dPC"
      },
      "source": [
        "# Arquiteturas Encoder-Decoder\n",
        "\n",
        "FCNs foram as primeiras redes neurais profundas para Segmentação Semântica, porém o uso de interpolações bilineares para alimentar as camadas convolucionais finais da rede pode tornar a segmentação pouco sensível a detalhes pequenos nas imagens de entrada, mesmo com o uso de **Skip Connections**. Entre 2015 e 2017 foram propostas as primeiras arquiteturas **Encoder-Decoder** para Segmentação Semântica que, ao invés de realizarem interpolações para recuperar a resolução espacial dos feature maps, usam **Convoluções Transpostas** com filtros treináveis. Isso torna o processo de upsampling dos feature maps de baixa resolução mais fidedigno a detalhes nas imagens de entrada.\n",
        "\n",
        "![**Convolução**](http://deeplearning.net/software/theano_versions/dev/_images/no_padding_no_strides.gif)![**Convolução Transposta**](http://deeplearning.net/software/theano_versions/dev/_images/no_padding_no_strides_transposed.gif)\n",
        "\n",
        "As regiões com os feature maps de menor resolução espacial em arquiteturas Encoder-Decoder são chamadas de bottlenecks dessas redes e as camadas de **Convolução** e **Convolução Transposta** dos dois lados do bottleneck de uma rede são muito frequentemente simétricas, como pode ser visto na imagem abaixo.\n",
        "\n",
        "![Encoder-Decoder](https://www.dropbox.com/s/kd40qyqmusweovu/Unet.png?dl=1)\n",
        "\n",
        "Duas arquiteturas Encoder-Decoder para segmentação se destacam na literatural atual: as [**U-Nets**](https://arxiv.org/pdf/1505.04597.pdf) e as [**SegNets**](https://arxiv.org/pdf/1511.00561.pdf), possuindo desempenho similar na maioria das aplicações. U-Nets seguem a filosofia da [**VGG**](https://arxiv.org/abs/1409.1556) com convoluções 3x3 e padding de tamanho 1 com a resolução espacial dos feature maps sendo controlada por Max Poolings entre os blocos convolucionais. As camadas do Decoder das U-Nets recebem os feature maps computados pelas suas camadas simétricas no Encoder concatenados à saída da camada anterior. Essa operação é análoga às Skip Connections das FCNs, porém feitas para cada par de camadas simétricas (ou seja, que geram filtros com as mesmas resoluções espaciais). Isso faz com que os decoders das U-Nets possuam consideravelmente mais filtros que os Encoders, como também pode ser visto no esquema abaixo.\n",
        "\n",
        "![U-Nets](https://www.dropbox.com/s/woistektygmv693/unets.png?dl=1)\n",
        "\n",
        "Assim como nas FCNs, as U-Nets implementam os Skip Connections com concanteções no eixo dos canais, como mostrado na imagem a seguir.\n",
        "\n",
        "![Skip U-Nets](https://www.dropbox.com/s/ao9878yf2ke5ce1/Nets_UNet_Petro.png?dl=1)\n",
        "\n",
        "Enquanto U-Nets concatenam os feature maps de saída das camadas do Encoder como inputs para as camadas do Decoder, as SegNets se aproveitam dos índices das operações de Max Pooling 2D do Encoder para computar o Decoder. A diferença mais marcante entre as U-Nets e as SegNets se dá na forma como as resoluções de entrada são restauradas após o bottleneck, ou seja, no Decoder.\n",
        "\n",
        "![SegNets](https://www.dropbox.com/s/4605yaowoxuqbfu/segnets.png?dl=1)\n",
        "\n",
        "Os índices dos Max Poolings das camadas do Encoder são utilizados para gerar feature maps esparsos no Decoder, os quais são passados para **Convoluções Transpostas** para realizar o upsampling.\n",
        "\n",
        "![Max Pooling Indices](https://www.dropbox.com/s/ub8y5t27u1ucdwd/pool_indices.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgFCtKuV53Ri"
      },
      "source": [
        "# Atividade Prática: Implementando U-Net\n",
        "\n",
        "1.   Implementar as classes *EncoderBlock* e *DecoderBlock*. Ambas *\\_\\_init\\_\\_()* e *forward()* devem ser complementadas para as classes.\n",
        "2.   Usar as classes *EncoderBlock* e *DecoderBlock* na classe *U-Net*.\n",
        "3.   Instanciar um otimizador e uma loss.\n",
        "4.   Complementar trechos TO DO nas funções *train()* e *test()*.\n",
        "\n",
        "PS.: Todas as convoluções de uma U-Net são 3x3 com padding igual a 1, menos a última que tem kernel 1x1 para realizar a predição densa para cada pixels de acordo com o número de classes.\n",
        "\n",
        "### Parte 2\n",
        "Comparar os resultados da U-Net e SegNet (já implementada) nas tasks 'lung', 'heart' e, principalmente, 'clavicles'. Essas tasks podem ser definidas no parâmetro 'task' do dicionário *args*. A tarefa de segmentação de clavículas é dentre todas a mais difícil devido ao grande desbalanceamento entre classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE0Z3zgHTorq"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        # TO DO: each encoder block be composed of:\n",
        "        #        1) 2d conv inputting in_channels and outputting out_channels;\n",
        "        #        2) batch norm;\n",
        "        #        3) ReLU;\n",
        "        #        4) 2d conv outputting out_channels;\n",
        "        #        5) batch norm;\n",
        "        #        6) ReLU;\n",
        "        #        7) max pooling 2d with kernel 2x2 and stride of 2.\n",
        "        self.encode = # ...\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TO DO: implement forward on self.encode.\n",
        "        return # ...\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # TO DO: each decoder block be composed of:\n",
        "        #        1) channel dropout;\n",
        "        #        2) 2d conv inputting in_channels and outputting\n",
        "        #           middle_channels;\n",
        "        #        3) batch norm;\n",
        "        #        4) 2d conv outputting middle_channels;\n",
        "        #        5) batch norm;\n",
        "        #        6) ReLU;\n",
        "        #        7) transposed 2d conv outputting out_channels with kernel 2x2,\n",
        "        #           stride of 2, padding and output_padding of 0.\n",
        "        self.decode = # ...\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TO DO: implement forward on self.decode.\n",
        "        return # ...\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # TO DO: this U-Net will be composed of 4 Encoder Blocks and 4\n",
        "        #        Decoder Blocks:\n",
        "        #        1) Encoder Block inputting input_channels and outputting 64\n",
        "        #           channels;\n",
        "        #        2) Encoder Block outputting 128 channels;\n",
        "        #        3) Encoder Block 256 channels;\n",
        "        #        4) Encoder Block outputting 512 channels;\n",
        "        #        5) Decoder Block inputting 512 channels, 1024 middle channels\n",
        "        #           and outputting 512 channels;\n",
        "        #        6) Decoder Block inputting 1024 channels, 512 middle channels\n",
        "        #           and outputting 256 channels;\n",
        "        #        7) Decoder Block inputting 512 channels, 256 middle channels\n",
        "        #           and outputting 128 channels;\n",
        "        #        8) Decoder Block inputting 256 channels, 128 middle channels\n",
        "        #           and outputting 64 channels;\n",
        "        #        9) 2d conv block (conv + batch norm + ReLU) outputting 64\n",
        "        #           channels;\n",
        "        #        10) 2d conv block (conv + batch norm + ReLU) outputting 64\n",
        "        #            channels;\n",
        "        #        11) 2d conv block with kernel 1x1 outputting num_classes\n",
        "        #            channels. This is the pixel classification layer.\n",
        "        \n",
        "        self.enc1 = # Encoder Block...\n",
        "        self.enc2 = # Encoder Block...\n",
        "        self.enc3 = # Encoder Block...\n",
        "        self.enc4 = # Encoder Block...\n",
        "        \n",
        "        self.center = # Decoder Block...\n",
        "        self.dec4 = # Decoder Block...\n",
        "        self.dec3 = # Decoder Block...\n",
        "        self.dec2 = # Decoder Block...\n",
        "\n",
        "        self.dec1 = # Channel dropout + 2 conv blocks.\n",
        "\n",
        "        self.final = # 1x1 conv outputting num_classes channels.\n",
        "\n",
        "        # Random initialization for weights.\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TO DO: implement forward on Encoder Blocks.\n",
        "        enc1_out = # ...\n",
        "        enc2_out = # ...\n",
        "        enc3_out = # ...\n",
        "        enc4_out = # ...\n",
        "\n",
        "        # TO DO: implement forward on Decoder Blocks. Each Decoder receives the\n",
        "        #        output of the last layer concatenated with the output of an\n",
        "        #        Encoder Block with compatible size. See U-Net architectural\n",
        "        #        scheme above.\n",
        "        center_out = # ...\n",
        "        dec4_out = # ...\n",
        "        dec3_out = # ...\n",
        "        dec2_out = # ...\n",
        "        \n",
        "        # TO DO: implement forward on the final 2 conv blocks and final 1x1\n",
        "        #        conv to predict classes.\n",
        "        dec1_out = # ...\n",
        "\n",
        "        final_out = # ...\n",
        "\n",
        "        # TO DO: return output.\n",
        "        return # ...\n",
        "\n",
        "net = UNet(1, num_classes=args['n_classes']).to(args['device'])\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3T7vq4kRSHP"
      },
      "source": [
        "class SegNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        \n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = num_classes\n",
        "\n",
        "        self.num_channels = input_channels\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_conv_00 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=self.input_channels,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "                nn.BatchNorm2d(64)\n",
        "        ])\n",
        "        self.encoder_conv_01 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "                nn.BatchNorm2d(64)\n",
        "            ])\n",
        "        self.encoder_conv_10 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ])\n",
        "        self.encoder_conv_11 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=128,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ])\n",
        "        self.encoder_conv_20 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=128,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.encoder_conv_21 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.encoder_conv_22 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.encoder_conv_30 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.encoder_conv_31 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.encoder_conv_32 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.encoder_conv_40 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.encoder_conv_41 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.encoder_conv_42 = nn.Sequential(*[\n",
        "            nn.Conv2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_convtr_42 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.decoder_convtr_41 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.decoder_convtr_40 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.decoder_convtr_32 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.decoder_convtr_31 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ])\n",
        "        self.decoder_convtr_30 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=512,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.decoder_convtr_22 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.decoder_convtr_21 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ])\n",
        "        self.decoder_convtr_20 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=256,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ])\n",
        "        self.decoder_convtr_11 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=128,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ])\n",
        "        self.decoder_convtr_10 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=128,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ])\n",
        "        self.decoder_convtr_01 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ])\n",
        "        self.decoder_convtr_00 = nn.Sequential(*[\n",
        "            nn.ConvTranspose2d(in_channels=64,\n",
        "                out_channels=self.output_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1)\n",
        "        ])\n",
        "\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Encoder\n",
        "        dim_0 = x.size()\n",
        "\n",
        "        # Encoder Stage - 1\n",
        "        x_00 = F.relu(self.encoder_conv_00(x))\n",
        "        x_01 = F.relu(self.encoder_conv_01(x_00))\n",
        "        x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 2\n",
        "        dim_1 = x_0.size()\n",
        "        x_10 = F.relu(self.encoder_conv_10(x_0))\n",
        "        x_11 = F.relu(self.encoder_conv_11(x_10))\n",
        "        x_1, indices_1 = F.max_pool2d(x_11, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 3\n",
        "        dim_2 = x_1.size()\n",
        "        x_20 = F.relu(self.encoder_conv_20(x_1))\n",
        "        x_21 = F.relu(self.encoder_conv_21(x_20))\n",
        "        x_22 = F.relu(self.encoder_conv_22(x_21))\n",
        "        x_2, indices_2 = F.max_pool2d(x_22, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 4\n",
        "        dim_3 = x_2.size()\n",
        "        x_30 = F.relu(self.encoder_conv_30(x_2))\n",
        "        x_31 = F.relu(self.encoder_conv_31(x_30))\n",
        "        x_32 = F.relu(self.encoder_conv_32(x_31))\n",
        "        x_3, indices_3 = F.max_pool2d(x_32, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 5\n",
        "        dim_4 = x_3.size()\n",
        "        x_40 = F.relu(self.encoder_conv_40(x_3))\n",
        "        x_41 = F.relu(self.encoder_conv_41(x_40))\n",
        "        x_42 = F.relu(self.encoder_conv_42(x_41))\n",
        "        x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        \n",
        "        # Decoder\n",
        "        dim_d = x_4.size()\n",
        "\n",
        "        # Decoder Stage - 5\n",
        "        x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
        "        x_42d = F.relu(self.decoder_convtr_42(x_4d))\n",
        "        x_41d = F.relu(self.decoder_convtr_41(x_42d))\n",
        "        x_40d = F.relu(self.decoder_convtr_40(x_41d))\n",
        "        dim_4d = x_40d.size()\n",
        "\n",
        "        # Decoder Stage - 4\n",
        "        x_3d = F.max_unpool2d(x_40d, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
        "        x_32d = F.relu(self.decoder_convtr_32(x_3d))\n",
        "        x_31d = F.relu(self.decoder_convtr_31(x_32d))\n",
        "        x_30d = F.relu(self.decoder_convtr_30(x_31d))\n",
        "        dim_3d = x_30d.size()\n",
        "\n",
        "        # Decoder Stage - 3\n",
        "        x_2d = F.max_unpool2d(x_30d, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
        "        x_22d = F.relu(self.decoder_convtr_22(x_2d))\n",
        "        x_21d = F.relu(self.decoder_convtr_21(x_22d))\n",
        "        x_20d = F.relu(self.decoder_convtr_20(x_21d))\n",
        "        dim_2d = x_20d.size()\n",
        "\n",
        "        # Decoder Stage - 2\n",
        "        x_1d = F.max_unpool2d(x_20d, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
        "        x_11d = F.relu(self.decoder_convtr_11(x_1d))\n",
        "        x_10d = F.relu(self.decoder_convtr_10(x_11d))\n",
        "        dim_1d = x_10d.size()\n",
        "\n",
        "        # Decoder Stage - 1\n",
        "        x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
        "        x_01d = F.relu(self.decoder_convtr_01(x_0d))\n",
        "        x_00d = self.decoder_convtr_00(x_01d)\n",
        "        dim_0d = x_00d.size()\n",
        "\n",
        "        return x_00d\n",
        "    \n",
        "net = SegNet(1, num_classes=args['n_classes']).cuda()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuo6esmKQOCL"
      },
      "source": [
        "# TO DO: Setting optimizer.\n",
        "optimizer = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8p2oLwCQFzX"
      },
      "source": [
        "# TO DO: Setting loss.\n",
        "criterion = # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2XCR_Q_WH_B"
      },
      "source": [
        "# Training function.\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Average Meter for batch loss.\n",
        "    train_loss = []\n",
        "\n",
        "    # Lists for whole epoch loss.\n",
        "    labs_all, prds_all = [], []\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # TO DO: Casting to cuda variables (inps and labs).\n",
        "\n",
        "        \n",
        "        # TO DO: Clearing the gradients of optimizer.\n",
        "\n",
        "        # TO DO: Forwarding.\n",
        "        outs = # ...\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        loss = # ...\n",
        "\n",
        "        # TO DO: Computing backpropagation and taking optimizer step.\n",
        "\n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Appending images for epoch loss calculation.\n",
        "        labs_all.append(labs.detach().data.squeeze(0).cpu().numpy())\n",
        "        prds_all.append(prds)\n",
        "\n",
        "        # Updating loss meter.\n",
        "        train_loss.append(loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Transforming list into numpy array.\n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou, f1 = evaluate(prds_all, labs_all, args['n_classes'])\n",
        "\n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), iou.mean(), iou.std(), f1.mean(), f1.std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Average Meter for batch loss.\n",
        "    test_loss = []\n",
        "\n",
        "    # Lists for whole epoch loss.\n",
        "    labs_all, prds_all = [], []\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # TO DO: Casting to cuda variables.\n",
        "\n",
        "        \n",
        "        # TO DO: Forwarding.\n",
        "        outs = # ...\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        loss = # ...\n",
        "\n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Appending images for epoch loss calculation.\n",
        "        labs_all.append(labs.detach().data.squeeze(0).cpu().numpy())\n",
        "        prds_all.append(prds)\n",
        "\n",
        "        # Updating loss meter.\n",
        "        test_loss.append(loss.data.item())\n",
        "        \n",
        "        if i < 3 and epoch % args['show_freq'] == 0:\n",
        "            \n",
        "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            \n",
        "            ax[0].imshow(inps.detach().cpu().numpy().squeeze())\n",
        "            ax[0].set_yticks([])\n",
        "            ax[0].set_xticks([])\n",
        "            ax[0].set_title('Image')\n",
        "            \n",
        "            ax[1].imshow(labs.detach().data.squeeze(0).cpu().numpy().squeeze())\n",
        "            ax[1].set_yticks([])\n",
        "            ax[1].set_xticks([])\n",
        "            ax[1].set_title('Label')\n",
        "            \n",
        "            ax[2].imshow(prds.squeeze())\n",
        "            ax[2].set_yticks([])\n",
        "            ax[2].set_xticks([])\n",
        "            ax[2].set_title('Prediction')\n",
        "            \n",
        "            plt.show()\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    # Transforming list into numpy array.\n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou, f1 = evaluate(prds_all, labs_all, args['n_classes'])\n",
        "\n",
        "    # Printing test epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), iou.mean(), iou.std(), f1.mean(), f1.std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19ywBEJ-NdWy"
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}