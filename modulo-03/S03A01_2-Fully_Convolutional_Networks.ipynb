{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S03A01_2-Fully_Convolutional_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "display_name": "Python [conda env:anaconda2]",
      "language": "python",
      "name": "conda-env-anaconda2-py"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQi1bQq1MTNh"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCr1F07lJ3lN"
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision import models\n",
        "\n",
        "from skimage import io\n",
        "from skimage import transform\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZHHFWXeif2A"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 50,      # Number of epochs.\n",
        "    'n_classes': 3, #19,      # Number of classes in segmentation task.\n",
        "    'pretrained': True,   # Boolean indicating pretraining of backbone.\n",
        "    'skip': True,         # Boolean indicating presence of Skip Connections.\n",
        "    'lr': 1e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 8,      # Mini-batch size.\n",
        "    'w_size': 224,        # Width size for image resizing.\n",
        "    'h_size': 224,        # Height size for image resizing.\n",
        "    'show_freq': 5,      # Show predictions in images each show_freq epochs.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3zMFUtNkCO7"
      },
      "source": [
        "Pré-definindo uma função para iniciação aleatória de pesos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EemBZ7ISIYP"
      },
      "source": [
        "# Random initialization for weights and biases.\n",
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8kpZhdJj4bM"
      },
      "source": [
        "## Métricas usadas para Segmentação Semântica\n",
        "\n",
        "As métricas mais utilizadas na área de segmentação semântica (e em algumas aplicações em terefas de detecção também) são variações da Intersection over Union ($IoU$), também chamada de **Jaccard**.\n",
        "\n",
        "![IoU](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Intersection_over_Union_-_poor%2C_good_and_excellent_score.png/300px-Intersection_over_Union_-_poor%2C_good_and_excellent_score.png)\n",
        "\n",
        "Em tarefas de segmentação/detecção binária, se usa a $IoU$ simples, dada pela fórmula:\n",
        "\n",
        "$IoU = \\frac{\\|A\\ \\cap\\ B\\|}{\\|A\\ \\cup\\ B\\|} = \\frac{TP}{TP + FP + FN}$.\n",
        "\n",
        "No caso de problemas multiclasse, usa-se a média do $IoU$ para todas as classes, chamada de mean Intersection over Union ($mIoU$).\n",
        "\n",
        "A métrica F1 (conhecida também como **Dice**) também é utilizada nesses contextos e é dada pela fórmula:\n",
        "\n",
        "$F1 = \\frac{2TP}{2TP + FP + FN}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwMOIO2uleJ"
      },
      "source": [
        "# Computing mean Intersection over Union (mIoU).\n",
        "def evaluate(prds, labs):\n",
        "    \n",
        "    int_sum = np.zeros(args['n_classes'], dtype=np.float32)\n",
        "    uni_sum = np.zeros(args['n_classes'], dtype=np.float32)\n",
        "    \n",
        "    for prd, lab in zip(prds, labs):\n",
        "        \n",
        "        for c in range(args['n_classes']):\n",
        "\n",
        "            union = np.sum(lab.ravel() == c)\n",
        "            if union > 0:\n",
        "                uni_sum[c] += union\n",
        "                \n",
        "                intersection = np.sum(np.logical_and(lab.ravel() == c, prd.ravel() == c))\n",
        "                int_sum[c] += intersection\n",
        "    \n",
        "    return int_sum, uni_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqNqprCdmdQ4"
      },
      "source": [
        "# O dataset Pascal VOC 2012\n",
        "\n",
        "O dataset da competição [Pascal VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) contém rótulos para detecção e segmentação de imagens, como pode ser visto nas imagens abaixo. Ele e o dataset [COCO](http://cocodataset.org/) são os principais benchmarks da área de Visão Computacional atualmente em tarefas de segmentação semântica e detecção em imagens.\n",
        "\n",
        "![](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg)![](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfEp1KJ28BW"
      },
      "source": [
        "O Pytorch possui um dataloader específico pro Pascal VOC 2012."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJImDO6sMTVX"
      },
      "source": [
        "# Root directory for VOC.\n",
        "root = './'\n",
        "\n",
        "# Classes left in Pascal VOC 2012.\n",
        "voc_classes = [0, 89, 147]\n",
        "\n",
        "# All classes in Pascal VOC 2012.\n",
        "# voc_classes = [0, 14, 19, 33, 37, 38, 52, 57, 72, 75, 89, 94, 108, 112, 113, 128, 132, 147, 150, 220]\n",
        "\n",
        "\n",
        "# Custom Image Transform.\n",
        "class VOCImgTransform(object):\n",
        "    \n",
        "    def __call__(self, img):\n",
        "#         img = transforms.functional.five_crop(img, (args['h_size'], args['w_size']))\n",
        "        img = transforms.functional.center_crop(img, (args['h_size'], args['w_size']))\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "        img = transforms.functional.normalize(img,\n",
        "                                              mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "        \n",
        "        return img\n",
        "\n",
        "# Custom Target Transform.\n",
        "class VOCTarTransform(object):\n",
        "    \n",
        "    def to_sequential_labels(self, tar):\n",
        "        \n",
        "        for i, c in enumerate(voc_classes):\n",
        "            tar[tar == c] = i\n",
        "        \n",
        "        tar[tar > 2] = 0\n",
        "        return tar\n",
        "    \n",
        "    def __call__(self, tar):\n",
        "        tar = tar.convert(mode='I')\n",
        "        tar = transforms.functional.center_crop(tar, (args['h_size'], args['w_size']))\n",
        "        tar = transforms.functional.to_tensor(tar).squeeze().type(torch.int64)\n",
        "        \n",
        "        tar = self.to_sequential_labels(tar)\n",
        "        \n",
        "        return tar\n",
        "\n",
        "\n",
        "# Setting datasets.\n",
        "download_voc = not os.path.isdir('./VOCdevkit/')\n",
        "train_set = datasets.VOCSegmentation(root,\n",
        "                                     image_set='train',\n",
        "                                     download=download_voc,\n",
        "                                     transform=VOCImgTransform(),\n",
        "                                     target_transform=VOCTarTransform())\n",
        "val_set = datasets.VOCSegmentation(root,\n",
        "                                   image_set='val',\n",
        "                                   download=False,\n",
        "                                   transform=VOCImgTransform(),\n",
        "                                   target_transform=VOCTarTransform())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnOyAeiD1Mf0"
      },
      "source": [
        "Exibindo algumas imagens do Pascal VOC 2012 e seus respectivos rótulos a nível de pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR8JAGoDjA4p"
      },
      "source": [
        "# Filtering VOC dataset to iterate only over samples from predefined classes.\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "\n",
        "valid_classes = [0, 1, 2]\n",
        "    \n",
        "for i, batch_data in enumerate(train_set):\n",
        "    \n",
        "    imgs, labs = batch_data\n",
        "    \n",
        "    curr_labels = set(list(labs.numpy().ravel()))\n",
        "    \n",
        "    if len([c for c in valid_classes if c in curr_labels]) >= 2: \n",
        "    \n",
        "        train_indices.append(i)\n",
        "        \n",
        "        if i < 50:\n",
        "            \n",
        "            fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "            ax[0].imshow(imgs.numpy().transpose(1, 2, 0))\n",
        "            ax[0].set_yticks([])\n",
        "            ax[0].set_xticks([])\n",
        "            ax[0].set_title('Image')\n",
        "\n",
        "            ax[1].imshow(labs.numpy().squeeze() * 127, cmap=plt.get_cmap('gray'),\n",
        "                         vmin=0, vmax=255)\n",
        "            ax[1].set_yticks([])\n",
        "            ax[1].set_xticks([])\n",
        "            ax[1].set_title('Mask')\n",
        "\n",
        "            plt.show()\n",
        "            \n",
        "for i, batch_data in enumerate(val_set):\n",
        "    \n",
        "    imgs, labs = batch_data\n",
        "    \n",
        "    curr_labels = set(list(labs.numpy().ravel()))\n",
        "    \n",
        "    if len([c for c in valid_classes if c in curr_labels]) >= 2:\n",
        "    \n",
        "        val_indices.append(i)\n",
        "\n",
        "print(len(train_set), len(train_indices))\n",
        "print(len(val_set), len(val_indices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sduJRGUulg7I"
      },
      "source": [
        "Definindo dataloader customizado baseado no subsample do dataset padrão Pascal VOC 2012."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pdN3-PzlM_P"
      },
      "source": [
        "# Sampler for limiting classes to humans and automobiles.\n",
        "sampler_train = data.SubsetRandomSampler(train_indices)\n",
        "sampler_val = data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          sampler=sampler_train)\n",
        "val_loader = DataLoader(val_set,\n",
        "                        batch_size=1,\n",
        "                        num_workers=args['num_workers'],\n",
        "                        sampler=sampler_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3unT2T8PJA"
      },
      "source": [
        "# Fully Convolutional Networks\n",
        "\n",
        "[Fully Convolutional Networks (FCNs)](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) foram propostas em 2015 com o intuito de reaproveitar as CNNs pré-treinadas que já existiam para classificação de imagens como a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) e a [VGG](https://arxiv.org/abs/1409.1556) em tarefas de **segmentação**. Como temos visto nas últimas aulas, CNNs tradicionais possuem vários blocos convolucionais e de pooling no começo da arquitetura, seguidos de blocos Fully Connected (FC) no fim da rede, os quais realizam a tarefa de inferência. Uma tarefa de segmentação é essencialmente uma classificação de pixels, também conhecida como uma tarefa de **rotulação densa**, contrária à **rotulação esparsa** de uma task de classificação de imagens, em que há um único rótulo por imagem.\n",
        "\n",
        "![AlexNet](https://miro.medium.com/max/686/0*xPOQ3btZ9rQO23LK.png)\n",
        "\n",
        "![VGG](https://miro.medium.com/max/700/0*V1muWIDnPVwZUuEv.png)\n",
        "\n",
        "Também já fomos introduzidos ao conceito de Transfer Learning, que consiste de reaproveitar o conhecimento de um domínio e transferí-lo para outro domínio relativamente semelhante. Transfer Learning pode ser aplicado para transferir o conhecimento obtido num dataset como o ImageNet (para classificação de imagens) para uma tarefa de segmentação num dataset como o Pascal VOC 2012. Como o nome indica, **Fully Convolutional Networks** possuem apenas camadas convolucionais, substituindo as camadas FC por mais blocos convolucionais.\n",
        "\n",
        "![CNN para FCN](https://www.dropbox.com/s/qsovwd3vuqxkhka/fcns.png?dl=1)\n",
        "\n",
        "Percebe-se que à medida que as imagens vão passando pelos blocos convolucionais, vão perdendo resolução espacial e sendo representados por cada vez mais canais numa CNN tradicional. No caso das FCNs tradicionais, utiliza-se uma interpolação bilinear simples nos eixos $(H, W)$ nos feature maps que saem da última camada convolucional da CNN tradicional e entram nas novas camadas convolucionais que substituíram as FCs. Por exemplo, no caso da **AlexNet**, os feature maps que saem da última camada convolucional possuem dimensões $(B, C, H, W) = (B, 256, 7, 7)$. Após a interpolação bilinear, esse feature map passa a ter $(B, 256, 224, 224)$, com as últimas dimensões correspondendo à dimensão de entrada $(H_{in}, W_{in}) = (224, 224)$ das imagens do ImageNet.\n",
        "\n",
        "A última camada de uma FCN (ou de basicamente qualquer outra rede para segmentação semântica) deve ser uma convolução com o número de canais igual ao número de classes. Isso faz com que cada canal de saída sirva como preditor de uma classe numa FCN. A saída da rede deve ter, portanto, as dimensões $(B, \\#classes, H_{in}, W_{in})$, de forma que, se for passado um softmax na dimensão 1 (referente a $\\#classes$), obtenha-se uma distribuição de probabilidades das classes para cada pixel. Dessa forma, conseguimos transformar uma rede que trata um problema de rotulação esparsa (como uma CNN para classificação) em uma rede que realiza rotulação densa.\n",
        "\n",
        "FCNs tradicionais partem do pressuposto de que toda a **informação espacial** da imagem original está codificada nos canais que saem das camadas convolucionais da CNN que serve de backbone para a arquitetura. Isso pode não ser verdade principalmente para arquiteturas mais profundas, nas quais há uma discrepância muito grande as dimensões espaciais das imagens de entrada e as dimensões do output da última camada convolucional (i.e. de $(224,224)$ para $(7,7)$). Portanto, no próprio paper das FCNs os autores propuseram pela primeira vez o uso de **Skip Connections** em redes neurais profundas para reconhecimento de imagens. As Skip Connections nas FCNs alimentam as saídas das primeiras convoluções da rede para as últimas camadas inseridas no lugar das antigas camadas FC. É notável que os outputs dessas primeiras camadas possuem resoluções espaciais muito maiores que a saída da última camada convolucional, permitindo que o processo de inferência possa ser feito diretamente com feature maps de maior resolução espacial, resultando em uma segmentação mais detalhada.\n",
        "\n",
        "![Skip Connections FCNs](https://www.dropbox.com/s/r9cvc14fju7kvpw/skip_connections.png?dl=1)\n",
        "\n",
        "Skip Connections em FCNs permitem duas coisas:\n",
        "\n",
        "1.   Treinamento mais eficiente das primeiras camadas por meio de \"atalhos\" para o backpropagation, assim como nas ResNets;\n",
        "2.   Incorporação de informação de alto nível semântico das camadas mais profundas com informação de baixo nível semântico, porém com maior resolução espacial no classificador de pixels da Rede.\n",
        "\n",
        "Na prática, Skip Connections são implementadas como concatenações no eixo dos canais, como mostrado na figura abaixo. Deve-se notar que a camada posterior deve estar preparada para receber a soma da quantidade de canais provenientes da camada anterior com o número de canais da camada que gerou os dados para o Skip Connection.\n",
        "\n",
        "![Skip FCN](https://www.dropbox.com/s/af4ao1h7kql4uk6/Nets_FCN_Petro.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6g4JGVRHJ7l"
      },
      "source": [
        "# Atividade Prática\n",
        "\n",
        "Vamos transformar uma CNN já conhecida por nós, a AlexNet, em uma FCN para segmentação semântica realizando os incrementos indicados em cada um dos passos a seguir. Lembre que segmentação é uma tarefa de classificação de pixels, portanto a loss deve ser a Cross Entropy. A única diferença para a tarefa de classificação da aula passada é que as dimensões espaciais dos labels $(224, 224)$ e das predições da rede $(\\#classes, 224, 224)$ devem ser linearizadas para o cálculo da loss, resultado em vetores de dimensões $(224*224)$ e $(\\#classes, 224*224)$. Essa linearização deve ser implementada nas funções *train()* e *test()*.\n",
        "\n",
        "1.   Implemente uma FCN com o backbone de uma [AlexNet](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.alexnet), inicialmente sem nenhuma skip connection. Para isso, utilize apenas os blocos convolucionais da AlexNet para extração de features, seguido do bloco classifier onde você implementará apenas blocos convolucionais. É importante adicionar no forward uma operação de [upsample](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.upsample) bilinear, como nas FCN tradicionais, na transição entre o bloco de features e o classifier.\n",
        "\n",
        "2.   Modifique sua implementação para inserir Skip Connections assim como numa FCN-8s ou uma FCN-16s.\n",
        "\n",
        "3.   Compare o desempenho de FCNs com e sem Skip Connections e com o backbone estando pré-treinado ou não.\n",
        "\n",
        "4.   Fazer a mesma adaptação para a VGG 16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE0Z3zgHTorq"
      },
      "source": [
        "class FCN_AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, pretrained=False, skip=False):\n",
        "\n",
        "        super(FCN_AlexNet, self).__init__()\n",
        "\n",
        "        self.skip = skip\n",
        "        \n",
        "        if self.skip:\n",
        "            \n",
        "            # TO DO: (2.) FCN with Skip Connections (adapted from FCN-8s).\n",
        "            self.backbone1 = # ...\n",
        "            self.backbone2 = # ...\n",
        "\n",
        "            self.classifier = # ...\n",
        "        else:\n",
        "            \n",
        "            # TO DO: (1.) FCN without Skip Connections.\n",
        "            self.backbone = # ...\n",
        "\n",
        "            self.classifier = # ...\n",
        "        \n",
        "        if not pretrained:\n",
        "            initialize_weights(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.skip:\n",
        "            \n",
        "            # TO DO: (2.) Forward on FCN with Skip Connections.\n",
        "            \n",
        "        else:\n",
        "\n",
        "            # TO DO: (1.) Forward on FCN without Skip Connections.\n",
        "            \n",
        "        \n",
        "        output = # ...\n",
        "\n",
        "        return output\n",
        "        \n",
        "net = FCN_AlexNet(args['n_classes'],\n",
        "                  pretrained=args['pretrained'],\n",
        "                  skip=args['skip']).to(args['device'])\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W95COSWjJ2O"
      },
      "source": [
        "# class FCN_VGG16(nn.Module):\n",
        "\n",
        "#     def __init__(self, num_classes, pretrained=False, skip=False):\n",
        "\n",
        "#         super(FCN_VGG16, self).__init__()\n",
        "\n",
        "#         self.skip = skip\n",
        "        \n",
        "#         if self.skip:\n",
        "            \n",
        "#             # TO DO: (2.) FCN with Skip Connections (adapted from FCN-8s).\n",
        "#             self.backbone1 = # ...\n",
        "#             self.backbone2 = # ...\n",
        "\n",
        "#             self.classifier = # ...\n",
        "#         else:\n",
        "            \n",
        "#             # TO DO: (1.) FCN without Skip Connections.\n",
        "#             self.backbone = # ...\n",
        "            \n",
        "#             self.classifier = # ...\n",
        "        \n",
        "#         if not pretrained:\n",
        "#             initialize_weights(self)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         if self.skip:\n",
        "            \n",
        "#             # TO DO: (2.) Forward on FCN with Skip Connections.\n",
        "\n",
        "#         else:\n",
        "\n",
        "#             # TO DO: (1.) Forward on FCN without Skip Connections.\n",
        "\n",
        "#         output = # ...\n",
        "\n",
        "#         return output\n",
        "        \n",
        "# net = FCN_VGG16(args['n_classes'],\n",
        "#                 pretrained=args['pretrained'],\n",
        "#                 skip=args['skip']).to(args['device'])\n",
        "\n",
        "# print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuo6esmKQOCL"
      },
      "source": [
        "# Setting optimizer.\n",
        "if args['pretrained']:\n",
        "    \n",
        "    if args['skip']:\n",
        "        \n",
        "        # Skip Connections + pretrained.\n",
        "        # ...\n",
        "\n",
        "    else:\n",
        "        \n",
        "        # No Skip Connections + pretrained.\n",
        "        optimizer = optim.Adam([{'params': net.backbone.parameters(), 'lr': args['lr'] * 0.1},\n",
        "                                {'params': net.classifier.parameters()}],\n",
        "                               lr=args['lr'],\n",
        "                               weight_decay=args['weight_decay'],\n",
        "                               betas=(args['momentum'], 0.999))\n",
        "else:\n",
        "    \n",
        "    if args['skip']:\n",
        "\n",
        "        # Skip Connections + from scratch.\n",
        "        # ...\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        # No Skip Connections + from scratch.\n",
        "        # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8p2oLwCQFzX"
      },
      "source": [
        "# Setting loss.\n",
        "criterion = #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2XCR_Q_WH_B"
      },
      "source": [
        "# Training function.\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    int_all = np.asarray(args['n_classes'], dtype=np.float32)\n",
        "    uni_all = np.asarray(args['n_classes'], dtype=np.float32)\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "\n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding through network.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        # Remeber to reshape the data\n",
        "        # ... \n",
        "\n",
        "        # TO DO: Computing backpropagation.\n",
        "        # ...\n",
        "\n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "        \n",
        "        # Appending metrics for epoch error calculation.\n",
        "        int_sum, uni_sum = evaluate([prds],\n",
        "                                    [labs.detach().squeeze(0).cpu().numpy()])\n",
        "\n",
        "        int_all = int_all + int_sum\n",
        "        uni_all = uni_all + uni_sum\n",
        "\n",
        "        # Updating loss meter.\n",
        "        train_loss.append(loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Transforming list into numpy array.\n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou = 0\n",
        "    iou = np.divide(int_all, uni_all)\n",
        "#     print(iou)\n",
        "\n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [miou %.4f], [time %.4f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), iou[1:].mean(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "def validate(val_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    val_loss = []\n",
        "\n",
        "    int_all = np.asarray(args['n_classes'], dtype=np.float32)\n",
        "    uni_all = np.asarray(args['n_classes'], dtype=np.float32)\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(val_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "\n",
        "        # Forwarding through network.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        # ...\n",
        "\n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Appending metrics for epoch error calculation.\n",
        "        int_sum, uni_sum = evaluate([prds],\n",
        "                                    [labs.detach().squeeze(0).cpu().numpy()])\n",
        "\n",
        "        int_all = int_all + int_sum\n",
        "        uni_all = uni_all + uni_sum\n",
        "\n",
        "        # Updating loss meter.\n",
        "        val_loss.append(loss.data.item())\n",
        "        \n",
        "        if i == 0 and epoch % args['show_freq'] == 0:\n",
        "            \n",
        "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            \n",
        "            ax[0].imshow(inps[0].detach().cpu().numpy().transpose(1, 2, 0))\n",
        "            ax[0].set_yticks([])\n",
        "            ax[0].set_xticks([])\n",
        "            ax[0].set_title('Image')\n",
        "            \n",
        "            ax[1].imshow(prds * 127,\n",
        "                         cmap=plt.get_cmap('gray'),\n",
        "                         vmin=0, vmax=255)\n",
        "            ax[1].set_yticks([])\n",
        "            ax[1].set_xticks([])\n",
        "            ax[1].set_title('Prediction')\n",
        "            \n",
        "            ax[2].imshow(labs[0].detach().squeeze(0).cpu().numpy() * 127,\n",
        "                         cmap=plt.get_cmap('gray'),\n",
        "                         vmin=0, vmax=255)\n",
        "            ax[2].set_yticks([])\n",
        "            ax[2].set_xticks([])\n",
        "            ax[2].set_title('True Label')\n",
        "            \n",
        "            plt.show()\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Transforming list into numpy array.\n",
        "    val_loss = np.asarray(val_loss)\n",
        "    \n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou = 0\n",
        "    iou = np.divide(int_all, uni_all)\n",
        "#     print(iou)\n",
        "\n",
        "    # Printing test epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [miou %.4f], [time %.4f]' % (\n",
        "        epoch, val_loss.mean(), val_loss.std(), iou[1:].mean(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19ywBEJ-NdWy"
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    validate(val_loader, net, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}