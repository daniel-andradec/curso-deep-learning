{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "05 - Problemas_Gabarito.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0bzMRy-nFKua",
        "Y0m-qic-0Wnl",
        "IDaRVNq1aMpm",
        "Pd8hG7HCDUib",
        "tOAxsRz0rpZl"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Problemas\n",
        "\n",
        "Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n",
        "A grande parte desses ajustes dependem diretamente do problemas.\n",
        "\n",
        "Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n",
        "\n",
        "\n",
        "**Seu objetivo é determinar e implementar um modelo para cada problema.**\n",
        "\n",
        "Isso inclui:\n",
        "\n",
        "1. definir uma arquitetura.\n",
        "Por enquanto usando somente camadas [Lineares](https://pytorch.org/docs/stable/nn.html#linear), porém podemos variar as ativações, como [Sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid), [Tanh](https://pytorch.org/docs/stable/nn.html#tanh), [ReLU](https://pytorch.org/docs/stable/nn.html#relu), [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html), [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html), [SeLU](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html), [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html), [RReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html)\n",
        "2. definir uma função de custo. Algums opções que vimos previamente incluem[L1](https://pytorch.org/docs/stable/nn.html#l1loss), [L2/MSE](https://pytorch.org/docs/stable/nn.html#mseloss), [Huber/SmoothL1](https://pytorch.org/docs/stable/nn.html#smoothl1loss), [*Cross-Entropy*](https://pytorch.org/docs/stable/nn.html#crossentropyloss), [Hinge](https://pytorch.org/docs/stable/nn.html#hingeembeddingloss)), e\n",
        "3. definir um algoritmo de otimização ([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [RMSProp](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop), [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam))\n",
        "\n",
        "A leitura do dado assim como a função de treinamento já estão implementados para você."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bzMRy-nFKua"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-VATPAldgt"
      },
      "source": [
        "# imports basicos\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5RRCEpFKug"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNofnRSOFKul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e09db8-1e1c-4852-b359-14164de440a5"
      },
      "source": [
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "source": [
        "# funções básicas\n",
        "\n",
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    \"\"\"Construct a Torch data loader\"\"\"\n",
        "    if type(features) != torch.tensor:\n",
        "        features = torch.tensor(features)\n",
        "    if type(labels) != torch.tensor:\n",
        "        labels = torch.tensor(labels)\n",
        "    dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "def _get_batch(batch):\n",
        "    \"\"\"Return features and labels on ctx.\"\"\"\n",
        "    features, labels = batch\n",
        "    if labels.type() != features.type():\n",
        "        labels = labels.type(features.type())\n",
        "    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n",
        "            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n",
        "\n",
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    acc_sum, n, l = 0, 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y.long())\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "          n += y.size(0)\n",
        "\n",
        "    return acc_sum / n, l.item() / len(data_iter)\n",
        "  \n",
        "    \n",
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n",
        "                   num_epochs, type='regression'):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            if type == 'regression':\n",
        "              l = loss(y_hat, y.float())\n",
        "            else:\n",
        "              l = loss(y_hat, y.long())\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size(0)\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        if type == 'regression':\n",
        "          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n",
        "                % (epoch + 1, train_l_sum / len(train_iter), test_loss, time.time() - start))\n",
        "        else:\n",
        "          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n",
        "                 test_acc, time.time() - start))\n",
        "          \n",
        "        \n",
        "# funcao usada para teste\n",
        "def test(net, test_iter):\n",
        "    print('testing on', device)\n",
        "    first = True\n",
        "    for X in test_iter:\n",
        "        X = X.to(device)\n",
        "        y_hat = net(X)\n",
        "        if first is True:\n",
        "            pred_logits = y_hat\n",
        "            pred_labels = y_hat.argmax(axis=1)\n",
        "            first = False\n",
        "        else:\n",
        "            pred_logits = torch.concat(pred_logits, y_hat, dim=0)\n",
        "            pred_labels = torch.concat(pred_labels, y_hat.argmax(axis=1), dim=0)\n",
        "\n",
        "    return pred_logits.numpy(), pred_labels.numpy()\n",
        "\n",
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0m-qic-0Wnl"
      },
      "source": [
        "# Problema 1\n",
        "\n",
        "Neste problema, você receberá 14 *features* coletadas de pacientes e tentará predizer se eles tem algum sinal de doença cardíaca. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUYOPZYH0Ztc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7d6507-64f5-4f20-bc71-b8f47feb2cc8"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "data = np.genfromtxt('processed.cleveland.data', delimiter=',', dtype=np.float32)\n",
        "data = np.nan_to_num(data)\n",
        "\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "  \n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:58:27--  https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18461 (18K) [application/x-httpd-php]\n",
            "Saving to: ‘processed.cleveland.data.3’\n",
            "\n",
            "processed.cleveland 100%[===================>]  18.03K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-02-02 16:58:28 (264 KB/s) - ‘processed.cleveland.data.3’ saved [18461/18461]\n",
            "\n",
            "(303, 14) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6.    0. ]\n",
            "(303, 13) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6. ]\n",
            "(303,) 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178XNdRUpiQW",
        "outputId": "a4fda35a-b321-49bb-d606-64bb2359ad90"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e lambda do weight decay\r\n",
        "num_epochs, lr, wd_lambda = 10, 0.001, 0.0001\r\n",
        "\r\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\r\n",
        "net = nn.Sequential(\r\n",
        "        nn.Linear(13, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(128, 256),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(256, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(128, 5)\r\n",
        ")\r\n",
        "net.to(device)\r\n",
        "\r\n",
        "# função de custo (ou loss)\r\n",
        "loss = nn.CrossEntropyLoss().to(device)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=wd_lambda)\r\n",
        "\r\n",
        "# treinamento e validação\r\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs, type='classification')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 7.5748, train acc 0.273, test loss 6.3751, test acc 0.393, time 0.0 sec\n",
            "epoch 2, train loss 5.6528, train acc 0.372, test loss 6.9824, test acc 0.295, time 0.0 sec\n",
            "epoch 3, train loss 6.1780, train acc 0.306, test loss 4.2531, test acc 0.410, time 0.0 sec\n",
            "epoch 4, train loss 4.8677, train acc 0.335, test loss 6.2629, test acc 0.246, time 0.0 sec\n",
            "epoch 5, train loss 4.1185, train acc 0.364, test loss 6.0365, test acc 0.311, time 0.0 sec\n",
            "epoch 6, train loss 4.3485, train acc 0.368, test loss 5.3161, test acc 0.328, time 0.0 sec\n",
            "epoch 7, train loss 4.5289, train acc 0.360, test loss 4.0035, test acc 0.377, time 0.0 sec\n",
            "epoch 8, train loss 3.9843, train acc 0.368, test loss 4.5059, test acc 0.279, time 0.0 sec\n",
            "epoch 9, train loss 3.2715, train acc 0.434, test loss 4.6388, test acc 0.279, time 0.0 sec\n",
            "epoch 10, train loss 3.7248, train acc 0.347, test loss 2.9318, test acc 0.328, time 0.0 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDaRVNq1aMpm"
      },
      "source": [
        "# Problema 2\n",
        "\n",
        "Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWdBT3zhW_Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93df909f-8450-494c-c83b-57c7daff7b6a"
      },
      "source": [
        "# download do dataset\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip\n",
        "data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, 1:], data[:, 0]\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  \n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:39:50--  http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘YearPredictionMSD.txt.zip.1’\n",
            "\n",
            "YearPredictionMSD.t 100%[===================>] 201.24M  28.4MB/s    in 16s     \n",
            "\n",
            "2021-02-02 16:40:07 (12.4 MB/s) - ‘YearPredictionMSD.txt.zip.1’ saved [211011981/211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "replace YearPredictionMSD.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "[ 2.0010000e+03  4.9943569e+01  2.1471140e+01  7.3077499e+01\n",
            "  8.7486095e+00 -1.7406281e+01 -1.3099050e+01 -2.5012020e+01\n",
            " -1.2232570e+01  7.8308902e+00 -2.4678299e+00  3.3213601e+00\n",
            " -2.3152101e+00  1.0205560e+01  6.1110913e+02  9.5108960e+02\n",
            "  6.9811426e+02  4.0898486e+02  3.8370911e+02  3.2651511e+02\n",
            "  2.3811327e+02  2.5142413e+02  1.8717351e+02  1.0042652e+02\n",
            "  1.7919498e+02 -8.4155798e+00 -3.1787039e+02  9.5862663e+01\n",
            "  4.8102589e+01 -9.5663033e+01 -1.8062149e+01  1.9698400e+00\n",
            "  3.4424381e+01  1.1726700e+01  1.3679000e+00  7.7944398e+00\n",
            " -3.6994001e-01 -1.3367851e+02 -8.3261650e+01 -3.7297649e+01\n",
            "  7.3046669e+01 -3.7366840e+01 -3.1385300e+00 -2.4215309e+01\n",
            " -1.3230660e+01  1.5938090e+01 -1.8604780e+01  8.2154793e+01\n",
            "  2.4057980e+02 -1.0294070e+01  3.1584311e+01 -2.5381870e+01\n",
            " -3.9077201e+00  1.3292580e+01  4.1550598e+01 -7.2627201e+00\n",
            " -2.1008631e+01  1.0550848e+02  6.4298561e+01  2.6084810e+01\n",
            " -4.4591099e+01 -8.3065701e+00  7.9370599e+00 -1.0736600e+01\n",
            " -9.5447662e+01 -8.2033073e+01 -3.5591942e+01  4.6952500e+00\n",
            "  7.0956261e+01  2.8091391e+01  6.0201502e+00 -3.7137669e+01\n",
            " -4.1124500e+01 -8.4081602e+00  7.1987700e+00 -8.6017599e+00\n",
            " -5.9085698e+00 -1.2324370e+01  1.4687340e+01 -5.4321251e+01\n",
            "  4.0147861e+01  1.3016200e+01 -5.4405479e+01  5.8993671e+01\n",
            "  1.5373440e+01  1.1114399e+00 -2.3087931e+01  6.8407951e+01\n",
            " -1.8222300e+00 -2.7463480e+01  2.2632699e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VoHCfAjzft_3",
        "outputId": "29f56313-1743-4827-b3de-65060329d198"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e lambda do weight decay\r\n",
        "num_epochs, lr, wd_lambda = 100, 0.00001, 0.0000001\r\n",
        "\r\n",
        "# arquitetura\r\n",
        "net = nn.Sequential(\r\n",
        "        nn.Linear(90, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(128, 256),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(256, 1)\r\n",
        ")\r\n",
        "net.to(device)\r\n",
        "\r\n",
        "# função de custo (ou loss)\r\n",
        "loss = nn.SmoothL1Loss().to(device)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd_lambda)\r\n",
        "\r\n",
        "# treinamento e validação\r\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([81])) that is different to the input size (torch.Size([81, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 828.3568, test loss 531.5590, time 10.9 sec\n",
            "epoch 2, train loss 484.3003, test loss 456.6558, time 11.0 sec\n",
            "epoch 3, train loss 433.7129, test loss 413.9627, time 11.0 sec\n",
            "epoch 4, train loss 392.5443, test loss 372.9806, time 11.0 sec\n",
            "epoch 5, train loss 351.7587, test loss 332.7040, time 10.8 sec\n",
            "epoch 6, train loss 313.0188, test loss 295.6150, time 11.0 sec\n",
            "epoch 7, train loss 276.8993, test loss 260.2495, time 11.5 sec\n",
            "epoch 8, train loss 243.1305, test loss 228.1428, time 10.6 sec\n",
            "epoch 9, train loss 212.8374, test loss 200.0085, time 11.0 sec\n",
            "epoch 10, train loss 188.9946, test loss 179.5744, time 11.0 sec\n",
            "epoch 11, train loss 172.7330, test loss 167.4416, time 10.8 sec\n",
            "epoch 12, train loss 162.0664, test loss 157.8766, time 11.1 sec\n",
            "epoch 13, train loss 154.2655, test loss 150.9136, time 11.1 sec\n",
            "epoch 14, train loss 147.8607, test loss 145.0188, time 10.8 sec\n",
            "epoch 15, train loss 142.3203, test loss 140.6690, time 11.0 sec\n",
            "epoch 16, train loss 137.3547, test loss 135.2710, time 11.3 sec\n",
            "epoch 17, train loss 132.8343, test loss 130.9589, time 10.8 sec\n",
            "epoch 18, train loss 128.7500, test loss 127.4244, time 11.0 sec\n",
            "epoch 19, train loss 124.9573, test loss 123.2976, time 10.8 sec\n",
            "epoch 20, train loss 121.4850, test loss 119.8070, time 11.4 sec\n",
            "epoch 21, train loss 118.3453, test loss 117.2331, time 10.7 sec\n",
            "epoch 22, train loss 115.4704, test loss 114.4143, time 11.2 sec\n",
            "epoch 23, train loss 112.9559, test loss 112.1414, time 10.9 sec\n",
            "epoch 24, train loss 110.6361, test loss 109.8622, time 11.1 sec\n",
            "epoch 25, train loss 108.6349, test loss 108.0823, time 11.0 sec\n",
            "epoch 26, train loss 106.9994, test loss 106.0877, time 11.0 sec\n",
            "epoch 27, train loss 105.4673, test loss 104.7429, time 11.1 sec\n",
            "epoch 28, train loss 104.2862, test loss 103.9466, time 10.9 sec\n",
            "epoch 29, train loss 103.2148, test loss 102.6618, time 11.1 sec\n",
            "epoch 30, train loss 102.2656, test loss 101.9043, time 10.5 sec\n",
            "epoch 31, train loss 101.4642, test loss 101.6566, time 10.6 sec\n",
            "epoch 32, train loss 100.8316, test loss 100.5479, time 11.4 sec\n",
            "epoch 33, train loss 100.2246, test loss 99.9228, time 11.0 sec\n",
            "epoch 34, train loss 99.7380, test loss 99.6513, time 11.0 sec\n",
            "epoch 35, train loss 99.2678, test loss 99.2915, time 11.1 sec\n",
            "epoch 36, train loss 98.8953, test loss 99.1625, time 10.8 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-a775b0c280fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# treinamento e validação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-e0e2b0d88e59>\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs, type)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m               \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNMAyyX8jSb8",
        "outputId": "9bc0fe08-e84e-4a1b-b546-7ab221f95a22"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\r\n",
        "y = net(torch.Tensor(test_features[0:5, :]).to(device))\r\n",
        "print(y, test_labels[0:5])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2123.6570],\n",
            "        [2055.9392],\n",
            "        [2100.2898],\n",
            "        [1703.6139],\n",
            "        [1944.5286]], device='cuda:0', grad_fn=<AddmmBackward>) [2008. 2001. 2006. 2008. 1998.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8hG7HCDUib"
      },
      "source": [
        "# Problema 3\n",
        "\n",
        "Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc). Mais informações sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/covertype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZcIXGqBDznB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce137507-fece-45e9-c412-e5d2442adf92"
      },
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "!gzip covtype.data.gz\n",
        "data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "train_labels = train_labels - 1\n",
        "test_labels = test_labels - 1\n",
        "  \n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:23:01--  http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11240707 (11M) [application/x-httpd-php]\n",
            "Saving to: ‘covtype.data.gz.3’\n",
            "\n",
            "covtype.data.gz.3   100%[===================>]  10.72M  14.0MB/s    in 0.8s    \n",
            "\n",
            "2021-02-02 16:23:03 (14.0 MB/s) - ‘covtype.data.gz.3’ saved [11240707/11240707]\n",
            "\n",
            "gzip: covtype.data.gz already has .gz suffix -- unchanged\n",
            "(581012, 55) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00]\n",
            "(581012, 54) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            "(581012,) 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O0HJVIOZZW4",
        "outputId": "57d08153-c1a1-4a6a-9a36-4211ac528a00"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e lambda do weight decay\r\n",
        "num_epochs, lr, wd_lambda = 20, 0.001, 0.0001\r\n",
        "\r\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\r\n",
        "net = nn.Sequential(\r\n",
        "        nn.Linear(54, 256),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(256, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(128, 64),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(64, 7)\r\n",
        ")\r\n",
        "net.to(device)\r\n",
        "\r\n",
        "# função de custo (ou loss)\r\n",
        "loss = nn.CrossEntropyLoss().to(device)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=wd_lambda)\r\n",
        "\r\n",
        "# treinamento e validação\r\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs, type='classification')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.3487, train acc 0.468, test loss 1.1978, test acc 0.483, time 11.7 sec\n",
            "epoch 2, train loss 1.1846, train acc 0.482, test loss 1.1695, test acc 0.488, time 12.3 sec\n",
            "epoch 3, train loss 1.1593, train acc 0.486, test loss 1.1472, test acc 0.488, time 11.8 sec\n",
            "epoch 4, train loss 1.1352, train acc 0.491, test loss 1.1230, test acc 0.496, time 11.9 sec\n",
            "epoch 5, train loss 1.1049, train acc 0.504, test loss 1.0931, test acc 0.510, time 11.6 sec\n",
            "epoch 6, train loss 1.0626, train acc 0.528, test loss 1.0507, test acc 0.528, time 12.0 sec\n",
            "epoch 7, train loss 1.0321, train acc 0.547, test loss 1.0091, test acc 0.558, time 11.8 sec\n",
            "epoch 8, train loss 1.0102, train acc 0.560, test loss 0.9897, test acc 0.573, time 12.3 sec\n",
            "epoch 9, train loss 0.9923, train acc 0.570, test loss 0.9691, test acc 0.583, time 12.0 sec\n",
            "epoch 10, train loss 0.9781, train acc 0.578, test loss 0.9557, test acc 0.586, time 11.7 sec\n",
            "epoch 11, train loss 0.9630, train acc 0.586, test loss 0.9464, test acc 0.599, time 12.5 sec\n",
            "epoch 12, train loss 0.9540, train acc 0.590, test loss 0.9450, test acc 0.590, time 12.0 sec\n",
            "epoch 13, train loss 0.9461, train acc 0.594, test loss 0.9461, test acc 0.579, time 11.5 sec\n",
            "epoch 14, train loss 0.9353, train acc 0.600, test loss 0.9283, test acc 0.600, time 11.6 sec\n",
            "epoch 15, train loss 0.9298, train acc 0.601, test loss 0.9571, test acc 0.577, time 12.0 sec\n",
            "epoch 16, train loss 0.9255, train acc 0.604, test loss 0.9149, test acc 0.602, time 11.8 sec\n",
            "epoch 17, train loss 0.9205, train acc 0.607, test loss 0.8956, test acc 0.622, time 12.0 sec\n",
            "epoch 18, train loss 0.9114, train acc 0.612, test loss 0.8922, test acc 0.616, time 11.7 sec\n",
            "epoch 19, train loss 0.9067, train acc 0.614, test loss 0.9004, test acc 0.612, time 12.2 sec\n",
            "epoch 20, train loss 0.9022, train acc 0.617, test loss 0.8844, test acc 0.631, time 11.8 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAxsRz0rpZl"
      },
      "source": [
        "# Problema 4\r\n",
        "\r\n",
        "Neste problema, você receberá 11 *features* extraídas de tipos de vinhos, e terá que predizer um *score* para cada vinho. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzA0O6rusi",
        "outputId": "2d202be6-69e9-4357-b2f4-592d65dc0a9b"
      },
      "source": [
        "# download do dataset\r\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\r\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\r\n",
        "data_red = np.genfromtxt('winequality-red.csv', delimiter=';', dtype=np.float32, skip_header=1)\r\n",
        "data_white = np.genfromtxt('winequality-white.csv', delimiter=';', dtype=np.float32, skip_header=1)\r\n",
        "data = np.concatenate((data_red, data_white), axis=0)\r\n",
        "data = np.nan_to_num(data)\r\n",
        "\r\n",
        "print(data[0, :])\r\n",
        "X, y = data[:, :-1], data[:, -1]\r\n",
        "print(X.shape, y.shape)\r\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\r\n",
        "\r\n",
        "batch_size = 100\r\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\r\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 17:13:31--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84199 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘winequality-red.csv’\n",
            "\n",
            "winequality-red.csv 100%[===================>]  82.23K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-02 17:13:31 (595 KB/s) - ‘winequality-red.csv’ saved [84199/84199]\n",
            "\n",
            "--2021-02-02 17:13:31--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264426 (258K) [application/x-httpd-php]\n",
            "Saving to: ‘winequality-white.csv’\n",
            "\n",
            "winequality-white.c 100%[===================>] 258.23K   946KB/s    in 0.3s    \n",
            "\n",
            "2021-02-02 17:13:32 (946 KB/s) - ‘winequality-white.csv’ saved [264426/264426]\n",
            "\n",
            "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
            "  0.56    9.4     5.    ]\n",
            "(6497, 11) (6497,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E7pwns4rx9l",
        "outputId": "75b80e3a-fa38-4952-ab49-0aa5bd569706"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e lambda do weight decay\r\n",
        "num_epochs, lr, wd_lambda = 100, 0.00001, 0.0000001\r\n",
        "\r\n",
        "# arquitetura\r\n",
        "net = nn.Sequential(\r\n",
        "        nn.Linear(11, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(128, 256),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(256, 1)\r\n",
        ")\r\n",
        "net.to(device)\r\n",
        "\r\n",
        "# função de custo (ou loss)\r\n",
        "loss = nn.SmoothL1Loss().to(device)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd_lambda)\r\n",
        "\r\n",
        "# treinamento e validação\r\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.5276, test loss 1.5283, time 0.1 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([45])) that is different to the input size (torch.Size([45, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, train loss 1.4710, test loss 1.4665, time 0.1 sec\n",
            "epoch 3, train loss 1.4104, test loss 1.4061, time 0.1 sec\n",
            "epoch 4, train loss 1.3554, test loss 1.3453, time 0.2 sec\n",
            "epoch 5, train loss 1.2932, test loss 1.2806, time 0.2 sec\n",
            "epoch 6, train loss 1.2262, test loss 1.2100, time 0.2 sec\n",
            "epoch 7, train loss 1.1495, test loss 1.1290, time 0.2 sec\n",
            "epoch 8, train loss 1.0649, test loss 1.0464, time 0.2 sec\n",
            "epoch 9, train loss 0.9923, test loss 0.9778, time 0.2 sec\n",
            "epoch 10, train loss 0.9324, test loss 0.9216, time 0.1 sec\n",
            "epoch 11, train loss 0.8785, test loss 0.8643, time 0.1 sec\n",
            "epoch 12, train loss 0.8231, test loss 0.8090, time 0.1 sec\n",
            "epoch 13, train loss 0.7736, test loss 0.7612, time 0.1 sec\n",
            "epoch 14, train loss 0.7327, test loss 0.7251, time 0.1 sec\n",
            "epoch 15, train loss 0.6908, test loss 0.6822, time 0.1 sec\n",
            "epoch 16, train loss 0.6549, test loss 0.6442, time 0.2 sec\n",
            "epoch 17, train loss 0.6203, test loss 0.6124, time 0.2 sec\n",
            "epoch 18, train loss 0.5936, test loss 0.5825, time 0.1 sec\n",
            "epoch 19, train loss 0.5660, test loss 0.5561, time 0.1 sec\n",
            "epoch 20, train loss 0.5395, test loss 0.5332, time 0.1 sec\n",
            "epoch 21, train loss 0.5169, test loss 0.5116, time 0.1 sec\n",
            "epoch 22, train loss 0.4955, test loss 0.4877, time 0.1 sec\n",
            "epoch 23, train loss 0.4788, test loss 0.4732, time 0.2 sec\n",
            "epoch 24, train loss 0.4653, test loss 0.4569, time 0.1 sec\n",
            "epoch 25, train loss 0.4520, test loss 0.4449, time 0.1 sec\n",
            "epoch 26, train loss 0.4426, test loss 0.4360, time 0.1 sec\n",
            "epoch 27, train loss 0.4325, test loss 0.4258, time 0.1 sec\n",
            "epoch 28, train loss 0.4283, test loss 0.4207, time 0.1 sec\n",
            "epoch 29, train loss 0.4207, test loss 0.4142, time 0.1 sec\n",
            "epoch 30, train loss 0.4163, test loss 0.4086, time 0.1 sec\n",
            "epoch 31, train loss 0.4121, test loss 0.4029, time 0.1 sec\n",
            "epoch 32, train loss 0.4083, test loss 0.3992, time 0.1 sec\n",
            "epoch 33, train loss 0.4058, test loss 0.4051, time 0.1 sec\n",
            "epoch 34, train loss 0.4051, test loss 0.3981, time 0.2 sec\n",
            "epoch 35, train loss 0.4019, test loss 0.3915, time 0.2 sec\n",
            "epoch 36, train loss 0.3993, test loss 0.3922, time 0.1 sec\n",
            "epoch 37, train loss 0.3981, test loss 0.3879, time 0.1 sec\n",
            "epoch 38, train loss 0.3981, test loss 0.3864, time 0.1 sec\n",
            "epoch 39, train loss 0.3958, test loss 0.3891, time 0.1 sec\n",
            "epoch 40, train loss 0.3929, test loss 0.3846, time 0.1 sec\n",
            "epoch 41, train loss 0.3922, test loss 0.3834, time 0.1 sec\n",
            "epoch 42, train loss 0.3916, test loss 0.3827, time 0.1 sec\n",
            "epoch 43, train loss 0.3915, test loss 0.3817, time 0.1 sec\n",
            "epoch 44, train loss 0.3920, test loss 0.3837, time 0.1 sec\n",
            "epoch 45, train loss 0.3914, test loss 0.3802, time 0.1 sec\n",
            "epoch 46, train loss 0.3895, test loss 0.3795, time 0.1 sec\n",
            "epoch 47, train loss 0.3892, test loss 0.3793, time 0.1 sec\n",
            "epoch 48, train loss 0.3896, test loss 0.3793, time 0.1 sec\n",
            "epoch 49, train loss 0.3898, test loss 0.3805, time 0.1 sec\n",
            "epoch 50, train loss 0.3890, test loss 0.3796, time 0.1 sec\n",
            "epoch 51, train loss 0.3871, test loss 0.3773, time 0.1 sec\n",
            "epoch 52, train loss 0.3891, test loss 0.3769, time 0.1 sec\n",
            "epoch 53, train loss 0.3866, test loss 0.3772, time 0.1 sec\n",
            "epoch 54, train loss 0.3873, test loss 0.3768, time 0.1 sec\n",
            "epoch 55, train loss 0.3855, test loss 0.3764, time 0.1 sec\n",
            "epoch 56, train loss 0.3868, test loss 0.3779, time 0.1 sec\n",
            "epoch 57, train loss 0.3874, test loss 0.3764, time 0.1 sec\n",
            "epoch 58, train loss 0.3849, test loss 0.3756, time 0.1 sec\n",
            "epoch 59, train loss 0.3870, test loss 0.3767, time 0.1 sec\n",
            "epoch 60, train loss 0.3845, test loss 0.3747, time 0.1 sec\n",
            "epoch 61, train loss 0.3872, test loss 0.3748, time 0.1 sec\n",
            "epoch 62, train loss 0.3869, test loss 0.3746, time 0.1 sec\n",
            "epoch 63, train loss 0.3860, test loss 0.3747, time 0.1 sec\n",
            "epoch 64, train loss 0.3849, test loss 0.3739, time 0.1 sec\n",
            "epoch 65, train loss 0.3861, test loss 0.3738, time 0.1 sec\n",
            "epoch 66, train loss 0.3871, test loss 0.3736, time 0.1 sec\n",
            "epoch 67, train loss 0.3845, test loss 0.3734, time 0.1 sec\n",
            "epoch 68, train loss 0.3858, test loss 0.3735, time 0.1 sec\n",
            "epoch 69, train loss 0.3834, test loss 0.3733, time 0.1 sec\n",
            "epoch 70, train loss 0.3843, test loss 0.3728, time 0.2 sec\n",
            "epoch 71, train loss 0.3840, test loss 0.3728, time 0.1 sec\n",
            "epoch 72, train loss 0.3839, test loss 0.3731, time 0.1 sec\n",
            "epoch 73, train loss 0.3829, test loss 0.3726, time 0.2 sec\n",
            "epoch 74, train loss 0.3825, test loss 0.3721, time 0.1 sec\n",
            "epoch 75, train loss 0.3855, test loss 0.3720, time 0.1 sec\n",
            "epoch 76, train loss 0.3845, test loss 0.3719, time 0.1 sec\n",
            "epoch 77, train loss 0.3831, test loss 0.3725, time 0.1 sec\n",
            "epoch 78, train loss 0.3821, test loss 0.3748, time 0.1 sec\n",
            "epoch 79, train loss 0.3839, test loss 0.3718, time 0.1 sec\n",
            "epoch 80, train loss 0.3833, test loss 0.3723, time 0.1 sec\n",
            "epoch 81, train loss 0.3832, test loss 0.3769, time 0.1 sec\n",
            "epoch 82, train loss 0.3831, test loss 0.3730, time 0.1 sec\n",
            "epoch 83, train loss 0.3823, test loss 0.3709, time 0.1 sec\n",
            "epoch 84, train loss 0.3818, test loss 0.3708, time 0.1 sec\n",
            "epoch 85, train loss 0.3835, test loss 0.3715, time 0.1 sec\n",
            "epoch 86, train loss 0.3817, test loss 0.3718, time 0.1 sec\n",
            "epoch 87, train loss 0.3835, test loss 0.3719, time 0.1 sec\n",
            "epoch 88, train loss 0.3801, test loss 0.3703, time 0.1 sec\n",
            "epoch 89, train loss 0.3814, test loss 0.3704, time 0.1 sec\n",
            "epoch 90, train loss 0.3807, test loss 0.3727, time 0.1 sec\n",
            "epoch 91, train loss 0.3822, test loss 0.3703, time 0.1 sec\n",
            "epoch 92, train loss 0.3817, test loss 0.3703, time 0.1 sec\n",
            "epoch 93, train loss 0.3822, test loss 0.3697, time 0.1 sec\n",
            "epoch 94, train loss 0.3809, test loss 0.3702, time 0.1 sec\n",
            "epoch 95, train loss 0.3817, test loss 0.3695, time 0.1 sec\n",
            "epoch 96, train loss 0.3808, test loss 0.3707, time 0.1 sec\n",
            "epoch 97, train loss 0.3800, test loss 0.3715, time 0.1 sec\n",
            "epoch 98, train loss 0.3819, test loss 0.3715, time 0.1 sec\n",
            "epoch 99, train loss 0.3810, test loss 0.3696, time 0.1 sec\n",
            "epoch 100, train loss 0.3805, test loss 0.3690, time 0.1 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-At99Iqzryg8",
        "outputId": "2686b156-51ea-4caa-e48e-33354a76641b"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\r\n",
        "y = net(torch.Tensor(test_features[0:5, :]).to(device))\r\n",
        "print(y, test_labels[0:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[6.2811],\n",
            "        [5.7228],\n",
            "        [6.1114],\n",
            "        [5.6124],\n",
            "        [5.8584]], device='cuda:0', grad_fn=<AddmmBackward>) [8. 5. 7. 6. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}