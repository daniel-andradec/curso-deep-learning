{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_2_Regularizadores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Regulizadores\n",
        "\n",
        "Neste código iremos analisar como funcionam os regularizadores e como eles são usados para evitar *overfitting*, fenônemo que faz com o modelo não generalize bem em outros dataset além do usando durante o treino.\n",
        "\n",
        "# *Overfitting*\n",
        "\n",
        "Redes neurais são muito flexíveis porque não se limitam a ver cada atributo a ser aprendido individualmente. Em vez disso, elas podem aprender interações entre os atributos. Por causa disso, mesmo quando temos apenas um pequeno número de atributos, as redes neurais profundas são capazes de chegar ao *overfitting*, um cenário onde o modelo aprende a classificar muito bem (as vezes, perfeitamente) as instâncias de treino, porém não generaliza para outras instâncias não vista (como são os casos de amostras do conjunto de validação ou teste).\n",
        "\n",
        "Para evitar esse cenário, algumas técnicas foram propostas para evitar o *overfitting*.\n",
        "\n",
        "**Como introduzido, nesta aula prática, implementaremos e testaremos duas técnicas para evitar o *overfitting*: *Dropout* e *Weight Decay***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6CwWnFnTwb"
      },
      "source": [
        "**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-VATPAldgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d1c62f-ba2e-4c50-d2b0-42f6c4d34a54"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9u0pCOtlWLu"
      },
      "source": [
        "## carregando dados\n",
        "\n",
        "# código para carregar o dataset do MNIST\n",
        "# http://yann.lecun.com/exdb/mnist/\n",
        "def load_data_mnist(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [transforms.Resize(resize)]\n",
        "    transformer += [transforms.ToTensor()]\n",
        "    transformer = transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = datasets.MNIST(root=root, train=True,download=True, transform=transformer)\n",
        "    mnist_test = datasets.MNIST(root=root, train=False,download=True, transform=transformer)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "# código para carregar o dataset do Fashion-MNIST\n",
        "# https://github.com/zalandoresearch/fashion-mnist\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [transforms.Resize(resize)]\n",
        "    transformer += [transforms.ToTensor()]\n",
        "    transformer = transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = datasets.FashionMNIST(root=root, train=True, download=True, transform=transformer)\n",
        "    mnist_test = datasets.FashionMNIST(root=root, train=False, download=True, transform=transformer)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "source": [
        "# funções básicas\n",
        "def _get_batch(batch):\n",
        "    \"\"\"Return features and labels on ctx.\"\"\"\n",
        "    features, labels = batch\n",
        "    if labels.type() != features.type():\n",
        "        labels = labels.type(features.type())\n",
        "    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n",
        "            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n",
        "\n",
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    acc_sum, n, l = torch.Tensor([0]), 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          #y = y.astype('float32')\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y).sum()\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "          n += y.size()[0]\n",
        "\n",
        "    return acc_sum.item() / n, l.item() / len(data_iter)\n",
        "  \n",
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n",
        "                   num_epochs):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            trainer.zero_grad()\n",
        "            l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size()[0]\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n",
        "                 test_acc, time.time() - start))\n",
        "\n",
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azv2ajIYkIjH"
      },
      "source": [
        "## *Dropout*\n",
        "\n",
        "*Dropout* é uma das formas mais interessantes de regularizar sua rede neural. \n",
        "A ideia do *Droupout* é simples: durante o passo de *Forward*, alguns neurônios são aleatoriamente \"desligados\", ou seja, são zerados, e não são utilizados em nenhum processamento.\n",
        "Em cada passo do *Forward*, neurônios diferentes são \"desligados\" aleatoriamente, de acordo com uma probabilide pré-definida.\n",
        "Lembrem-se que esse processo só acontece durante o treino.\n",
        "Durante o teste, *Dropout* não tem nenhuma ação e todos os neurônios são usados para gerar o resultado fina.\n",
        "\n",
        "Formalmente, suponha um neurônio com ativação $h$ e um *Dropout* com probabilide $p$ (de zerar ou \"desligar\" o neurônio).\n",
        "Logo, essa técnica irá \"desligar\" a ativação desse neurônio com probabilidade $p$ ou reescala-la baseado na probabilidade de essa unidade de processamento permanecer ativa (isto é, $1-p$):\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h' =\n",
        "\\begin{cases}\n",
        "    0 & \\text{ com probabilidade } p \\\\\n",
        "    \\frac{h}{1-p} & \\text{ caso contrário}\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Tal método é interessante e chamou a atenção do mundo acadêmico por ser muito simples de implementar e poder impulsar significativamente o desempenho do modelo.\n",
        "\n",
        "### Implementação\n",
        "\n",
        "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), para utilizar os benefícios do Dropout basta adicionar a camada homônima (passando como argumento a probabilidade de desligamento dos neurônios) durante a construção da arquitetura.\n",
        "\n",
        "**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**\n",
        "\n",
        "Durante o treino, a camada *Dropout* irá \"desligar\" aleatoriamente algumas saídas da camada anterior (ou equivalentemente, as entradas para a camada subsequente) de acordo com a probabilidade especificada.\n",
        "\n",
        "Quando o PyTorch não está no modo de treinamento, a camada *Dropout* simplesmente passa os dados sem fazer nenhum \"desligamento\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlBGUroCFfyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6b7f2b-56db-4cf8-e4c7-14b23c26fa44"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
        "# tamanho do batch\n",
        "num_epochs, lr, batch_size = 20, 0.5, 256\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.2),                   # dropout com 20% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.5),                   # dropout com 50% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.1077, train acc 0.568, test loss 0.7163, test acc 0.739, time 5.5 sec\n",
            "epoch 2, train loss 0.5749, train acc 0.788, test loss 0.5168, test acc 0.815, time 5.8 sec\n",
            "epoch 3, train loss 0.4837, train acc 0.825, test loss 0.6097, test acc 0.767, time 5.7 sec\n",
            "epoch 4, train loss 0.4442, train acc 0.838, test loss 0.7559, test acc 0.702, time 5.6 sec\n",
            "epoch 5, train loss 0.4104, train acc 0.851, test loss 0.4663, test acc 0.832, time 5.4 sec\n",
            "epoch 6, train loss 0.3891, train acc 0.857, test loss 0.4083, test acc 0.846, time 5.5 sec\n",
            "epoch 7, train loss 0.3748, train acc 0.863, test loss 0.4335, test acc 0.844, time 6.2 sec\n",
            "epoch 8, train loss 0.3622, train acc 0.867, test loss 0.4706, test acc 0.826, time 5.7 sec\n",
            "epoch 9, train loss 0.3504, train acc 0.871, test loss 0.6285, test acc 0.798, time 5.6 sec\n",
            "epoch 10, train loss 0.3415, train acc 0.874, test loss 0.4274, test acc 0.845, time 5.8 sec\n",
            "epoch 11, train loss 0.3319, train acc 0.878, test loss 0.3908, test acc 0.858, time 5.6 sec\n",
            "epoch 12, train loss 0.3224, train acc 0.881, test loss 0.4302, test acc 0.838, time 5.8 sec\n",
            "epoch 13, train loss 0.3170, train acc 0.882, test loss 0.4069, test acc 0.854, time 5.4 sec\n",
            "epoch 14, train loss 0.3098, train acc 0.885, test loss 0.3816, test acc 0.864, time 5.8 sec\n",
            "epoch 15, train loss 0.3027, train acc 0.887, test loss 0.3787, test acc 0.860, time 5.7 sec\n",
            "epoch 16, train loss 0.2977, train acc 0.890, test loss 0.3955, test acc 0.858, time 5.8 sec\n",
            "epoch 17, train loss 0.2927, train acc 0.891, test loss 0.4107, test acc 0.858, time 5.8 sec\n",
            "epoch 18, train loss 0.2844, train acc 0.895, test loss 0.3629, test acc 0.867, time 5.8 sec\n",
            "epoch 19, train loss 0.2848, train acc 0.894, test loss 0.4004, test acc 0.858, time 5.5 sec\n",
            "epoch 20, train loss 0.2797, train acc 0.896, test loss 0.3733, test acc 0.870, time 5.7 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJTUwjd4QSkF"
      },
      "source": [
        "## *Weight Decay*\n",
        "\n",
        "*Weight Decay* (comumente chamado regularização *L2*), é uma das técnicas mais utilizadas para regularizar modelos paramétricos de aprendizado de máquina.\n",
        "A intuição básica por trás do *Weight Decay* é a noção de que entre todas as funções $f$, a função $f=0$ é a mais simples. Intuitivamente, podemos medir funções pela sua proximidade a zero. Mas quão devemos medir a distância entre uma função e zero? Não há uma resposta correta. De fato, ramos inteiros da matemática são dedicados a responder a esta questão.\n",
        "\n",
        "Para nossos propósitos atuais, uma interpretação muito simples será suficiente: vamos considerar uma função linear $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ é simples se o seu vetor de peso $\\mathbf{w}$ for pequeno. Podemos medir isso via norma $||\\mathbf{w}||^2$. Uma maneira de manter o vetor de peso pequeno é adicionar sua norma como um termo de penalidade ao problema de minimizar a função de perda (ou *loss*). Assim, nós substituímos nosso objetivo original, *minimizar o erro de previsão nos rótulos de treinamento*, com novo objetivo, *minimizar o erro de previsão e o termo de penalidade*. Agora, se o vetor de peso se tornar muito grande, nosso algoritmo de aprendizagem vai encontrar mais lucro minimizando a norma $||\\mathbf{w}||^2$ do que minimizando o erro de treinamento. \n",
        "\n",
        "Tecnicamente, para uma função de custo qualquer $\\mathcal{L}$, a adição do novo termo de penalidade (ou *weight decay*) acontece da seguinte forma:\n",
        "\n",
        "$$\\mathcal{L}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
        "\n",
        "Esse parâmetro não negativo $\\lambda \\geq 0$ dita a quantidade de regularização. Para $\\lambda = 0$, recuperamos nossa função de perda original, enquanto para $\\lambda > 0 $ garantimos que os pesos $\\mathbf{w}$ não crescerão demais.\n",
        "\n",
        "### Implementação\n",
        "\n",
        "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), *Weight Decay* pode ser facilmente agregado à função de custo durante a construção do modelo.\n",
        "\n",
        "**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-koQ_vd9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a922c174-80bb-47d1-e8fc-030b411f41d7"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n",
        "# tamanho do batch, e valor de weight decay\n",
        "num_epochs, lr, batch_size, weight_decay = 20, 0.5, 256, 0.005\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.2832, train acc 0.501, test loss 0.8203, test acc 0.692, time 5.6 sec\n",
            "epoch 2, train loss 0.6402, train acc 0.759, test loss 0.9408, test acc 0.633, time 5.6 sec\n",
            "epoch 3, train loss 0.6671, train acc 0.756, test loss 0.5815, test acc 0.792, time 5.7 sec\n",
            "epoch 4, train loss 0.5705, train acc 0.794, test loss 0.7470, test acc 0.712, time 5.6 sec\n",
            "epoch 5, train loss 0.5616, train acc 0.797, test loss 0.7229, test acc 0.734, time 5.7 sec\n",
            "epoch 6, train loss 0.5845, train acc 0.787, test loss 0.5289, test acc 0.817, time 5.5 sec\n",
            "epoch 7, train loss 0.8481, train acc 0.774, test loss 1.8410, test acc 0.325, time 5.6 sec\n",
            "epoch 8, train loss 0.8498, train acc 0.684, test loss 0.5942, test acc 0.784, time 5.7 sec\n",
            "epoch 9, train loss 0.5924, train acc 0.783, test loss 0.5341, test acc 0.782, time 6.2 sec\n",
            "epoch 10, train loss 0.6041, train acc 0.782, test loss 0.7973, test acc 0.706, time 5.3 sec\n",
            "epoch 11, train loss 0.6649, train acc 0.759, test loss 0.6505, test acc 0.793, time 5.9 sec\n",
            "epoch 12, train loss 0.6172, train acc 0.779, test loss 0.8537, test acc 0.693, time 5.5 sec\n",
            "epoch 13, train loss 0.5659, train acc 0.796, test loss 0.6731, test acc 0.772, time 6.0 sec\n",
            "epoch 14, train loss 0.5854, train acc 0.789, test loss 0.5920, test acc 0.786, time 5.7 sec\n",
            "epoch 15, train loss 0.6475, train acc 0.764, test loss 0.7645, test acc 0.725, time 5.8 sec\n",
            "epoch 16, train loss 0.5635, train acc 0.795, test loss 0.7671, test acc 0.729, time 5.6 sec\n",
            "epoch 17, train loss 0.5607, train acc 0.798, test loss 0.7879, test acc 0.690, time 5.5 sec\n",
            "epoch 18, train loss 0.6238, train acc 0.777, test loss 0.7684, test acc 0.701, time 5.4 sec\n",
            "epoch 19, train loss 0.5748, train acc 0.795, test loss 0.6922, test acc 0.744, time 5.6 sec\n",
            "epoch 20, train loss 0.5693, train acc 0.794, test loss 0.5139, test acc 0.808, time 5.6 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eXXJN5MoAca"
      },
      "source": [
        "## MNIST - *Overfitting* and regularizadores\n",
        "\n",
        "Agora usaremos um dataset específico juntamente com um rede mais produnda para tentar mostrar o efeito de *overfitting* e entender como as técnicas de regularização aprendidas podem ser usadas para resolver esse problema.\n",
        "\n",
        "O modelo implementado abaixo usa o dataset MNIST treinando uma rede com quatro camadas que não usa nenhum método de regularização.\n",
        "Note a diferença entre o *loss* e a acurácia de treino e teste.\n",
        "Um resultado onde o *loss* do teste é relativamente maior que o treino (caso do exemplo abaixo) indica que um (neste caso, princípio de) *overfitting* está acontecendo.\n",
        "Use as técnicas vistas nesta aula prática para tratar esse problema.\n",
        "\n",
        "Especificamente, implemente:\n",
        "\n",
        "1. Uma versão dessa arquitetura com *Dropout*. Teste diferentes valores de probabilidade de forma a diminuir o *overfitting*.\n",
        "2. Uma versão desse modelo com *Weight Decay*. Teste diferentes valores de $\\lambda$ de forma a diminuir o *overfitting*.\n",
        "3. Uma versão que combina os dois métodos de regularização aprendidos. Talvez seja necessário testar diferentes valores para a probabilidade do *Dropout* e do $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblIVW2CoV-P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "4fdaa1ad-f31c-4869-a910-caf7af6ec1ed"
      },
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
        "# tamanho do batch\n",
        "num_epochs, lr, batch_size = 20, 0.5, 256\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10)\n",
        ")  \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 20835856.59it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/datasets/fashion-mnist/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 332417.36it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting /root/.pytorch/datasets/fashion-mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 4986039.82it/s]                           \n",
            "8192it [00:00, 130660.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/datasets/fashion-mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting /root/.pytorch/datasets/fashion-mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "training on cuda\n",
            "epoch 1, train loss 2.3017, train acc 0.112, test loss 2.3004, test acc 0.114, time 7.4 sec\n",
            "epoch 2, train loss 1.8158, train acc 0.286, test loss 1.4500, test acc 0.420, time 7.4 sec\n",
            "epoch 3, train loss 0.6222, train acc 0.790, test loss 0.3616, test acc 0.892, time 7.4 sec\n",
            "epoch 4, train loss 0.2093, train acc 0.943, test loss 0.1459, test acc 0.959, time 7.4 sec\n",
            "epoch 5, train loss 0.1273, train acc 0.964, test loss 0.1348, test acc 0.963, time 7.4 sec\n",
            "epoch 6, train loss 0.0965, train acc 0.972, test loss 0.1890, test acc 0.947, time 7.5 sec\n",
            "epoch 7, train loss 0.0746, train acc 0.978, test loss 0.1013, test acc 0.973, time 7.4 sec\n",
            "epoch 8, train loss 0.0545, train acc 0.984, test loss 0.1171, test acc 0.968, time 7.4 sec\n",
            "epoch 9, train loss 0.0436, train acc 0.986, test loss 0.1046, test acc 0.970, time 7.5 sec\n",
            "epoch 10, train loss 0.0368, train acc 0.989, test loss 0.1840, test acc 0.954, time 7.4 sec\n",
            "epoch 11, train loss 0.7898, train acc 0.807, test loss 0.1627, test acc 0.951, time 7.4 sec\n",
            "epoch 12, train loss 0.0943, train acc 0.972, test loss 0.1269, test acc 0.963, time 7.4 sec\n",
            "epoch 13, train loss 0.0691, train acc 0.980, test loss 0.1297, test acc 0.965, time 7.4 sec\n",
            "epoch 14, train loss 0.0545, train acc 0.983, test loss 0.1354, test acc 0.964, time 7.4 sec\n",
            "epoch 15, train loss 0.0647, train acc 0.981, test loss 0.1037, test acc 0.974, time 7.4 sec\n",
            "epoch 16, train loss 0.0356, train acc 0.989, test loss 0.1078, test acc 0.975, time 7.4 sec\n",
            "epoch 17, train loss 0.0306, train acc 0.991, test loss 0.1182, test acc 0.974, time 7.3 sec\n",
            "epoch 18, train loss 0.0245, train acc 0.993, test loss 0.1375, test acc 0.967, time 7.4 sec\n",
            "epoch 19, train loss 0.0223, train acc 0.993, test loss 0.1116, test acc 0.973, time 7.5 sec\n",
            "epoch 20, train loss 2.2267, train acc 0.875, test loss 2.3436, test acc 0.097, time 7.5 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEu01dDokIlS"
      },
      "source": [
        "## Exercícios\n",
        "\n",
        "1. Qual impacto de alterar a probabilidade da camada de *Dropout*?  Como fica uma rede com probabilidade de 50% de \"desligar\" os neurônios?\n",
        "1. E em relação ao valor de $\\lambda$ para o *Weight Decay*, qual impacto? Teste valores como 0.0005 e 0.0001.\n",
        "1. Qual efeito da quantidade de *epochs* na acurácia final do modelo?\n",
        "\n",
        "\n"
      ]
    }
  ]
}