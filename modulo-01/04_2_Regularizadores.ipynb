{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Regulizadores\n",
        "\n",
        "Neste código iremos analisar como funcionam os regularizadores e como eles são usados para evitar *overfitting*, fenônemo que faz com o modelo não generalize bem em outros dataset além do usando durante o treino.\n",
        "\n",
        "# *Overfitting*\n",
        "\n",
        "Redes neurais são muito flexíveis porque não se limitam a ver cada atributo a ser aprendido individualmente. Em vez disso, elas podem aprender interações entre os atributos. Por causa disso, mesmo quando temos apenas um pequeno número de atributos, as redes neurais profundas são capazes de chegar ao *overfitting*, um cenário onde o modelo aprende a classificar muito bem (as vezes, perfeitamente) as instâncias de treino, porém não generaliza para outras instâncias não vistas (como são os casos de amostras do conjunto de validação ou teste).\n",
        "\n",
        "Para evitar esse cenário, algumas técnicas foram propostas para evitar o *overfitting*.\n",
        "\n",
        "**Como introduzido, nesta aula prática, implementaremos e testaremos duas técnicas para evitar o *overfitting*: *Dropout* e *Weight Decay***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6CwWnFnTwb"
      },
      "source": [
        "**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW-VATPAldgt",
        "outputId": "b1d1c62f-ba2e-4c50-d2b0-42f6c4d34a54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g9u0pCOtlWLu"
      },
      "outputs": [],
      "source": [
        "## carregando dados\n",
        "\n",
        "# código para carregar o dataset do MNIST\n",
        "# http://yann.lecun.com/exdb/mnist/\n",
        "def load_data_mnist(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [transforms.Resize(resize)]\n",
        "    transformer += [transforms.ToTensor()]\n",
        "    transformer = transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = datasets.MNIST(root=root, train=True,download=True, transform=transformer)\n",
        "    mnist_test = datasets.MNIST(root=root, train=False,download=True, transform=transformer)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "# código para carregar o dataset do Fashion-MNIST\n",
        "# https://github.com/zalandoresearch/fashion-mnist\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [transforms.Resize(resize)]\n",
        "    transformer += [transforms.ToTensor()]\n",
        "    transformer = transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = datasets.FashionMNIST(root=root, train=True, download=True, transform=transformer)\n",
        "    mnist_test = datasets.FashionMNIST(root=root, train=False, download=True, transform=transformer)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "outputs": [],
      "source": [
        "# funções básicas\n",
        "def _get_batch(batch):\n",
        "    \"\"\"Return features and labels on ctx.\"\"\"\n",
        "    features, labels = batch\n",
        "    if labels.type() != features.type():\n",
        "        labels = labels.type(features.type())\n",
        "    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n",
        "            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n",
        "\n",
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    acc_sum, n, l = torch.Tensor([0]), 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          #y = y.astype('float32')\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y).sum()\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "          n += y.size()[0]\n",
        "\n",
        "    return acc_sum.item() / n, l.item() / len(data_iter)\n",
        "  \n",
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n",
        "                   num_epochs):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            trainer.zero_grad()\n",
        "            l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size()[0]\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n",
        "                 test_acc, time.time() - start))\n",
        "\n",
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Azv2ajIYkIjH"
      },
      "source": [
        "## *Dropout*\n",
        "\n",
        "*Dropout* é uma das formas mais interessantes de regularizar sua rede neural. \n",
        "A ideia do *Droupout* é simples: durante o passo de *Forward*, alguns neurônios são aleatoriamente \"desligados\", ou seja, são zerados, e não são utilizados em nenhum processamento.\n",
        "Em cada passo do *Forward*, neurônios diferentes são \"desligados\" aleatoriamente, de acordo com uma probabilide pré-definida.\n",
        "Lembrem-se que esse processo só acontece durante o treino.\n",
        "Durante o teste, *Dropout* não tem nenhuma ação e todos os neurônios são usados para gerar o resultado final.\n",
        "\n",
        "Formalmente, suponha um neurônio com ativação $h$ e um *Dropout* com probabilide $p$ (de zerar ou \"desligar\" o neurônio).\n",
        "Logo, essa técnica irá \"desligar\" a ativação desse neurônio com probabilidade $p$ ou reescala-la baseado na probabilidade de essa unidade de processamento permanecer ativa (isto é, $1-p$):\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h' =\n",
        "\\begin{cases}\n",
        "    0 & \\text{ com probabilidade } p \\\\\n",
        "    \\frac{h}{1-p} & \\text{ caso contrário}\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Tal método é interessante e chamou a atenção do mundo acadêmico por ser muito simples de implementar e poder impulsar significativamente o desempenho do modelo.\n",
        "\n",
        "### Implementação\n",
        "\n",
        "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), para utilizar os benefícios do Dropout basta adicionar a camada homônima (passando como argumento a probabilidade de desligamento dos neurônios) durante a construção da arquitetura.\n",
        "\n",
        "**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**\n",
        "\n",
        "Durante o treino, a camada *Dropout* irá \"desligar\" aleatoriamente algumas saídas da camada anterior (ou equivalentemente, as entradas para a camada subsequente) de acordo com a probabilidade especificada.\n",
        "\n",
        "Quando o PyTorch não está no modo de treinamento, a camada *Dropout* simplesmente passa os dados sem fazer nenhum \"desligamento\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Early stopping & Dropout: não passamos todas features por todos os neurônios em todas as iterações do nosso modelo. Para fazer isso, desativamos alguns neurônios (dropouts) e observamos o impacto disto no training dataset. Existe uma porcentagem ideal de neurônios desativados em que o modelo minimiza o erro, sem overfitting. Se não droparmos nenhum neurônio: overfitting. Se droparmos neurônios demais: o modelo deixa de ser eficiente e começa a fazer classificações erradas. Vídeo bom: https://www.youtube.com/watch?v=aloQiXhNORs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlBGUroCFfyh",
        "outputId": "cd6b7f2b-56db-4cf8-e4c7-14b23c26fa44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/danielcarmo/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "training on mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 1.1802, train acc 0.546, test loss 0.6882, test acc 0.750, time 6.3 sec\n",
            "epoch 2, train loss 0.5840, train acc 0.782, test loss 0.5647, test acc 0.788, time 3.6 sec\n",
            "epoch 3, train loss 0.4915, train acc 0.822, test loss 0.4975, test acc 0.815, time 3.6 sec\n",
            "epoch 4, train loss 0.4489, train acc 0.836, test loss 0.4513, test acc 0.833, time 3.5 sec\n",
            "epoch 5, train loss 0.4213, train acc 0.846, test loss 0.4486, test acc 0.837, time 3.6 sec\n",
            "epoch 6, train loss 0.3998, train acc 0.854, test loss 0.4939, test acc 0.816, time 3.5 sec\n",
            "epoch 7, train loss 0.3825, train acc 0.859, test loss 0.5168, test acc 0.814, time 3.6 sec\n",
            "epoch 8, train loss 0.3711, train acc 0.864, test loss 0.5306, test acc 0.808, time 3.6 sec\n",
            "epoch 9, train loss 0.3604, train acc 0.867, test loss 0.4337, test acc 0.840, time 3.6 sec\n",
            "epoch 10, train loss 0.3482, train acc 0.873, test loss 0.4798, test acc 0.827, time 3.6 sec\n",
            "epoch 11, train loss 0.3395, train acc 0.873, test loss 0.4803, test acc 0.829, time 3.6 sec\n",
            "epoch 12, train loss 0.3323, train acc 0.878, test loss 0.4343, test acc 0.842, time 3.6 sec\n",
            "epoch 13, train loss 0.3262, train acc 0.879, test loss 0.4016, test acc 0.853, time 3.6 sec\n",
            "epoch 14, train loss 0.3185, train acc 0.883, test loss 0.3995, test acc 0.856, time 3.6 sec\n",
            "epoch 15, train loss 0.3143, train acc 0.887, test loss 0.4241, test acc 0.841, time 3.6 sec\n",
            "epoch 16, train loss 0.3081, train acc 0.886, test loss 0.4235, test acc 0.847, time 3.6 sec\n",
            "epoch 17, train loss 0.3018, train acc 0.888, test loss 0.3725, test acc 0.867, time 3.6 sec\n",
            "epoch 18, train loss 0.2956, train acc 0.890, test loss 0.4163, test acc 0.849, time 3.6 sec\n",
            "epoch 19, train loss 0.2914, train acc 0.891, test loss 0.3908, test acc 0.858, time 3.6 sec\n",
            "epoch 20, train loss 0.2888, train acc 0.894, test loss 0.4214, test acc 0.855, time 3.6 sec\n"
          ]
        }
      ],
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
        "# tamanho do batch\n",
        "num_epochs, lr, batch_size = 20, 0.5, 256\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.2),                   # dropout com 20% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.5),                   # dropout com 50% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JJTUwjd4QSkF"
      },
      "source": [
        "## *Weight Decay*\n",
        "\n",
        "*Weight Decay* (comumente chamado regularização *L2*), é uma das técnicas mais utilizadas para regularizar modelos paramétricos de aprendizado de máquina.\n",
        "A intuição básica por trás do *Weight Decay* é a noção de que entre todas as funções $f$, a função $f=0$ é a mais simples. Intuitivamente, podemos medir funções pela sua proximidade a zero. Mas quão devemos medir a distância entre uma função e zero? Não há uma resposta correta. De fato, ramos inteiros da matemática são dedicados a responder a esta questão.\n",
        "\n",
        "Para nossos propósitos atuais, uma interpretação muito simples será suficiente: vamos considerar uma função linear $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ é simples se o seu vetor de peso $\\mathbf{w}$ for pequeno. Podemos medir isso via norma $||\\mathbf{w}||^2$. Uma maneira de manter o vetor de peso pequeno é adicionar sua norma como um termo de penalidade ao problema de minimizar a função de perda (ou *loss*). Assim, nós substituímos nosso objetivo original, *minimizar o erro de previsão nos rótulos de treinamento*, com novo objetivo, *minimizar o erro de previsão e o termo de penalidade*. Agora, se o vetor de peso se tornar muito grande, nosso algoritmo de aprendizagem vai encontrar mais lucro minimizando a norma $||\\mathbf{w}||^2$ do que minimizando o erro de treinamento. \n",
        "\n",
        "Tecnicamente, para uma função de custo qualquer $\\mathcal{L}$, a adição do novo termo de penalidade (ou *weight decay*) acontece da seguinte forma:\n",
        "\n",
        "$$\\mathcal{L}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
        "\n",
        "Esse parâmetro não negativo $\\lambda \\geq 0$ dita a quantidade de regularização. Para $\\lambda = 0$, recuperamos nossa função de perda original, enquanto para $\\lambda > 0 $ garantimos que os pesos $\\mathbf{w}$ não crescerão demais.\n",
        "\n",
        "### Implementação\n",
        "\n",
        "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), *Weight Decay* pode ser facilmente agregado à função de custo durante a construção do modelo.\n",
        "\n",
        "**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://www.youtube.com/watch?v=_SlPBbxuqas <- vídeo bom sobre Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB-koQ_vd9ef",
        "outputId": "a922c174-80bb-47d1-e8fc-030b411f41d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on mps\n",
            "epoch 1, train loss 1.2811, train acc 0.512, test loss 0.9115, test acc 0.689, time 3.7 sec\n",
            "epoch 2, train loss 0.6537, train acc 0.755, test loss 0.6474, test acc 0.756, time 3.6 sec\n",
            "epoch 3, train loss 0.6047, train acc 0.780, test loss 0.7502, test acc 0.746, time 3.6 sec\n",
            "epoch 4, train loss 0.7718, train acc 0.736, test loss 0.5906, test acc 0.799, time 3.6 sec\n",
            "epoch 5, train loss 0.6038, train acc 0.782, test loss 0.5031, test acc 0.819, time 3.6 sec\n",
            "epoch 6, train loss 0.5464, train acc 0.801, test loss 0.6311, test acc 0.754, time 3.6 sec\n",
            "epoch 7, train loss 0.5440, train acc 0.800, test loss 0.7216, test acc 0.759, time 3.6 sec\n",
            "epoch 8, train loss 0.5483, train acc 0.801, test loss 0.5547, test acc 0.793, time 3.6 sec\n",
            "epoch 9, train loss 0.5433, train acc 0.802, test loss 0.4987, test acc 0.820, time 3.7 sec\n",
            "epoch 10, train loss 1.0658, train acc 0.658, test loss 1.4996, test acc 0.386, time 3.5 sec\n",
            "epoch 11, train loss 0.8991, train acc 0.652, test loss 0.6865, test acc 0.728, time 3.6 sec\n",
            "epoch 12, train loss 0.6229, train acc 0.771, test loss 0.6146, test acc 0.769, time 3.6 sec\n",
            "epoch 13, train loss 0.7967, train acc 0.712, test loss 0.6565, test acc 0.761, time 3.6 sec\n",
            "epoch 14, train loss 0.5935, train acc 0.784, test loss 0.5901, test acc 0.795, time 3.5 sec\n",
            "epoch 15, train loss 0.5658, train acc 0.795, test loss 0.6265, test acc 0.776, time 3.5 sec\n",
            "epoch 16, train loss 0.5859, train acc 0.789, test loss 0.7280, test acc 0.727, time 3.6 sec\n",
            "epoch 17, train loss 0.5770, train acc 0.789, test loss 0.6353, test acc 0.757, time 3.6 sec\n",
            "epoch 18, train loss 0.5634, train acc 0.795, test loss 0.6069, test acc 0.756, time 3.5 sec\n",
            "epoch 19, train loss 0.5611, train acc 0.792, test loss 0.6811, test acc 0.710, time 3.5 sec\n",
            "epoch 20, train loss 1.0977, train acc 0.588, test loss 1.0635, test acc 0.534, time 3.6 sec\n"
          ]
        }
      ],
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n",
        "# tamanho do batch, e valor de weight decay\n",
        "num_epochs, lr, batch_size, weight_decay = 20, 0.5, 256, 0.005\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eXXJN5MoAca"
      },
      "source": [
        "## MNIST - *Overfitting* and regularizadores\n",
        "\n",
        "Agora usaremos um dataset específico juntamente com um rede mais produnda para tentar mostrar o efeito de *overfitting* e entender como as técnicas de regularização aprendidas podem ser usadas para resolver esse problema.\n",
        "\n",
        "O modelo implementado abaixo usa o dataset MNIST treinando uma rede com quatro camadas que não usa nenhum método de regularização.\n",
        "Note a diferença entre o *loss* e a acurácia de treino e teste.\n",
        "Um resultado onde o *loss* do teste é relativamente maior que o treino (caso do exemplo abaixo) indica que um (neste caso, princípio de) *overfitting* está acontecendo.\n",
        "Use as técnicas vistas nesta aula prática para tratar esse problema.\n",
        "\n",
        "Especificamente, implemente:\n",
        "\n",
        "1. Uma versão dessa arquitetura com *Dropout*. Teste diferentes valores de probabilidade de forma a diminuir o *overfitting*.\n",
        "2. Uma versão desse modelo com *Weight Decay*. Teste diferentes valores de $\\lambda$ de forma a diminuir o *overfitting*.\n",
        "3. Uma versão que combina os dois métodos de regularização aprendidos. Talvez seja necessário testar diferentes valores para a probabilidade do *Dropout* e do $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "BblIVW2CoV-P",
        "outputId": "4fdaa1ad-f31c-4869-a910-caf7af6ec1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on mps\n",
            "epoch 1, train loss 2.3016, train acc 0.111, test loss 2.3010, test acc 0.114, time 3.8 sec\n",
            "epoch 2, train loss 1.8922, train acc 0.257, test loss 1.1043, test acc 0.579, time 3.6 sec\n",
            "epoch 3, train loss 0.5050, train acc 0.838, test loss 0.2050, test acc 0.940, time 3.5 sec\n",
            "epoch 4, train loss 0.1725, train acc 0.950, test loss 0.1447, test acc 0.958, time 3.5 sec\n",
            "epoch 5, train loss 0.1138, train acc 0.966, test loss 0.1119, test acc 0.966, time 3.5 sec\n",
            "epoch 6, train loss 0.0861, train acc 0.974, test loss 0.0939, test acc 0.972, time 3.5 sec\n",
            "epoch 7, train loss 0.0664, train acc 0.980, test loss 0.1169, test acc 0.966, time 3.5 sec\n",
            "epoch 8, train loss 0.0545, train acc 0.983, test loss 0.1472, test acc 0.956, time 3.5 sec\n",
            "epoch 9, train loss 0.0435, train acc 0.987, test loss 0.1334, test acc 0.961, time 3.5 sec\n",
            "epoch 10, train loss 0.3857, train acc 0.916, test loss 0.2103, test acc 0.947, time 3.5 sec\n",
            "epoch 11, train loss 0.1141, train acc 0.967, test loss 0.1239, test acc 0.963, time 3.6 sec\n",
            "epoch 12, train loss 0.0692, train acc 0.979, test loss 0.1010, test acc 0.973, time 3.5 sec\n",
            "epoch 13, train loss 0.0548, train acc 0.983, test loss 0.1449, test acc 0.960, time 3.5 sec\n",
            "epoch 14, train loss 0.0432, train acc 0.987, test loss 0.1028, test acc 0.973, time 3.6 sec\n",
            "epoch 15, train loss 0.0341, train acc 0.990, test loss 0.0890, test acc 0.976, time 3.5 sec\n",
            "epoch 16, train loss 0.0300, train acc 0.990, test loss 0.1028, test acc 0.976, time 3.6 sec\n",
            "epoch 17, train loss 0.0232, train acc 0.993, test loss 0.1984, test acc 0.956, time 3.5 sec\n",
            "epoch 18, train loss 0.0207, train acc 0.993, test loss 0.1094, test acc 0.974, time 3.5 sec\n",
            "epoch 19, train loss 0.0195, train acc 0.994, test loss 0.2130, test acc 0.952, time 3.5 sec\n",
            "epoch 20, train loss 0.0153, train acc 0.995, test loss 0.1272, test acc 0.971, time 3.5 sec\n"
          ]
        }
      ],
      "source": [
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
        "# tamanho do batch\n",
        "num_epochs, lr, batch_size = 20, 0.5, 256\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10)\n",
        ")  \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A diferença entre train loss e test loss está grande demais, o que é indicativo do início de um overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on mps\n",
            "epoch 1, train loss 2.3016, train acc 0.111, test loss 2.3017, test acc 0.103, time 3.8 sec\n",
            "epoch 2, train loss 2.0023, train acc 0.223, test loss 1.3912, test acc 0.444, time 3.8 sec\n",
            "epoch 3, train loss 0.8083, train acc 0.720, test loss 0.4889, test acc 0.855, time 3.8 sec\n",
            "epoch 4, train loss 0.3249, train acc 0.912, test loss 0.3691, test acc 0.898, time 3.8 sec\n",
            "epoch 5, train loss 0.2291, train acc 0.936, test loss 0.2139, test acc 0.941, time 3.7 sec\n",
            "epoch 6, train loss 0.1837, train acc 0.948, test loss 0.2841, test acc 0.916, time 3.8 sec\n",
            "epoch 7, train loss 0.1656, train acc 0.952, test loss 0.1922, test acc 0.945, time 3.7 sec\n",
            "epoch 8, train loss 0.1492, train acc 0.957, test loss 0.1692, test acc 0.951, time 3.8 sec\n",
            "epoch 9, train loss 0.1386, train acc 0.959, test loss 0.1573, test acc 0.957, time 3.8 sec\n",
            "epoch 10, train loss 0.1329, train acc 0.961, test loss 0.1815, test acc 0.950, time 3.7 sec\n",
            "epoch 11, train loss 0.1243, train acc 0.964, test loss 0.1471, test acc 0.959, time 3.8 sec\n",
            "epoch 12, train loss 0.1171, train acc 0.965, test loss 0.1501, test acc 0.956, time 3.7 sec\n",
            "epoch 13, train loss 0.1092, train acc 0.968, test loss 0.1610, test acc 0.954, time 3.8 sec\n",
            "epoch 14, train loss 0.1086, train acc 0.968, test loss 0.1488, test acc 0.959, time 3.8 sec\n",
            "epoch 15, train loss 0.1006, train acc 0.971, test loss 0.1421, test acc 0.959, time 3.8 sec\n",
            "epoch 16, train loss 0.0989, train acc 0.971, test loss 0.1889, test acc 0.948, time 3.8 sec\n",
            "epoch 17, train loss 0.0945, train acc 0.971, test loss 0.1394, test acc 0.961, time 3.9 sec\n",
            "epoch 18, train loss 0.0915, train acc 0.973, test loss 0.1420, test acc 0.962, time 3.8 sec\n",
            "epoch 19, train loss 0.0862, train acc 0.975, test loss 0.1678, test acc 0.952, time 3.8 sec\n",
            "epoch 20, train loss 0.0846, train acc 0.975, test loss 0.1549, test acc 0.957, time 3.8 sec\n"
          ]
        }
      ],
      "source": [
        "#implementando modelo com Dropout\n",
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
        "# tamanho do batch\n",
        "num_epochs, lr, batch_size = 20, 0.5, 256\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),\n",
        "        nn.Dropout(0.50),    # dropout com 20% de probabilidade de desligar os neurônios          \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),              \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.Dropout(0.20),   # dropout com 20% de probabilidade de desligar os neurônios           \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10)\n",
        ")  \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: fashion mnist\n",
        "train_iter, test_iter = load_data_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um resultado razoável, melhor do que sem o uso de dropout!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on mps\n",
            "epoch 1, train loss 1.0213, train acc 0.647, test loss 0.4307, test acc 0.871, time 3.6 sec\n",
            "epoch 2, train loss 0.2929, train acc 0.914, test loss 0.2638, test acc 0.922, time 3.6 sec\n",
            "epoch 3, train loss 0.2170, train acc 0.937, test loss 0.8225, test acc 0.837, time 3.5 sec\n",
            "epoch 4, train loss 0.2037, train acc 0.942, test loss 0.1826, test acc 0.946, time 3.6 sec\n",
            "epoch 5, train loss 0.1900, train acc 0.947, test loss 0.2142, test acc 0.936, time 3.5 sec\n",
            "epoch 6, train loss 0.2044, train acc 0.943, test loss 0.1697, test acc 0.953, time 3.6 sec\n",
            "epoch 7, train loss 0.1800, train acc 0.951, test loss 0.1625, test acc 0.953, time 3.6 sec\n",
            "epoch 8, train loss 0.2076, train acc 0.943, test loss 0.2002, test acc 0.936, time 3.6 sec\n",
            "epoch 9, train loss 0.1709, train acc 0.953, test loss 0.1790, test acc 0.954, time 3.5 sec\n",
            "epoch 10, train loss 0.1566, train acc 0.957, test loss 0.2024, test acc 0.936, time 3.6 sec\n",
            "epoch 11, train loss 0.1621, train acc 0.955, test loss 0.2034, test acc 0.937, time 3.6 sec\n",
            "epoch 12, train loss 0.1601, train acc 0.957, test loss 0.1485, test acc 0.959, time 3.6 sec\n",
            "epoch 13, train loss 0.1749, train acc 0.952, test loss 0.1315, test acc 0.965, time 3.5 sec\n",
            "epoch 14, train loss 0.1585, train acc 0.956, test loss 0.1363, test acc 0.964, time 3.6 sec\n",
            "epoch 15, train loss 0.1559, train acc 0.958, test loss 0.1352, test acc 0.963, time 3.7 sec\n",
            "epoch 16, train loss 0.1702, train acc 0.954, test loss 0.1556, test acc 0.953, time 3.6 sec\n",
            "epoch 17, train loss 0.1573, train acc 0.957, test loss 0.2279, test acc 0.925, time 3.6 sec\n",
            "epoch 18, train loss 0.1882, train acc 0.949, test loss 0.1897, test acc 0.945, time 3.7 sec\n",
            "epoch 19, train loss 0.1547, train acc 0.958, test loss 0.1855, test acc 0.945, time 3.6 sec\n",
            "epoch 20, train loss 0.1831, train acc 0.951, test loss 0.1833, test acc 0.950, time 3.6 sec\n"
          ]
        }
      ],
      "source": [
        "#implementação usando weight decay\n",
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n",
        "# tamanho do batch, e valor de weight decay\n",
        "num_epochs, lr, batch_size, weight_decay = 20, 0.5, 256, 0.005\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: mnist\n",
        "train_iter, test_iter = load_data_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on mps\n",
            "epoch 1, train loss 1.1933, train acc 0.576, test loss 0.5532, test acc 0.820, time 3.8 sec\n",
            "epoch 2, train loss 0.4024, train acc 0.881, test loss 0.4138, test acc 0.866, time 3.8 sec\n",
            "epoch 3, train loss 0.3464, train acc 0.899, test loss 0.5107, test acc 0.831, time 3.7 sec\n",
            "epoch 4, train loss 0.3204, train acc 0.910, test loss 0.4552, test acc 0.859, time 3.7 sec\n",
            "epoch 5, train loss 0.3108, train acc 0.912, test loss 0.3162, test acc 0.912, time 3.8 sec\n",
            "epoch 6, train loss 0.3068, train acc 0.913, test loss 0.3049, test acc 0.906, time 3.7 sec\n",
            "epoch 7, train loss 0.2981, train acc 0.915, test loss 0.3133, test acc 0.906, time 3.7 sec\n",
            "epoch 8, train loss 0.2965, train acc 0.916, test loss 0.2416, test acc 0.931, time 3.7 sec\n",
            "epoch 9, train loss 0.2999, train acc 0.916, test loss 0.2926, test acc 0.913, time 3.7 sec\n",
            "epoch 10, train loss 0.2869, train acc 0.920, test loss 0.2484, test acc 0.927, time 3.7 sec\n",
            "epoch 11, train loss 0.2918, train acc 0.918, test loss 0.2380, test acc 0.938, time 3.7 sec\n",
            "epoch 12, train loss 0.2863, train acc 0.920, test loss 0.2624, test acc 0.926, time 3.7 sec\n",
            "epoch 13, train loss 0.2936, train acc 0.917, test loss 0.5415, test acc 0.822, time 3.8 sec\n",
            "epoch 14, train loss 0.2877, train acc 0.920, test loss 0.2836, test acc 0.916, time 3.8 sec\n",
            "epoch 15, train loss 0.2981, train acc 0.916, test loss 0.3250, test acc 0.892, time 3.8 sec\n",
            "epoch 16, train loss 0.2840, train acc 0.921, test loss 0.2594, test acc 0.931, time 3.9 sec\n",
            "epoch 17, train loss 0.2852, train acc 0.920, test loss 0.2484, test acc 0.930, time 3.8 sec\n",
            "epoch 18, train loss 0.2926, train acc 0.918, test loss 0.4029, test acc 0.879, time 3.9 sec\n",
            "epoch 19, train loss 0.2789, train acc 0.923, test loss 0.2850, test acc 0.918, time 3.8 sec\n",
            "epoch 20, train loss 0.2838, train acc 0.921, test loss 0.2254, test acc 0.940, time 3.8 sec\n"
          ]
        }
      ],
      "source": [
        "#implementação usando weight decay e dropout\n",
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n",
        "# tamanho do batch, e valor de weight decay\n",
        "num_epochs, lr, batch_size, weight_decay = 20, 0.5, 256, 0.01\n",
        "\n",
        "# rede simples somente com perceptrons e camadas densamente conectadas\n",
        "net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.20),    # dropout com 20% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),               # camada densamente conectada\n",
        "        nn.Dropout(0.20),    # dropout com 20% de probabilidade de desligar os neurônios\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)                 # camada densamente conectada para classificação\n",
        ")                     \n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device) # diz para a rede que ela deve ser treinada na GPU\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# carregamento do dado: mnist\n",
        "train_iter, test_iter = load_data_mnist(batch_size)\n",
        "\n",
        "# trainer do gluon\n",
        "trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "# treinamento e validação\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lEu01dDokIlS"
      },
      "source": [
        "## Exercícios\n",
        "\n",
        "1. Qual impacto de alterar a probabilidade da camada de *Dropout*?  Como fica uma rede com probabilidade de 50% de \"desligar\" os neurônios?\n",
        "1. E em relação ao valor de $\\lambda$ para o *Weight Decay*, qual impacto? Teste valores como 0.0005 e 0.0001.\n",
        "1. Qual efeito da quantidade de *epochs* na acurácia final do modelo?\n",
        "\n",
        "\n",
        "1 - Se aumentarmos demais a probabilidade de dropout dos neurônios, o modelo pode ficar generalizado demais, não conseguindo prever os inputs corretamente em nenhum caso. No entanto, deve haver uma probabilidade de dropout ideal, em que o overfitting é previnido e a precisão do modelo continua satisfatória. \n",
        "2 - Aparentemente, após a realização de testes, percebemos que o aumento do lambda para um weight decay faz com que sejam exigidas mais épocas de treinamento para que o modelo seja treinado com eficácian alta. Isto se dá pois quanto maior for lambda, mais distante do erro real estamos penalizando nosso modelo, o que faz com que o modelo se sinta cada vez mais impreciso, demorando mais para aprender. \n",
        "3 - Quanto mais épocas, mais o nosso modelo aprende e, potencialmente, tem seu overfitting aumentado. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "04_2_Regularizadores.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
